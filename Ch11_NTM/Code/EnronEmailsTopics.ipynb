{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover topics from Enron Emails: A demo of how NTM works \n",
    "1. [Introduction](#Introduction)\n",
    "2. [Preprocessing](#Preprocessing)\n",
    "   1. [Create Bag-of-Words and Vocabulary](#Create-Bag-of-Words-and-Vocabulary)\n",
    "   2. [TF-IDF Term Frequency Inverse Document Frequency](#TF-IDF-Term-Frequency-Inverse-Document-Frequency)\n",
    "3. [Create Training Validation and Test Datasets](#Create-Training-Validation-and-Test-Datasets)\n",
    "   1. [Store Data on S3](#Store-Data-on-S3)\n",
    "   2. [Model Training](#Model-Training)\n",
    "   3. [Set Hyperparameters](#Set-Hyperparameters)\n",
    "4. [Model Hosting and Inference](#Model-Hosting-and-Inference)\n",
    "  1. [Inference with CSV](#Inference-with-CSV)\n",
    "  2. [Creating Word Cloud from Trained Model](#Creating-Word-Cloud-from-Trained-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "In this notebook, we will look at discovering topics from enron emails. Neural Topic Model (NTM) from Amazon SageMaker uses neural networks to learn word embeddings or topics. The word embeddings are derived by minimizing loss when building stochastic representation of input documents/emails.\n",
    "\n",
    "First, we build bag of words representation of each email, with each column representing a word and each row representing an email. The values in the matrix are number of times each word is repeated in a given email. We then scale the counts by multiplying them with TF-IDF factor (Term Frequency-Inverse Document Frequency). This factor ensures that words that are specific to a given email and are not repeated frequently across all emails are given higher weight, relative to the words that commonly occur across all emails (for ex: the, so, as, because etc). \n",
    "\n",
    "The prepared bag of words representation of emails is then fed into a neural network whose architecture is defined by the hyperparameters listed below. The network optimizes across several epochs by minimizing loss in building stochastic representation of emails (topic embeddings) and in reconstructing original emails from topic embeddings.\n",
    "\n",
    "The dataset for this notebook is downloaded from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/bag+of+words). It primarily contains list of words (vocabulary) used across all emails and a lookup table detailing number of occurrences of a word in a given email (EmailID WordID Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Acknowledgements, Copyright Information, and Availability**\n",
    "# You may use this material free of charge for any educational purpose, \n",
    "# provided attribution is given in any lectures or publications that make use of this material.\n",
    "#\n",
    "# Source: https://archive.ics.uci.edu/ml/datasets/bag+of+words\n",
    "# Source: SageMaker AWS Labs\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import sagemaker\n",
    "from sagemaker.session import s3_input\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "\n",
    "First, let's run user defined functions used to conduct common operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run bowemails.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag-of-Words and Vocabulary\n",
    "We will only take 10% of emails to have a manageable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_fn = 'data/docword.enron.txt.gz'\n",
    "percent_emails = .10 # get only a x% of emails to avoid memory errors\n",
    "vocab_ip_fn = 'data/vocab.enron.txt'\n",
    "vocab_op_fn = 'data/vocab.txt'\n",
    "\n",
    "#Get bag-of-words from input of enron emails\n",
    "# We will filter emails to reduce data size\n",
    "# Create vocabulary based on the subset of emails that will be sent to training\n",
    "pvt_emails = prepare_bow_vocab(ip_fn, percent_emails, vocab_ip_fn, vocab_op_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>word_ID</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>...</th>\n",
       "      <th>28090</th>\n",
       "      <th>28091</th>\n",
       "      <th>28092</th>\n",
       "      <th>28093</th>\n",
       "      <th>28095</th>\n",
       "      <th>28096</th>\n",
       "      <th>28097</th>\n",
       "      <th>28098</th>\n",
       "      <th>28100</th>\n",
       "      <th>28101</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>email_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 17524 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "word_ID   1      3      4      8      9      15     16     19     20     \\\n",
       "email_ID                                                                  \n",
       "1             0      0      0      0      0      0      0      0      0   \n",
       "2             0      0      0      0      0      0      0      0      0   \n",
       "3             0      0      0      0      0      0      0      0      0   \n",
       "4             0      0      0      0      0      0      0      0      0   \n",
       "5             0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "word_ID   21     ...    28090  28091  28092  28093  28095  28096  28097  \\\n",
       "email_ID         ...                                                      \n",
       "1             0  ...        0      0      0      0      0      0      0   \n",
       "2             0  ...        0      0      0      0      0      0      0   \n",
       "3             0  ...        0      0      0      0      0      0      0   \n",
       "4             0  ...        0      0      0      0      0      0      0   \n",
       "5             0  ...        0      0      0      0      0      0      0   \n",
       "\n",
       "word_ID   28098  28100  28101  \n",
       "email_ID                       \n",
       "1             0      0      0  \n",
       "2             0      0      0  \n",
       "3             0      0      0  \n",
       "4             0      0      0  \n",
       "5             0      0      0  \n",
       "\n",
       "[5 rows x 17524 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvt_emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3986, 17524)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvt_emails.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Term Frequency Inverse Document Frequency\n",
    "We assume that the words that help surface topics are those that are not repeated across all emails but are common within an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_emails = TF_IDF(pvt_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# convert pivoted dataframe to compressed sparse row matrix\n",
    "# compressed sparse row matrix contains row pointer, column index and values\n",
    "sparse_emails = csr_matrix(pvt_emails, dtype=np.float32)\n",
    "print(sparse_emails[:16].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sparse_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Validation and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(0.8 * sparse_emails.shape[0])\n",
    "\n",
    "# split train and test\n",
    "train_vectors = sparse_emails[:n_train, :] \n",
    "test_vectors = sparse_emails[n_train:, :] \n",
    "\n",
    "# further split test set into validation set and test set\n",
    "n_test = test_vectors.shape[0]\n",
    "val_vectors = test_vectors[:n_test//2, :]\n",
    "test_vectors = test_vectors[n_test//2:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3188, 17524) (399, 17524) (399, 17524)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors.shape, test_vectors.shape, val_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Data on S3\n",
    "\n",
    "The NTM algorithm, as well as other first-party SageMaker algorithms, accepts data in RecordIO Protobuf format. The SageMaker Python API provides helper functions for easily converting your data into this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Location s3://ai-in-aws/enronemails/train\n",
      "Validation set Location s3://ai-in-aws/enronemails/val\n",
      "Trained model will be saved at s3://ai-in-aws/enronemails/output\n",
      "Auxiliary will be saved at s3://ai-in-aws/enronemails/aux\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "# provide your bucket name here\n",
    "#bucket = '<bucket-name>'\n",
    "bucket = 'ai-in-aws'\n",
    "prefix = 'enronemails'\n",
    "\n",
    "train_prefix = os.path.join(prefix, 'train')\n",
    "val_prefix = os.path.join(prefix, 'val')\n",
    "output_prefix = os.path.join(prefix, 'output')\n",
    "aux_prefix = os.path.join(prefix, 'aux')\n",
    "\n",
    "s3_train_data = os.path.join('s3://', bucket, train_prefix)\n",
    "s3_val_data = os.path.join('s3://', bucket, val_prefix)\n",
    "s3_aux_data = os.path.join('s3://', bucket, aux_prefix)\n",
    "output_path = os.path.join('s3://', bucket, output_prefix)\n",
    "\n",
    "print('Training Location', s3_train_data)\n",
    "print('Validation set Location', s3_val_data)\n",
    "print('Trained model will be saved at', output_path)\n",
    "print('Auxiliary will be saved at', s3_aux_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the training data for parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert compressed sparse row matrix to recordio-wrapped-protobuf format\n",
    "# RecordIO is used to efficiently load large datasets (data can be read continuously and stored in a compressed format) \n",
    "\n",
    "split_convert_upload(train_vectors, bucket=bucket, prefix=train_prefix, fname_template='data_part{}.pbr', n_parts=3)\n",
    "split_convert_upload(val_vectors, bucket=bucket, prefix=val_prefix, fname_template='val_part{}.pbr', n_parts=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "SageMaker uses Amazon Elastic Container Registry (ECR) docker container to host the NTM training image. The ECR containers are currently available for SageMaker NTM training in different regions\n",
    "\n",
    "Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, which allows Python developers to write software that makes use of services like Amazon S3 and Amazon EC2\n",
    "\n",
    "SageMaker SDK - A library for training and deploying machine learning models on Amazon SageMaker - aws/sagemaker-python-sdk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'ntm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "ntm = sagemaker.estimator.Estimator(container,\n",
    "                                   role,\n",
    "                                   train_instance_count=2,\n",
    "                                   train_instance_type='ml.c4.xlarge',\n",
    "                                   output_path=output_path,\n",
    "                                   sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Hyperparameters\n",
    "\n",
    "__feature_dim__ - it should be set to vocabulary size <br>\n",
    "__num_topics__ - topics to extract <br>\n",
    "__mini_batch_size__ - this is the batch_size for each worker instance. <br>\n",
    "__epochs__ - the maximal number of epochs to train for. <br>\n",
    "__num_patience_epochs__ and tolerance control the early shopping behavior. Improvements smaller than the tolerance are not considered as improvement <br>\n",
    "__optimizer and learning_rate__ - Adadelta optimizer and learning rate is not required <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://ai-in-aws/enronemails/train'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 3\n",
    "vocab_size = 17524 # from shape from pivoted emails dataframe\n",
    "ntm.set_hyperparameters(num_topics=num_topics, \n",
    "                        feature_dim=vocab_size, \n",
    "                        mini_batch_size=30, \n",
    "                        epochs=150, \n",
    "                        num_patience_epochs=5, \n",
    "                        tolerance=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 141.6 KiB/141.6 KiB (1.3 MiB/s) with 1 file(s) remaining\r",
      "upload: data/vocab.txt to s3://ai-in-aws/enronemails/aux/vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Upload vocabulary file to auxiliary folder on S3 bucket -- this is used to identify words associated with latent topics\n",
    "aux_path = s3_aux_data + \"/\"\n",
    "\n",
    "!aws s3 cp $vocab_op_fn $aux_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train = s3_input(s3_train_data, distribution='ShardedByS3Key', content_type='application/x-recordio-protobuf')\n",
    "s3_val = s3_input(s3_val_data, distribution='FullyReplicated',\n",
    "                  content_type='application/x-recordio-protobuf')\n",
    "s3_aux = s3_input(s3_aux_data, distribution='FullyReplicated', content_type='text/plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: ntm-2019-04-09-17-24-19-713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-09 17:24:19 Starting - Starting the training job...\n",
      "2019-04-09 17:24:23 Starting - Launching requested ML instances......\n",
      "2019-04-09 17:25:29 Starting - Preparing the instances for training.........\n",
      "2019-04-09 17:27:13 Downloading - Downloading input data\n",
      "2019-04-09 17:27:13 Training - Training image download completed. Training in progress..\n",
      "\u001b[32mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:15 INFO 139701951670080] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:15 INFO 139701951670080] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'5', u'num_topics': u'3', u'epochs': u'150', u'feature_dim': u'17524', u'mini_batch_size': u'30', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:15 INFO 139701951670080] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'17524', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'5', u'epochs': u'150', u'mini_batch_size': u'30', u'num_topics': u'3', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:15 INFO 139701951670080] nvidia-smi took: 0.025242805481 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:16 INFO 139701951670080] Launching parameter server for role server\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:16 INFO 139701951670080] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/4400ca8d-1346-4b7d-a8c2-d16423fb29d0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-04-09-17-24-19-713', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/c6ea5f25-2caf-48dd-ab30-65179b1cec3a', 'PWD': '/', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:16 INFO 139701951670080] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/4400ca8d-1346-4b7d-a8c2-d16423fb29d0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-04-09-17-24-19-713', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/c6ea5f25-2caf-48dd-ab30-65179b1cec3a', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:16 INFO 139701951670080] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/4400ca8d-1346-4b7d-a8c2-d16423fb29d0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '2', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-04-09-17-24-19-713', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/c6ea5f25-2caf-48dd-ab30-65179b1cec3a', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[32mProcess 39 is a shell:server.\u001b[0m\n",
      "\u001b[32mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:16 INFO 139701951670080] Using default worker.\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:16.318] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/train\", \"num_examples\": 30, \"features\": [{\"name\": \"values\", \"shape\": [17524], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:16.320] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:16.321] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"num_examples\": 30, \"features\": [{\"name\": \"values\", \"shape\": [17524], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:16 INFO 139701951670080] Initializing\u001b[0m\n",
      "\u001b[32m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:16 INFO 139701951670080] /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:16 INFO 139701951670080] vocab.txt\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:16 INFO 139701951670080] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:16 INFO 139701951670080] Loading pre-trained token embedding vectors from /opt/amazon/lib/python2.7/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:15 INFO 139661896189760] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:15 INFO 139661896189760] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'5', u'num_topics': u'3', u'epochs': u'150', u'feature_dim': u'17524', u'mini_batch_size': u'30', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:15 INFO 139661896189760] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'17524', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'5', u'epochs': u'150', u'mini_batch_size': u'30', u'num_topics': u'3', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:15 INFO 139661896189760] nvidia-smi took: 0.0251941680908 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:16 INFO 139661896189760] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:16 INFO 139661896189760] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/17d3b170-6903-4458-bfdf-63969dd611b8', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-04-09-17-24-19-713', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/451b86d1-3f7f-4c78-828c-2a5573d698f9', 'PWD': '/', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:16 INFO 139661896189760] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/17d3b170-6903-4458-bfdf-63969dd611b8', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-04-09-17-24-19-713', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/451b86d1-3f7f-4c78-828c-2a5573d698f9', 'DMLC_ROLE': 'scheduler', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:16 INFO 139661896189760] Launching parameter server for role server\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:16 INFO 139661896189760] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/17d3b170-6903-4458-bfdf-63969dd611b8', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-04-09-17-24-19-713', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/451b86d1-3f7f-4c78-828c-2a5573d698f9', 'PWD': '/', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:16 INFO 139661896189760] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/17d3b170-6903-4458-bfdf-63969dd611b8', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-04-09-17-24-19-713', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/451b86d1-3f7f-4c78-828c-2a5573d698f9', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:16 INFO 139661896189760] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/17d3b170-6903-4458-bfdf-63969dd611b8', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '2', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-04-09-17-24-19-713', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/451b86d1-3f7f-4c78-828c-2a5573d698f9', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31mProcess 40 is a shell:scheduler.\u001b[0m\n",
      "\u001b[31mProcess 41 is a shell:server.\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:16 INFO 139661896189760] Using default worker.\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:16.354] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/train\", \"num_examples\": 30, \"features\": [{\"name\": \"values\", \"shape\": [17524], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:16.364] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:16.369] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"num_examples\": 30, \"features\": [{\"name\": \"values\", \"shape\": [17524], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:16 INFO 139661896189760] Initializing\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:16 INFO 139661896189760] /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:16 INFO 139661896189760] vocab.txt\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:16 INFO 139661896189760] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:16 INFO 139661896189760] Loading pre-trained token embedding vectors from /opt/amazon/lib/python2.7/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/09/2019 17:27:32 WARNING 139701951670080] 1421 out of 17524 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:32 INFO 139701951670080] Vocab embedding shape\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:32 INFO 139701951670080] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:32 INFO 139701951670080] Create Store: dist_async\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1554830852.534066, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1554830852.534037}\n",
      "\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:32.534] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 16216, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:32 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:32 INFO 139701951670080] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:32 WARNING 139661896189760] 1421 out of 17524 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:32 INFO 139661896189760] Vocab embedding shape\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:32 INFO 139661896189760] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:32 INFO 139661896189760] Create Store: dist_async\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1554830852.570109, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1554830852.570078}\n",
      "\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:32.570] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 16228, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:32 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:32 INFO 139661896189760] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] # Finished training epoch 1 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] Loss (name: value) total: 9.14763383512\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] Loss (name: value) kld: 0.163867848313\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] Loss (name: value) recons: 8.9837660189\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] Loss (name: value) logppx: 9.14763383512\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] #quality_metric: host=algo-2, epoch=1, train total_loss <loss>=9.14763383512\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:34.524] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 18203, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] Loss (name: value) total: 9.0187683888\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] Loss (name: value) kld: 0.213124080805\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] Loss (name: value) recons: 8.8056442652\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] Loss (name: value) logppx: 9.0187683888\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] #validation_score (1): 9.0187683888\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] Timing: train: 1.99s, val: 0.26s, epoch: 2.25s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] #progress_metric: host=algo-2, completed 0 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Total Records Seen\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1554830854.786623, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1554830852.534386}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=471.502725958 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:34.786] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 2252, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:34 INFO 139701951670080] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] # Finished training epoch 1 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] Loss (name: value) total: 8.9674353801\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] Loss (name: value) kld: 0.229345826698\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] Loss (name: value) recons: 8.73808961967\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] Loss (name: value) logppx: 8.9674353801\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=8.9674353801\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:36.658] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 20294, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] Loss (name: value) total: 8.80770611885\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] Loss (name: value) kld: 0.222364645738\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] Loss (name: value) recons: 8.58534138997\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] Loss (name: value) logppx: 8.80770611885\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] #validation_score (1): 8.80770611885\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] Timing: train: 4.09s, val: 0.25s, epoch: 4.34s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Total Records Seen\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1554830856.90814, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1554830852.570541}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=490.116413919 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:36.908] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 4337, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:36 INFO 139661896189760] # Starting training for epoch 2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/09/2019 17:27:36 INFO 139701951670080] # Finished training epoch 2 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:36 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:36 INFO 139701951670080] Loss (name: value) total: 8.44441777688\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:36 INFO 139701951670080] Loss (name: value) kld: 0.237974265107\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:36 INFO 139701951670080] Loss (name: value) recons: 8.20644351112\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:36 INFO 139701951670080] Loss (name: value) logppx: 8.44441777688\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:36 INFO 139701951670080] #quality_metric: host=algo-2, epoch=2, train total_loss <loss>=8.44441777688\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:36.831] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 1, \"duration\": 2307, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:37 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:37 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:37 INFO 139701951670080] Loss (name: value) total: 8.80692330385\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:37 INFO 139701951670080] Loss (name: value) kld: 0.255397141285\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:37 INFO 139701951670080] Loss (name: value) recons: 8.55152611366\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:37 INFO 139701951670080] Loss (name: value) logppx: 8.80692330385\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:37 INFO 139701951670080] #validation_score (2): 8.80692330385\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:37 INFO 139701951670080] Timing: train: 2.04s, val: 0.26s, epoch: 2.30s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:37 INFO 139701951670080] #progress_metric: host=algo-2, completed 1 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}, \"Total Records Seen\": {\"count\": 1, \"max\": 2124, \"sum\": 2124.0, \"min\": 2124}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1554830857.089245, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1554830854.786892}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:37 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=461.181480808 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:37.089] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 2302, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:37 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:37 INFO 139701951670080] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:38 INFO 139701951670080] # Finished training epoch 3 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:38 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:38 INFO 139701951670080] Loss (name: value) total: 8.28098208816\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:38 INFO 139701951670080] Loss (name: value) kld: 0.197271749708\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:38 INFO 139701951670080] Loss (name: value) recons: 8.08371036671\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:38 INFO 139701951670080] Loss (name: value) logppx: 8.28098208816\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:38 INFO 139701951670080] #quality_metric: host=algo-2, epoch=3, train total_loss <loss>=8.28098208816\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:38.976] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 2144, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:39 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:39 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:39 INFO 139701951670080] Loss (name: value) total: 8.75949507493\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:39 INFO 139701951670080] Loss (name: value) kld: 0.204255502652\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:39 INFO 139701951670080] Loss (name: value) recons: 8.55523951604\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:39 INFO 139701951670080] Loss (name: value) logppx: 8.75949507493\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:39 INFO 139701951670080] #validation_score (3): 8.75949507493\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:39 INFO 139701951670080] Timing: train: 1.89s, val: 0.25s, epoch: 2.13s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:39 INFO 139701951670080] #progress_metric: host=algo-2, completed 2 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 108, \"sum\": 108.0, \"min\": 108}, \"Total Records Seen\": {\"count\": 1, \"max\": 3186, \"sum\": 3186.0, \"min\": 3186}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1554830859.224495, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1554830857.0902}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:39 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=497.546236712 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:39.224] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 3, \"duration\": 2132, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:39 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:39 INFO 139701951670080] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:40 INFO 139661896189760] # Finished training epoch 2 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:40 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:40 INFO 139661896189760] Loss (name: value) total: 8.44827570669\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:40 INFO 139661896189760] Loss (name: value) kld: 0.210268284905\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:40 INFO 139661896189760] Loss (name: value) recons: 8.23800744428\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:40 INFO 139661896189760] Loss (name: value) logppx: 8.44827570669\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:40 INFO 139661896189760] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=8.44827570669\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:40.921] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 1, \"duration\": 4263, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:41 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:41 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:41 INFO 139661896189760] Loss (name: value) total: 8.71889198499\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:41 INFO 139661896189760] Loss (name: value) kld: 0.210466685662\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:41 INFO 139661896189760] Loss (name: value) recons: 8.50842535557\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:41 INFO 139661896189760] Loss (name: value) logppx: 8.71889198499\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:41 INFO 139661896189760] #validation_score (2): 8.71889198499\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:41 INFO 139661896189760] Timing: train: 4.01s, val: 0.27s, epoch: 4.29s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:41 INFO 139661896189760] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 142, \"sum\": 142.0, \"min\": 142}, \"Total Records Seen\": {\"count\": 1, \"max\": 4252, \"sum\": 4252.0, \"min\": 4252}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1554830861.195231, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1554830856.908428}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:41 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=495.916050277 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:41.195] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 4286, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:41 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:41 INFO 139661896189760] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] # Finished training epoch 4 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] Loss (name: value) total: 8.2056757821\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] Loss (name: value) kld: 0.177817007127\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] Loss (name: value) recons: 8.02785876239\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] Loss (name: value) logppx: 8.2056757821\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] #quality_metric: host=algo-2, epoch=4, train total_loss <loss>=8.2056757821\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:41.133] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 3, \"duration\": 2155, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] Loss (name: value) total: 8.72753917988\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] Loss (name: value) kld: 0.218812759106\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] Loss (name: value) recons: 8.50872638409\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] Loss (name: value) logppx: 8.72753917988\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] #validation_score (4): 8.72753917988\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] Timing: train: 1.91s, val: 0.24s, epoch: 2.15s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] #progress_metric: host=algo-2, completed 2 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 144, \"sum\": 144.0, \"min\": 144}, \"Total Records Seen\": {\"count\": 1, \"max\": 4248, \"sum\": 4248.0, \"min\": 4248}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1554830861.377024, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1554830859.224763}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=493.401780294 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:41.377] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 2152, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:41 INFO 139701951670080] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] # Finished training epoch 5 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] Loss (name: value) total: 8.17493897191\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] Loss (name: value) kld: 0.168249333788\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] Loss (name: value) recons: 8.00668967918\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] Loss (name: value) logppx: 8.17493897191\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] #quality_metric: host=algo-2, epoch=5, train total_loss <loss>=8.17493897191\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:43.298] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 4, \"duration\": 2165, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] Loss (name: value) total: 8.71244420761\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] Loss (name: value) kld: 0.171309487025\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] Loss (name: value) recons: 8.54113491743\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] Loss (name: value) logppx: 8.71244420761\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] #validation_score (5): 8.71244420761\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] Timing: train: 1.92s, val: 0.29s, epoch: 2.21s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] #progress_metric: host=algo-2, completed 3 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 180, \"sum\": 180.0, \"min\": 180}, \"Total Records Seen\": {\"count\": 1, \"max\": 5310, \"sum\": 5310.0, \"min\": 5310}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1554830863.585882, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1554830861.377293}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=480.818208255 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:43.586] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 2206, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:43 INFO 139701951670080] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] # Finished training epoch 3 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] Loss (name: value) total: 8.36528508003\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] Loss (name: value) kld: 0.177492990852\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] Loss (name: value) recons: 8.18779205895\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] Loss (name: value) logppx: 8.36528508003\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=8.36528508003\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:45.231] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 4309, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] # Finished training epoch 6 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] Loss (name: value) total: 8.14291726572\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] Loss (name: value) kld: 0.154503288092\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] Loss (name: value) recons: 7.98841392376\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] Loss (name: value) logppx: 8.14291726572\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] #quality_metric: host=algo-2, epoch=6, train total_loss <loss>=8.14291726572\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:45.547] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 2248, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] Loss (name: value) total: 8.68127711369\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] Loss (name: value) kld: 0.169403045605\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] Loss (name: value) recons: 8.51187407665\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] Loss (name: value) logppx: 8.68127711369\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] #validation_score (6): 8.68127711369\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] patience losses:[9.018768388797076, 8.8069233038486576, 8.7594950749323921, 8.7275391798753006, 8.712444207607172] min patience loss:8.71244420761 current loss:8.68127711369 absolute loss difference:0.0311670939128\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] Timing: train: 1.96s, val: 0.27s, epoch: 2.23s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] #progress_metric: host=algo-2, completed 4 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 216, \"sum\": 216.0, \"min\": 216}, \"Total Records Seen\": {\"count\": 1, \"max\": 6372, \"sum\": 6372.0, \"min\": 6372}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1554830865.820294, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1554830863.586153}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=475.320073937 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:45.820] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 2234, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:45 INFO 139701951670080] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] Loss (name: value) total: 8.68106771616\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] Loss (name: value) kld: 0.161709739\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] Loss (name: value) recons: 8.51935792581\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] Loss (name: value) logppx: 8.68106771616\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] #validation_score (3): 8.68106771616\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] Timing: train: 4.04s, val: 0.25s, epoch: 4.28s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 213, \"sum\": 213.0, \"min\": 213}, \"Total Records Seen\": {\"count\": 1, \"max\": 6378, \"sum\": 6378.0, \"min\": 6378}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1554830865.479572, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1554830861.195685}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=496.257062228 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:45.479] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 3, \"duration\": 4282, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:45 INFO 139661896189760] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] # Finished training epoch 4 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] Loss (name: value) total: 8.33137919108\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] Loss (name: value) kld: 0.163217860544\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] Loss (name: value) recons: 8.16816136893\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] Loss (name: value) logppx: 8.33137919108\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=8.33137919108\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:49.457] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 3, \"duration\": 4226, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] Loss (name: value) total: 8.66618816669\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] Loss (name: value) kld: 0.159680581093\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] Loss (name: value) recons: 8.50650756053\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] Loss (name: value) logppx: 8.66618816669\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] #validation_score (4): 8.66618816669\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] Timing: train: 3.98s, val: 0.25s, epoch: 4.22s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 284, \"sum\": 284.0, \"min\": 284}, \"Total Records Seen\": {\"count\": 1, \"max\": 8504, \"sum\": 8504.0, \"min\": 8504}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1554830869.703779, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1554830865.480008}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=503.307010442 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:49.704] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 4223, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:49 INFO 139661896189760] # Starting training for epoch 5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] # Finished training epoch 7 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] Loss (name: value) total: 8.13558810905\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] Loss (name: value) kld: 0.151424572644\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] Loss (name: value) recons: 7.98416351742\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] Loss (name: value) logppx: 8.13558810905\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] #quality_metric: host=algo-2, epoch=7, train total_loss <loss>=8.13558810905\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:47.640] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 6, \"duration\": 2093, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] Loss (name: value) total: 8.7008792975\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] Loss (name: value) kld: 0.168751443961\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] Loss (name: value) recons: 8.53212777162\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] Loss (name: value) logppx: 8.7008792975\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] #validation_score (7): 8.7008792975\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] patience losses:[8.8069233038486576, 8.7594950749323921, 8.7275391798753006, 8.712444207607172, 8.6812771136944118] min patience loss:8.68127711369 current loss:8.7008792975 absolute loss difference:0.0196021838066\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] Timing: train: 1.82s, val: 0.26s, epoch: 2.08s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] #progress_metric: host=algo-2, completed 4 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 252, \"sum\": 252.0, \"min\": 252}, \"Total Records Seen\": {\"count\": 1, \"max\": 7434, \"sum\": 7434.0, \"min\": 7434}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1554830867.897031, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1554830865.820579}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=511.408229488 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:47.897] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 7, \"duration\": 2076, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:47 INFO 139701951670080] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] # Finished training epoch 8 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] Loss (name: value) total: 8.11591263524\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] Loss (name: value) kld: 0.148449264412\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] Loss (name: value) recons: 7.96746345803\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] Loss (name: value) logppx: 8.11591263524\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] #quality_metric: host=algo-2, epoch=8, train total_loss <loss>=8.11591263524\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:49.642] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 7, \"duration\": 1999, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] Loss (name: value) total: 8.67689553285\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] Loss (name: value) kld: 0.167498292678\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] Loss (name: value) recons: 8.50939718393\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] Loss (name: value) logppx: 8.67689553285\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] #validation_score (8): 8.67689553285\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] patience losses:[8.7594950749323921, 8.7275391798753006, 8.712444207607172, 8.6812771136944118, 8.7008792975010021] min patience loss:8.68127711369 current loss:8.67689553285 absolute loss difference:0.00438158084185\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] Timing: train: 1.74s, val: 0.24s, epoch: 1.99s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] #progress_metric: host=algo-2, completed 5 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 288, \"sum\": 288.0, \"min\": 288}, \"Total Records Seen\": {\"count\": 1, \"max\": 8496, \"sum\": 8496.0, \"min\": 8496}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1554830869.887306, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1554830867.897464}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=533.656830575 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:49.887] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 1989, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:49 INFO 139701951670080] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] # Finished training epoch 9 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] Loss (name: value) total: 8.1152160362\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] Loss (name: value) kld: 0.150470509573\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] Loss (name: value) recons: 7.964745571\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] Loss (name: value) logppx: 8.1152160362\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] #quality_metric: host=algo-2, epoch=9, train total_loss <loss>=8.1152160362\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:51.696] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 2053, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] Loss (name: value) total: 8.70916110308\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] Loss (name: value) kld: 0.183828951762\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] Loss (name: value) recons: 8.5253321721\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] Loss (name: value) logppx: 8.70916110308\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] #validation_score (9): 8.70916110308\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] patience losses:[8.7275391798753006, 8.712444207607172, 8.6812771136944118, 8.7008792975010021, 8.6768955328525639] min patience loss:8.67689553285 current loss:8.70916110308 absolute loss difference:0.0322655702249\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] Timing: train: 1.81s, val: 0.24s, epoch: 2.05s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] #progress_metric: host=algo-2, completed 6 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 324, \"sum\": 324.0, \"min\": 324}, \"Total Records Seen\": {\"count\": 1, \"max\": 9558, \"sum\": 9558.0, \"min\": 9558}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1554830871.938717, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1554830869.887669}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=517.664442032 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:51.939] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 9, \"duration\": 2051, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:51 INFO 139701951670080] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] # Finished training epoch 10 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] Loss (name: value) total: 8.09737886499\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] Loss (name: value) kld: 0.147008015491\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] Loss (name: value) recons: 7.9503708239\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] Loss (name: value) logppx: 8.09737886499\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] #quality_metric: host=algo-2, epoch=10, train total_loss <loss>=8.09737886499\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:53.742] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 9, \"duration\": 2045, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] Loss (name: value) total: 8.66084403014\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] Loss (name: value) kld: 0.155570244789\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] Loss (name: value) recons: 8.505273594\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] Loss (name: value) logppx: 8.66084403014\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] #validation_score (10): 8.66084403014\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] patience losses:[8.712444207607172, 8.6812771136944118, 8.7008792975010021, 8.6768955328525639, 8.7091611030774239] min patience loss:8.67689553285 current loss:8.66084403014 absolute loss difference:0.0160515027168\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] Timing: train: 1.80s, val: 0.23s, epoch: 2.03s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] #progress_metric: host=algo-2, completed 6 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 360, \"sum\": 360.0, \"min\": 360}, \"Total Records Seen\": {\"count\": 1, \"max\": 10620, \"sum\": 10620.0, \"min\": 10620}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1554830873.967754, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1554830871.93982}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=523.633519733 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:53.967] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 2026, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:53 INFO 139701951670080] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] # Finished training epoch 5 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] Loss (name: value) total: 8.32247873942\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] Loss (name: value) kld: 0.161216248593\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] Loss (name: value) recons: 8.16126251221\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] Loss (name: value) logppx: 8.32247873942\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=8.32247873942\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:53.719] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 4, \"duration\": 4261, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] Loss (name: value) total: 8.67325580303\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] Loss (name: value) kld: 0.162867933665\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] Loss (name: value) recons: 8.51038794884\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] Loss (name: value) logppx: 8.67325580303\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] #validation_score (5): 8.67325580303\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] Timing: train: 4.02s, val: 0.26s, epoch: 4.27s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 355, \"sum\": 355.0, \"min\": 355}, \"Total Records Seen\": {\"count\": 1, \"max\": 10630, \"sum\": 10630.0, \"min\": 10630}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1554830873.976742, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1554830869.704303}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=497.589519533 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:53.976] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 4272, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:53 INFO 139661896189760] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] # Finished training epoch 11 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] Loss (name: value) total: 8.09543720528\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] Loss (name: value) kld: 0.142912058919\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] Loss (name: value) recons: 7.95252514592\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] Loss (name: value) logppx: 8.09543720528\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] #quality_metric: host=algo-2, epoch=11, train total_loss <loss>=8.09543720528\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:55.696] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 10, \"duration\": 1953, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] Loss (name: value) total: 8.67492840107\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] Loss (name: value) kld: 0.140305501987\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] Loss (name: value) recons: 8.53462293576\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] Loss (name: value) logppx: 8.67492840107\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] #validation_score (11): 8.67492840107\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] patience losses:[8.6812771136944118, 8.7008792975010021, 8.6768955328525639, 8.7091611030774239, 8.660844030135717] min patience loss:8.66084403014 current loss:8.67492840107 absolute loss difference:0.014084370931\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] Timing: train: 1.73s, val: 0.25s, epoch: 1.98s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] #progress_metric: host=algo-2, completed 7 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 396, \"sum\": 396.0, \"min\": 396}, \"Total Records Seen\": {\"count\": 1, \"max\": 11682, \"sum\": 11682.0, \"min\": 11682}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1554830875.948184, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1554830873.968046}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=536.286979306 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:55.948] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 1980, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:55 INFO 139701951670080] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] # Finished training epoch 12 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] Loss (name: value) total: 8.08595200291\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] Loss (name: value) kld: 0.138739590292\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] Loss (name: value) recons: 7.94721236052\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] Loss (name: value) logppx: 8.08595200291\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] #quality_metric: host=algo-2, epoch=12, train total_loss <loss>=8.08595200291\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:58.071] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 2375, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] # Finished training epoch 6 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] Loss (name: value) total: 8.30861745485\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] Loss (name: value) kld: 0.154545439857\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] Loss (name: value) recons: 8.15407204068\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] Loss (name: value) logppx: 8.30861745485\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=8.30861745485\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:58.039] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 4319, \"num_examples\": 14}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] Loss (name: value) total: 8.65798648932\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] Loss (name: value) kld: 0.162592315674\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] Loss (name: value) recons: 8.49539427146\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] Loss (name: value) logppx: 8.65798648932\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] #validation_score (12): 8.65798648932\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] patience losses:[8.7008792975010021, 8.6768955328525639, 8.7091611030774239, 8.660844030135717, 8.6749284010667065] min patience loss:8.66084403014 current loss:8.65798648932 absolute loss difference:0.0028575408153\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] Timing: train: 2.12s, val: 0.23s, epoch: 2.36s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] #progress_metric: host=algo-2, completed 8 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 432, \"sum\": 432.0, \"min\": 432}, \"Total Records Seen\": {\"count\": 1, \"max\": 12744, \"sum\": 12744.0, \"min\": 12744}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1554830878.306155, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1554830875.948447}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=450.412412202 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:27:58.306] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 2357, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:27:58 INFO 139701951670080] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] Loss (name: value) total: 8.65582561004\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] Loss (name: value) kld: 0.159970848988\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] Loss (name: value) recons: 8.49585461739\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] Loss (name: value) logppx: 8.65582561004\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] #validation_score (6): 8.65582561004\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] patience losses:[8.8077061188526642, 8.7188919849884812, 8.6810677161583527, 8.6661881666917076, 8.6732558030348557] min patience loss:8.66618816669 current loss:8.65582561004 absolute loss difference:0.0103625566531\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] Timing: train: 4.06s, val: 0.25s, epoch: 4.31s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 426, \"sum\": 426.0, \"min\": 426}, \"Total Records Seen\": {\"count\": 1, \"max\": 12756, \"sum\": 12756.0, \"min\": 12756}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1554830878.289106, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1554830873.977073}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=493.021186537 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:27:58.289] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 4311, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:27:58 INFO 139661896189760] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] # Finished training epoch 13 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] Loss (name: value) total: 8.08806517566\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] Loss (name: value) kld: 0.140769922071\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] Loss (name: value) recons: 7.94729523129\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] Loss (name: value) logppx: 8.08806517566\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] #quality_metric: host=algo-2, epoch=13, train total_loss <loss>=8.08806517566\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:00.030] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 12, \"duration\": 1958, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] Loss (name: value) total: 8.66575501271\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] Loss (name: value) kld: 0.130205571346\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] Loss (name: value) recons: 8.53554949638\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] Loss (name: value) logppx: 8.66575501271\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] #validation_score (13): 8.66575501271\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] patience losses:[8.6768955328525639, 8.7091611030774239, 8.660844030135717, 8.6749284010667065, 8.6579864893204128] min patience loss:8.65798648932 current loss:8.66575501271 absolute loss difference:0.00776852338742\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] Timing: train: 1.72s, val: 0.26s, epoch: 1.98s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] #progress_metric: host=algo-2, completed 8 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 468, \"sum\": 468.0, \"min\": 468}, \"Total Records Seen\": {\"count\": 1, \"max\": 13806, \"sum\": 13806.0, \"min\": 13806}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1554830880.286275, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1554830878.306408}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=536.362856173 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:00.286] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 13, \"duration\": 1978, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:00 INFO 139701951670080] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] # Finished training epoch 14 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] Loss (name: value) total: 8.07452323348\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] Loss (name: value) kld: 0.138289086686\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] Loss (name: value) recons: 7.93623417748\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] Loss (name: value) logppx: 8.07452323348\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] #quality_metric: host=algo-2, epoch=14, train total_loss <loss>=8.07452323348\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:02.376] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 13, \"duration\": 2346, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] Loss (name: value) total: 8.65332375551\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] Loss (name: value) kld: 0.14758424331\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] Loss (name: value) recons: 8.50573973044\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] Loss (name: value) logppx: 8.65332375551\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] #validation_score (14): 8.65332375551\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] patience losses:[8.7091611030774239, 8.660844030135717, 8.6749284010667065, 8.6579864893204128, 8.665755012707832] min patience loss:8.65798648932 current loss:8.65332375551 absolute loss difference:0.0046627338116\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] Timing: train: 2.09s, val: 0.26s, epoch: 2.35s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] #progress_metric: host=algo-2, completed 9 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 504, \"sum\": 504.0, \"min\": 504}, \"Total Records Seen\": {\"count\": 1, \"max\": 14868, \"sum\": 14868.0, \"min\": 14868}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1554830882.638068, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1554830880.286532}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=451.593190179 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:02.638] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 2351, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:02 INFO 139701951670080] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] # Finished training epoch 7 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] Loss (name: value) total: 8.28805798454\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] Loss (name: value) kld: 0.15069304839\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] Loss (name: value) recons: 8.13736489166\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] Loss (name: value) logppx: 8.28805798454\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=8.28805798454\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:02.297] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 6, \"duration\": 4257, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] Loss (name: value) total: 8.64349791698\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] Loss (name: value) kld: 0.144271925779\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] Loss (name: value) recons: 8.49922590989\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] Loss (name: value) logppx: 8.64349791698\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] #validation_score (7): 8.64349791698\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] patience losses:[8.7188919849884812, 8.6810677161583527, 8.6661881666917076, 8.6732558030348557, 8.655825610038562] min patience loss:8.65582561004 current loss:8.64349791698 absolute loss difference:0.0123276930589\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] Timing: train: 4.01s, val: 0.25s, epoch: 4.26s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 497, \"sum\": 497.0, \"min\": 497}, \"Total Records Seen\": {\"count\": 1, \"max\": 14882, \"sum\": 14882.0, \"min\": 14882}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1554830882.55153, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1554830878.289399}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=498.79184207 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:02.551] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 7, \"duration\": 4262, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:02 INFO 139661896189760] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] # Finished training epoch 15 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] Loss (name: value) total: 8.07385484907\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] Loss (name: value) kld: 0.13758242351\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] Loss (name: value) recons: 7.93627236684\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] Loss (name: value) logppx: 8.07385484907\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] #quality_metric: host=algo-2, epoch=15, train total_loss <loss>=8.07385484907\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:04.603] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 14, \"duration\": 2226, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] Loss (name: value) total: 8.6704114865\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] Loss (name: value) kld: 0.148613983546\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] Loss (name: value) recons: 8.52179764968\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] Loss (name: value) logppx: 8.6704114865\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] #validation_score (15): 8.6704114865\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] patience losses:[8.660844030135717, 8.6749284010667065, 8.6579864893204128, 8.665755012707832, 8.6533237555088149] min patience loss:8.65332375551 current loss:8.6704114865 absolute loss difference:0.0170877309946\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] Timing: train: 1.97s, val: 0.27s, epoch: 2.24s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] #progress_metric: host=algo-2, completed 10 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 540, \"sum\": 540.0, \"min\": 540}, \"Total Records Seen\": {\"count\": 1, \"max\": 15930, \"sum\": 15930.0, \"min\": 15930}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1554830884.877776, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1554830882.638329}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=474.194154117 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:04.878] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 15, \"duration\": 2239, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:04 INFO 139701951670080] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:06 INFO 139701951670080] # Finished training epoch 16 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:06 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:06 INFO 139701951670080] Loss (name: value) total: 8.06297248558\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:06 INFO 139701951670080] Loss (name: value) kld: 0.134471244724\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:06 INFO 139701951670080] Loss (name: value) recons: 7.92850123511\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:06 INFO 139701951670080] Loss (name: value) logppx: 8.06297248558\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:06 INFO 139701951670080] #quality_metric: host=algo-2, epoch=16, train total_loss <loss>=8.06297248558\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:06.863] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 15, \"duration\": 2259, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:07 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:07 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:07 INFO 139701951670080] Loss (name: value) total: 8.64558367607\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:07 INFO 139701951670080] Loss (name: value) kld: 0.15183512003\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:07 INFO 139701951670080] Loss (name: value) recons: 8.49374870887\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:07 INFO 139701951670080] Loss (name: value) logppx: 8.64558367607\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:07 INFO 139701951670080] #validation_score (16): 8.64558367607\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:07 INFO 139701951670080] patience losses:[8.6749284010667065, 8.6579864893204128, 8.665755012707832, 8.6533237555088149, 8.6704114865034061] min patience loss:8.65332375551 current loss:8.64558367607 absolute loss difference:0.0077400794396\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:07 INFO 139701951670080] Timing: train: 1.98s, val: 0.26s, epoch: 2.25s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:07 INFO 139701951670080] #progress_metric: host=algo-2, completed 10 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 576, \"sum\": 576.0, \"min\": 576}, \"Total Records Seen\": {\"count\": 1, \"max\": 16992, \"sum\": 16992.0, \"min\": 16992}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1554830887.128315, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1554830884.878115}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:07 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=471.917539806 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:07.128] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 2250, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:07 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:07 INFO 139701951670080] # Starting training for epoch 17\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] # Finished training epoch 8 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] Loss (name: value) total: 8.28090968602\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] Loss (name: value) kld: 0.150343493509\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] Loss (name: value) recons: 8.13056624865\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] Loss (name: value) logppx: 8.28090968602\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=8.28090968602\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:06.624] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 7, \"duration\": 4326, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] Loss (name: value) total: 8.63628191826\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] Loss (name: value) kld: 0.137741933725\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] Loss (name: value) recons: 8.49854012514\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] Loss (name: value) logppx: 8.63628191826\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] #validation_score (8): 8.63628191826\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] patience losses:[8.6810677161583527, 8.6661881666917076, 8.6732558030348557, 8.655825610038562, 8.6434979169796673] min patience loss:8.64349791698 current loss:8.63628191826 absolute loss difference:0.00721599872296\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] Timing: train: 4.07s, val: 0.27s, epoch: 4.34s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 568, \"sum\": 568.0, \"min\": 568}, \"Total Records Seen\": {\"count\": 1, \"max\": 17008, \"sum\": 17008.0, \"min\": 17008}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1554830886.896299, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1554830882.551839}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=489.341834709 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:06.896] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 4344, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:06 INFO 139661896189760] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] # Finished training epoch 17 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] Loss (name: value) total: 8.07042049832\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] Loss (name: value) kld: 0.134954653184\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] Loss (name: value) recons: 7.93546581975\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] Loss (name: value) logppx: 8.07042049832\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] #quality_metric: host=algo-2, epoch=17, train total_loss <loss>=8.07042049832\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:09.234] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 16, \"duration\": 2371, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] Loss (name: value) total: 8.66484476725\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] Loss (name: value) kld: 0.138730791899\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] Loss (name: value) recons: 8.52611396985\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] Loss (name: value) logppx: 8.66484476725\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] #validation_score (17): 8.66484476725\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] patience losses:[8.6579864893204128, 8.665755012707832, 8.6533237555088149, 8.6704114865034061, 8.6455836760692115] min patience loss:8.64558367607 current loss:8.66484476725 absolute loss difference:0.0192610911834\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] Timing: train: 2.11s, val: 0.24s, epoch: 2.34s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] #progress_metric: host=algo-2, completed 11 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 612, \"sum\": 612.0, \"min\": 612}, \"Total Records Seen\": {\"count\": 1, \"max\": 18054, \"sum\": 18054.0, \"min\": 18054}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1554830889.471495, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 16}, \"StartTime\": 1554830887.128707}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=453.2735442 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:09.471] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 2342, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:09 INFO 139701951670080] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] # Finished training epoch 9 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] Loss (name: value) total: 8.26385336146\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] Loss (name: value) kld: 0.14922887932\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] Loss (name: value) recons: 8.11462446759\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] Loss (name: value) logppx: 8.26385336146\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=8.26385336146\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:11.007] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 4381, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] Loss (name: value) total: 8.6370113275\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] Loss (name: value) kld: 0.151062379739\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] Loss (name: value) recons: 8.48594892453\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] Loss (name: value) logppx: 8.6370113275\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] #validation_score (9): 8.6370113275\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] patience losses:[8.6661881666917076, 8.6732558030348557, 8.655825610038562, 8.6434979169796673, 8.6362819182567101] min patience loss:8.63628191826 current loss:8.6370113275 absolute loss difference:0.000729409242288\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] Timing: train: 4.11s, val: 0.26s, epoch: 4.37s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 639, \"sum\": 639.0, \"min\": 639}, \"Total Records Seen\": {\"count\": 1, \"max\": 19134, \"sum\": 19134.0, \"min\": 19134}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1554830891.266494, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1554830886.89658}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=486.488623418 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:11.266] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 9, \"duration\": 4369, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:11 INFO 139661896189760] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] # Finished training epoch 18 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] Loss (name: value) total: 8.0539758541\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] Loss (name: value) kld: 0.134487805102\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] Loss (name: value) recons: 7.91948812273\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] Loss (name: value) logppx: 8.0539758541\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] #quality_metric: host=algo-2, epoch=18, train total_loss <loss>=8.0539758541\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:11.281] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 17, \"duration\": 2047, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] Loss (name: value) total: 8.62959242601\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] Loss (name: value) kld: 0.137163077868\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] Loss (name: value) recons: 8.49242933224\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] Loss (name: value) logppx: 8.62959242601\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] #validation_score (18): 8.62959242601\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] patience losses:[8.665755012707832, 8.6533237555088149, 8.6704114865034061, 8.6455836760692115, 8.664844767252605] min patience loss:8.64558367607 current loss:8.62959242601 absolute loss difference:0.0159912500626\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] Timing: train: 1.81s, val: 0.25s, epoch: 2.06s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] #progress_metric: host=algo-2, completed 12 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 648, \"sum\": 648.0, \"min\": 648}, \"Total Records Seen\": {\"count\": 1, \"max\": 19116, \"sum\": 19116.0, \"min\": 19116}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1554830891.529569, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 17}, \"StartTime\": 1554830889.471758}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=515.980865962 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:11.530] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 2057, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:11 INFO 139701951670080] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] # Finished training epoch 19 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] Loss (name: value) total: 8.05430407348\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] Loss (name: value) kld: 0.134638032428\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] Loss (name: value) recons: 7.91966601478\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] Loss (name: value) logppx: 8.05430407348\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] #quality_metric: host=algo-2, epoch=19, train total_loss <loss>=8.05430407348\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:13.359] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 18, \"duration\": 2076, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] Loss (name: value) total: 8.67380402394\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] Loss (name: value) kld: 0.142999588526\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] Loss (name: value) recons: 8.53080424773\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] Loss (name: value) logppx: 8.67380402394\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] #validation_score (19): 8.67380402394\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] patience losses:[8.6533237555088149, 8.6704114865034061, 8.6455836760692115, 8.664844767252605, 8.6295924260066101] min patience loss:8.62959242601 current loss:8.67380402394 absolute loss difference:0.0442115979317\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] Timing: train: 1.83s, val: 0.25s, epoch: 2.08s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] #progress_metric: host=algo-2, completed 12 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 684, \"sum\": 684.0, \"min\": 684}, \"Total Records Seen\": {\"count\": 1, \"max\": 20178, \"sum\": 20178.0, \"min\": 20178}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1554830893.613732, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 18}, \"StartTime\": 1554830891.530264}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=509.67935158 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:13.613] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 19, \"duration\": 2083, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:13 INFO 139701951670080] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] # Finished training epoch 20 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] Loss (name: value) total: 8.04054815504\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] Loss (name: value) kld: 0.135875302774\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] Loss (name: value) recons: 7.9046728558\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] Loss (name: value) logppx: 8.04054815504\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] #quality_metric: host=algo-2, epoch=20, train total_loss <loss>=8.04054815504\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:15.380] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 19, \"duration\": 2021, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] Loss (name: value) total: 8.63991816594\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] Loss (name: value) kld: 0.145881984784\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] Loss (name: value) recons: 8.49403623923\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] Loss (name: value) logppx: 8.63991816594\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] #validation_score (20): 8.63991816594\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] patience losses:[8.6704114865034061, 8.6455836760692115, 8.664844767252605, 8.6295924260066101, 8.6738040239383007] min patience loss:8.62959242601 current loss:8.63991816594 absolute loss difference:0.0103257399339\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] Timing: train: 1.77s, val: 0.23s, epoch: 2.00s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] #progress_metric: host=algo-2, completed 13 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 720, \"sum\": 720.0, \"min\": 720}, \"Total Records Seen\": {\"count\": 1, \"max\": 21240, \"sum\": 21240.0, \"min\": 21240}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1554830895.612093, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 19}, \"StartTime\": 1554830893.614126}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=531.480692865 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:15.612] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 1997, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:15 INFO 139701951670080] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] # Finished training epoch 21 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] Loss (name: value) total: 8.04954781002\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] Loss (name: value) kld: 0.132195528569\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] Loss (name: value) recons: 7.91735223841\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] Loss (name: value) logppx: 8.04954781002\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] #quality_metric: host=algo-2, epoch=21, train total_loss <loss>=8.04954781002\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:17.548] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 20, \"duration\": 2167, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] Loss (name: value) total: 8.64934418507\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] Loss (name: value) kld: 0.136677654584\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] Loss (name: value) recons: 8.51266651643\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] Loss (name: value) logppx: 8.64934418507\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] #validation_score (21): 8.64934418507\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] patience losses:[8.6455836760692115, 8.664844767252605, 8.6295924260066101, 8.6738040239383007, 8.6399181659405055] min patience loss:8.62959242601 current loss:8.64934418507 absolute loss difference:0.0197517590645\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] Timing: train: 1.94s, val: 0.25s, epoch: 2.19s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] #progress_metric: host=algo-2, completed 14 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 756, \"sum\": 756.0, \"min\": 756}, \"Total Records Seen\": {\"count\": 1, \"max\": 22302, \"sum\": 22302.0, \"min\": 22302}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1554830897.804036, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 20}, \"StartTime\": 1554830895.612382}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=484.531495058 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:17.804] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 21, \"duration\": 2189, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:17 INFO 139701951670080] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] # Finished training epoch 22 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] Loss (name: value) total: 8.03772237566\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] Loss (name: value) kld: 0.13641194657\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] Loss (name: value) recons: 7.90131047567\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] Loss (name: value) logppx: 8.03772237566\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] #quality_metric: host=algo-2, epoch=22, train total_loss <loss>=8.03772237566\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:19.605] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 21, \"duration\": 2053, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] Loss (name: value) total: 8.63033975454\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] Loss (name: value) kld: 0.135515651947\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] Loss (name: value) recons: 8.49482410137\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] Loss (name: value) logppx: 8.63033975454\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] #validation_score (22): 8.63033975454\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] patience losses:[8.664844767252605, 8.6295924260066101, 8.6738040239383007, 8.6399181659405055, 8.6493441850711132] min patience loss:8.62959242601 current loss:8.63033975454 absolute loss difference:0.000747328538161\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] Timing: train: 1.80s, val: 0.24s, epoch: 2.04s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] #progress_metric: host=algo-2, completed 14 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 792, \"sum\": 792.0, \"min\": 792}, \"Total Records Seen\": {\"count\": 1, \"max\": 23364, \"sum\": 23364.0, \"min\": 23364}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1554830899.843278, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 21}, \"StartTime\": 1554830897.804339}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=520.809865471 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:19.843] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 2038, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:19 INFO 139701951670080] # Starting training for epoch 23\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] # Finished training epoch 10 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] Loss (name: value) total: 8.2641465308\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] Loss (name: value) kld: 0.146514151802\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] Loss (name: value) recons: 8.1176324119\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] Loss (name: value) logppx: 8.2641465308\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=8.2641465308\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:15.306] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 9, \"duration\": 4298, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] Loss (name: value) total: 8.64282833002\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] Loss (name: value) kld: 0.140369005692\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] Loss (name: value) recons: 8.5024592473\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] Loss (name: value) logppx: 8.64282833002\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] #validation_score (10): 8.64282833002\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] patience losses:[8.6732558030348557, 8.655825610038562, 8.6434979169796673, 8.6362819182567101, 8.6370113274989979] min patience loss:8.63628191826 current loss:8.64282833002 absolute loss difference:0.00654641175881\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] Timing: train: 4.04s, val: 0.24s, epoch: 4.28s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 710, \"sum\": 710.0, \"min\": 710}, \"Total Records Seen\": {\"count\": 1, \"max\": 21260, \"sum\": 21260.0, \"min\": 21260}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1554830895.549192, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1554830891.266876}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=496.440595614 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:15.549] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 4282, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:15 INFO 139661896189760] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] # Finished training epoch 11 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] Loss (name: value) total: 8.25585000481\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] Loss (name: value) kld: 0.150118097173\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] Loss (name: value) recons: 8.10573190322\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] Loss (name: value) logppx: 8.25585000481\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=8.25585000481\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:19.392] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 10, \"duration\": 4086, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] Loss (name: value) total: 8.62494878524\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] Loss (name: value) kld: 0.130712359991\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] Loss (name: value) recons: 8.49423655975\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] Loss (name: value) logppx: 8.62494878524\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] #validation_score (11): 8.62494878524\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] patience losses:[8.655825610038562, 8.6434979169796673, 8.6362819182567101, 8.6370113274989979, 8.642828330015524] min patience loss:8.63628191826 current loss:8.62494878524 absolute loss difference:0.0113331330128\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] Timing: train: 3.84s, val: 0.24s, epoch: 4.08s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 781, \"sum\": 781.0, \"min\": 781}, \"Total Records Seen\": {\"count\": 1, \"max\": 23386, \"sum\": 23386.0, \"min\": 23386}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1554830899.631051, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1554830895.549497}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=520.858474016 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:19.631] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 4081, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:19 INFO 139661896189760] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] # Finished training epoch 23 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] Loss (name: value) total: 8.03699004562\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] Loss (name: value) kld: 0.138519694408\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] Loss (name: value) recons: 7.89847034878\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] Loss (name: value) logppx: 8.03699004562\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] #quality_metric: host=algo-2, epoch=23, train total_loss <loss>=8.03699004562\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:21.699] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 22, \"duration\": 2093, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] Loss (name: value) total: 8.653514999\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] Loss (name: value) kld: 0.141872566786\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] Loss (name: value) recons: 8.51164241693\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] Loss (name: value) logppx: 8.653514999\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] #validation_score (23): 8.653514999\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] patience losses:[8.6295924260066101, 8.6738040239383007, 8.6399181659405055, 8.6493441850711132, 8.6303397545447709] min patience loss:8.62959242601 current loss:8.653514999 absolute loss difference:0.0239225729918\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] Timing: train: 1.86s, val: 0.27s, epoch: 2.12s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] #progress_metric: host=algo-2, completed 15 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 828, \"sum\": 828.0, \"min\": 828}, \"Total Records Seen\": {\"count\": 1, \"max\": 24426, \"sum\": 24426.0, \"min\": 24426}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1554830901.965721, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 22}, \"StartTime\": 1554830899.843652}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=500.420543488 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:21.965] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 2121, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:21 INFO 139701951670080] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:23 INFO 139701951670080] # Finished training epoch 24 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:23 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:23 INFO 139701951670080] Loss (name: value) total: 8.02813900135\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:23 INFO 139701951670080] Loss (name: value) kld: 0.134694853094\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:23 INFO 139701951670080] Loss (name: value) recons: 7.89344416018\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:23 INFO 139701951670080] Loss (name: value) logppx: 8.02813900135\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:23 INFO 139701951670080] #quality_metric: host=algo-2, epoch=24, train total_loss <loss>=8.02813900135\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:23.849] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 23, \"duration\": 2150, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:24 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:24 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:24 INFO 139701951670080] Loss (name: value) total: 8.61253446921\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:24 INFO 139701951670080] Loss (name: value) kld: 0.136201731975\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:24 INFO 139701951670080] Loss (name: value) recons: 8.47633271829\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:24 INFO 139701951670080] Loss (name: value) logppx: 8.61253446921\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:24 INFO 139701951670080] #validation_score (24): 8.61253446921\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:24 INFO 139701951670080] patience losses:[8.6738040239383007, 8.6399181659405055, 8.6493441850711132, 8.6303397545447709, 8.6535149989983982] min patience loss:8.63033975454 current loss:8.61253446921 absolute loss difference:0.0178052853315\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:24 INFO 139701951670080] Timing: train: 1.88s, val: 0.24s, epoch: 2.13s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:24 INFO 139701951670080] #progress_metric: host=algo-2, completed 16 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 864, \"sum\": 864.0, \"min\": 864}, \"Total Records Seen\": {\"count\": 1, \"max\": 25488, \"sum\": 25488.0, \"min\": 25488}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1554830904.09155, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 23}, \"StartTime\": 1554830901.965984}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:24 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=499.598955703 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:24.091] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 2125, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:24 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:24 INFO 139701951670080] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] # Finished training epoch 12 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] Loss (name: value) total: 8.24088449971\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] Loss (name: value) kld: 0.151970355388\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] Loss (name: value) recons: 8.08891416021\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] Loss (name: value) logppx: 8.24088449971\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=8.24088449971\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:23.356] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 3963, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] Loss (name: value) total: 8.60992239928\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] Loss (name: value) kld: 0.12943144945\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] Loss (name: value) recons: 8.48049097306\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] Loss (name: value) logppx: 8.60992239928\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] #validation_score (12): 8.60992239928\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] patience losses:[8.6434979169796673, 8.6362819182567101, 8.6370113274989979, 8.642828330015524, 8.6249487852438911] min patience loss:8.62494878524 current loss:8.60992239928 absolute loss difference:0.0150263859675\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] Timing: train: 3.73s, val: 0.24s, epoch: 3.97s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 852, \"sum\": 852.0, \"min\": 852}, \"Total Records Seen\": {\"count\": 1, \"max\": 25512, \"sum\": 25512.0, \"min\": 25512}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1554830903.597485, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1554830899.631436}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=536.027693542 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:23.597] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 3965, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:23 INFO 139661896189760] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:25 INFO 139701951670080] # Finished training epoch 25 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:25 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:25 INFO 139701951670080] Loss (name: value) total: 8.02341462594\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:25 INFO 139701951670080] Loss (name: value) kld: 0.138618864837\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:25 INFO 139701951670080] Loss (name: value) recons: 7.88479576817\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:25 INFO 139701951670080] Loss (name: value) logppx: 8.02341462594\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:25 INFO 139701951670080] #quality_metric: host=algo-2, epoch=25, train total_loss <loss>=8.02341462594\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:25.937] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 24, \"duration\": 2087, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:26 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:26 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:26 INFO 139701951670080] Loss (name: value) total: 8.6685534355\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:26 INFO 139701951670080] Loss (name: value) kld: 0.151127983974\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:26 INFO 139701951670080] Loss (name: value) recons: 8.51742541973\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:26 INFO 139701951670080] Loss (name: value) logppx: 8.6685534355\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:26 INFO 139701951670080] #validation_score (25): 8.6685534355\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:26 INFO 139701951670080] patience losses:[8.6399181659405055, 8.6493441850711132, 8.6303397545447709, 8.6535149989983982, 8.6125344692132408] min patience loss:8.61253446921 current loss:8.6685534355 absolute loss difference:0.0560189662836\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:26 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:26 INFO 139701951670080] Timing: train: 1.85s, val: 0.27s, epoch: 2.12s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:26 INFO 139701951670080] #progress_metric: host=algo-2, completed 16 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 900, \"sum\": 900.0, \"min\": 900}, \"Total Records Seen\": {\"count\": 1, \"max\": 26550, \"sum\": 26550.0, \"min\": 26550}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1554830906.212742, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 24}, \"StartTime\": 1554830904.091812}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:26 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=500.689979177 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:26.212] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 25, \"duration\": 2120, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:26 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:26 INFO 139701951670080] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] # Finished training epoch 26 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] Loss (name: value) total: 7.99718470397\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] Loss (name: value) kld: 0.140457214912\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] Loss (name: value) recons: 7.85672750826\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] Loss (name: value) logppx: 7.99718470397\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] #quality_metric: host=algo-2, epoch=26, train total_loss <loss>=7.99718470397\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:28.154] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 25, \"duration\": 2216, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] # Finished training epoch 13 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] Loss (name: value) total: 8.22718152686\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] Loss (name: value) kld: 0.154837770529\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] Loss (name: value) recons: 8.07234378868\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] Loss (name: value) logppx: 8.22718152686\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=8.22718152686\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:27.393] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 12, \"duration\": 4035, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] Loss (name: value) total: 8.61625350561\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] Loss (name: value) kld: 0.125316660221\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] Loss (name: value) recons: 8.49093682705\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] Loss (name: value) logppx: 8.61625350561\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] #validation_score (13): 8.61625350561\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] patience losses:[8.6362819182567101, 8.6370113274989979, 8.642828330015524, 8.6249487852438911, 8.6099223992763427] min patience loss:8.60992239928 current loss:8.61625350561 absolute loss difference:0.00633110633263\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] Timing: train: 3.80s, val: 0.26s, epoch: 4.06s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 923, \"sum\": 923.0, \"min\": 923}, \"Total Records Seen\": {\"count\": 1, \"max\": 27638, \"sum\": 27638.0, \"min\": 27638}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1554830907.654792, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1554830903.597812}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=524.015609243 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:27.655] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 13, \"duration\": 4056, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:27 INFO 139661896189760] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] Loss (name: value) total: 8.61640668038\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] Loss (name: value) kld: 0.138821552961\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] Loss (name: value) recons: 8.47758530837\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] Loss (name: value) logppx: 8.61640668038\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] #validation_score (26): 8.61640668038\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] patience losses:[8.6493441850711132, 8.6303397545447709, 8.6535149989983982, 8.6125344692132408, 8.6685534354967952] min patience loss:8.61253446921 current loss:8.61640668038 absolute loss difference:0.00387221116286\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] Timing: train: 1.94s, val: 0.29s, epoch: 2.23s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] #progress_metric: host=algo-2, completed 17 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 936, \"sum\": 936.0, \"min\": 936}, \"Total Records Seen\": {\"count\": 1, \"max\": 27612, \"sum\": 27612.0, \"min\": 27612}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}}, \"EndTime\": 1554830908.444198, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 25}, \"StartTime\": 1554830906.213067}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=475.952288847 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:28.444] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 2231, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:28 INFO 139701951670080] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] # Finished training epoch 27 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] Loss (name: value) total: 7.98455602151\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] Loss (name: value) kld: 0.145338935101\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] Loss (name: value) recons: 7.83921705175\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] Loss (name: value) logppx: 7.98455602151\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] #quality_metric: host=algo-2, epoch=27, train total_loss <loss>=7.98455602151\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:30.467] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 26, \"duration\": 2312, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] Loss (name: value) total: 8.62628522042\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] Loss (name: value) kld: 0.120459802945\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] Loss (name: value) recons: 8.50582541441\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] Loss (name: value) logppx: 8.62628522042\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] #validation_score (27): 8.62628522042\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] patience losses:[8.6303397545447709, 8.6535149989983982, 8.6125344692132408, 8.6685534354967952, 8.6164066803761017] min patience loss:8.61253446921 current loss:8.62628522042 absolute loss difference:0.0137507512019\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] Timing: train: 2.02s, val: 0.27s, epoch: 2.29s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] #progress_metric: host=algo-2, completed 18 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 972, \"sum\": 972.0, \"min\": 972}, \"Total Records Seen\": {\"count\": 1, \"max\": 28674, \"sum\": 28674.0, \"min\": 28674}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}}, \"EndTime\": 1554830910.732384, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 26}, \"StartTime\": 1554830908.444483}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=464.151531142 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:30.732] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 27, \"duration\": 2287, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:30 INFO 139701951670080] # Starting training for epoch 28\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] # Finished training epoch 14 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] Loss (name: value) total: 8.20477106515\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] Loss (name: value) kld: 0.158357581062\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] Loss (name: value) recons: 8.04641348969\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] Loss (name: value) logppx: 8.20477106515\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=8.20477106515\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:31.689] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 13, \"duration\": 4295, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] Loss (name: value) total: 8.58860442333\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] Loss (name: value) kld: 0.144940068783\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] Loss (name: value) recons: 8.44366427691\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] Loss (name: value) logppx: 8.58860442333\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] #validation_score (14): 8.58860442333\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] patience losses:[8.6370113274989979, 8.642828330015524, 8.6249487852438911, 8.6099223992763427, 8.6162535056089737] min patience loss:8.60992239928 current loss:8.58860442333 absolute loss difference:0.021317975949\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] Timing: train: 4.03s, val: 0.25s, epoch: 4.29s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 994, \"sum\": 994.0, \"min\": 994}, \"Total Records Seen\": {\"count\": 1, \"max\": 29764, \"sum\": 29764.0, \"min\": 29764}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1554830911.943629, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1554830907.65506}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=495.718794368 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:31.943] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 4288, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:31 INFO 139661896189760] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] # Finished training epoch 28 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] Loss (name: value) total: 7.95565764816\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] Loss (name: value) kld: 0.14912507821\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] Loss (name: value) recons: 7.80653262668\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] Loss (name: value) logppx: 7.95565764816\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] #quality_metric: host=algo-2, epoch=28, train total_loss <loss>=7.95565764816\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:32.695] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 27, \"duration\": 2227, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] Loss (name: value) total: 8.59100185296\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] Loss (name: value) kld: 0.129519241895\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] Loss (name: value) recons: 8.4614827474\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] Loss (name: value) logppx: 8.59100185296\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] #validation_score (28): 8.59100185296\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] patience losses:[8.6535149989983982, 8.6125344692132408, 8.6685534354967952, 8.6164066803761017, 8.6262852204151645] min patience loss:8.61253446921 current loss:8.59100185296 absolute loss difference:0.0215326162485\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] Timing: train: 1.96s, val: 0.28s, epoch: 2.24s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] #progress_metric: host=algo-2, completed 18 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1008, \"sum\": 1008.0, \"min\": 1008}, \"Total Records Seen\": {\"count\": 1, \"max\": 29736, \"sum\": 29736.0, \"min\": 29736}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}}, \"EndTime\": 1554830912.97256, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 27}, \"StartTime\": 1554830910.73271}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=474.058651357 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:32.973] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 2240, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:32 INFO 139701951670080] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] # Finished training epoch 29 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] Loss (name: value) total: 7.94040311884\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] Loss (name: value) kld: 0.151202690822\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] Loss (name: value) recons: 7.78920042956\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] Loss (name: value) logppx: 7.94040311884\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] #quality_metric: host=algo-2, epoch=29, train total_loss <loss>=7.94040311884\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:35.065] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 28, \"duration\": 2370, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] Loss (name: value) total: 8.62965987768\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] Loss (name: value) kld: 0.135773626963\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] Loss (name: value) recons: 8.49388635097\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] Loss (name: value) logppx: 8.62965987768\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] #validation_score (29): 8.62965987768\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] patience losses:[8.6125344692132408, 8.6685534354967952, 8.6164066803761017, 8.6262852204151645, 8.5910018529647427] min patience loss:8.59100185296 current loss:8.62965987768 absolute loss difference:0.0386580247145\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] Timing: train: 2.09s, val: 0.25s, epoch: 2.34s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] #progress_metric: host=algo-2, completed 19 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1044, \"sum\": 1044.0, \"min\": 1044}, \"Total Records Seen\": {\"count\": 1, \"max\": 30798, \"sum\": 30798.0, \"min\": 30798}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 58, \"sum\": 58.0, \"min\": 58}}, \"EndTime\": 1554830915.318466, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 28}, \"StartTime\": 1554830912.973358}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=452.823626789 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:35.318] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 2342, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:35 INFO 139701951670080] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] # Finished training epoch 15 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] Loss (name: value) total: 8.17632889725\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] Loss (name: value) kld: 0.161822005337\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] Loss (name: value) recons: 8.01450686746\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] Loss (name: value) logppx: 8.17632889725\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=8.17632889725\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:36.123] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 14, \"duration\": 4433, \"num_examples\": 14}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] Loss (name: value) total: 8.58119182098\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] Loss (name: value) kld: 0.138855264126\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] Loss (name: value) recons: 8.44233644926\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] Loss (name: value) logppx: 8.58119182098\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] #validation_score (15): 8.58119182098\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] patience losses:[8.642828330015524, 8.6249487852438911, 8.6099223992763427, 8.6162535056089737, 8.5886044233273235] min patience loss:8.58860442333 current loss:8.58119182098 absolute loss difference:0.00741260235126\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] Timing: train: 4.18s, val: 0.27s, epoch: 4.45s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1065, \"sum\": 1065.0, \"min\": 1065}, \"Total Records Seen\": {\"count\": 1, \"max\": 31890, \"sum\": 31890.0, \"min\": 31890}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1554830916.393397, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1554830911.943933}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=477.793941246 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:36.393] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 15, \"duration\": 4448, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:36 INFO 139661896189760] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] # Finished training epoch 30 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] Loss (name: value) total: 7.91383014962\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] Loss (name: value) kld: 0.15321287579\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] Loss (name: value) recons: 7.76061726323\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] Loss (name: value) logppx: 7.91383014962\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] #quality_metric: host=algo-2, epoch=30, train total_loss <loss>=7.91383014962\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:37.313] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 29, \"duration\": 2247, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] Loss (name: value) total: 8.59469733605\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] Loss (name: value) kld: 0.147068883823\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] Loss (name: value) recons: 8.4476285103\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] Loss (name: value) logppx: 8.59469733605\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] #validation_score (30): 8.59469733605\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] patience losses:[8.6685534354967952, 8.6164066803761017, 8.6262852204151645, 8.5910018529647427, 8.6296598776792877] min patience loss:8.59100185296 current loss:8.59469733605 absolute loss difference:0.00369548308544\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] Timing: train: 1.99s, val: 0.25s, epoch: 2.24s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] #progress_metric: host=algo-2, completed 20 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1080, \"sum\": 1080.0, \"min\": 1080}, \"Total Records Seen\": {\"count\": 1, \"max\": 31860, \"sum\": 31860.0, \"min\": 31860}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}}, \"EndTime\": 1554830917.563546, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 29}, \"StartTime\": 1554830915.318834}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=473.068079373 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:37.563] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 2244, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:37 INFO 139701951670080] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] # Finished training epoch 31 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] Loss (name: value) total: 7.89597006904\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] Loss (name: value) kld: 0.156016482026\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] Loss (name: value) recons: 7.73995356383\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] Loss (name: value) logppx: 7.89597006904\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] #quality_metric: host=algo-2, epoch=31, train total_loss <loss>=7.89597006904\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:39.608] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 30, \"duration\": 2294, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] Loss (name: value) total: 8.63330426338\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] Loss (name: value) kld: 0.135778781695\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] Loss (name: value) recons: 8.49752557217\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] Loss (name: value) logppx: 8.63330426338\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] #validation_score (31): 8.63330426338\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] patience losses:[8.6164066803761017, 8.6262852204151645, 8.5910018529647427, 8.6296598776792877, 8.5946973360501797] min patience loss:8.59100185296 current loss:8.63330426338 absolute loss difference:0.0423024104192\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] Timing: train: 2.04s, val: 0.28s, epoch: 2.33s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] #progress_metric: host=algo-2, completed 20 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1116, \"sum\": 1116.0, \"min\": 1116}, \"Total Records Seen\": {\"count\": 1, \"max\": 32922, \"sum\": 32922.0, \"min\": 32922}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 62, \"sum\": 62.0, \"min\": 62}}, \"EndTime\": 1554830919.89195, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 30}, \"StartTime\": 1554830917.563977}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=456.16383 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:39.892] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 31, \"duration\": 2327, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:39 INFO 139701951670080] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] # Finished training epoch 16 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] Loss (name: value) total: 8.15865734978\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] Loss (name: value) kld: 0.164815712311\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] Loss (name: value) recons: 7.9938416387\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] Loss (name: value) logppx: 8.15865734978\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=8.15865734978\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:40.510] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 15, \"duration\": 4387, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] Loss (name: value) total: 8.57393071101\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] Loss (name: value) kld: 0.152117298811\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] Loss (name: value) recons: 8.42181329972\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] Loss (name: value) logppx: 8.57393071101\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] #validation_score (16): 8.57393071101\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] patience losses:[8.6249487852438911, 8.6099223992763427, 8.6162535056089737, 8.5886044233273235, 8.5811918209760609] min patience loss:8.58119182098 current loss:8.57393071101 absolute loss difference:0.00726110996344\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] Timing: train: 4.12s, val: 0.27s, epoch: 4.38s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1136, \"sum\": 1136.0, \"min\": 1136}, \"Total Records Seen\": {\"count\": 1, \"max\": 34016, \"sum\": 34016.0, \"min\": 34016}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1554830920.77653, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1554830916.393816}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=485.061555684 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:40.776] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 4382, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:40 INFO 139661896189760] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] # Finished training epoch 32 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] Loss (name: value) total: 7.87442692651\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] Loss (name: value) kld: 0.160219927187\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] Loss (name: value) recons: 7.71420697106\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] Loss (name: value) logppx: 7.87442692651\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] #quality_metric: host=algo-2, epoch=32, train total_loss <loss>=7.87442692651\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:41.568] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 31, \"duration\": 1960, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] Loss (name: value) total: 8.56768657978\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] Loss (name: value) kld: 0.138997856165\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] Loss (name: value) recons: 8.42868859707\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] Loss (name: value) logppx: 8.56768657978\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] #validation_score (32): 8.56768657978\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] patience losses:[8.6262852204151645, 8.5910018529647427, 8.6296598776792877, 8.5946973360501797, 8.6333042633839145] min patience loss:8.59100185296 current loss:8.56768657978 absolute loss difference:0.0233152731871\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] Timing: train: 1.68s, val: 0.25s, epoch: 1.92s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] #progress_metric: host=algo-2, completed 21 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1152, \"sum\": 1152.0, \"min\": 1152}, \"Total Records Seen\": {\"count\": 1, \"max\": 33984, \"sum\": 33984.0, \"min\": 33984}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}}, \"EndTime\": 1554830921.81725, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 31}, \"StartTime\": 1554830919.892335}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=551.668784038 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:41.817] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 1924, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:41 INFO 139701951670080] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:43 INFO 139701951670080] # Finished training epoch 33 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:43 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:43 INFO 139701951670080] Loss (name: value) total: 7.86776651453\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:43 INFO 139701951670080] Loss (name: value) kld: 0.163022725008\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:43 INFO 139701951670080] Loss (name: value) recons: 7.70474376678\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:43 INFO 139701951670080] Loss (name: value) logppx: 7.86776651453\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:43 INFO 139701951670080] #quality_metric: host=algo-2, epoch=33, train total_loss <loss>=7.86776651453\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:43.763] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 32, \"duration\": 2194, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:44 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:44 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:44 INFO 139701951670080] Loss (name: value) total: 8.63824059902\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:44 INFO 139701951670080] Loss (name: value) kld: 0.165147531338\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:44 INFO 139701951670080] Loss (name: value) recons: 8.47309312087\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:44 INFO 139701951670080] Loss (name: value) logppx: 8.63824059902\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:44 INFO 139701951670080] #validation_score (33): 8.63824059902\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:44 INFO 139701951670080] patience losses:[8.5910018529647427, 8.6296598776792877, 8.5946973360501797, 8.6333042633839145, 8.5676865797776447] min patience loss:8.56768657978 current loss:8.63824059902 absolute loss difference:0.0705540192433\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:44 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:44 INFO 139701951670080] Timing: train: 1.95s, val: 0.28s, epoch: 2.23s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:44 INFO 139701951670080] #progress_metric: host=algo-2, completed 22 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1188, \"sum\": 1188.0, \"min\": 1188}, \"Total Records Seen\": {\"count\": 1, \"max\": 35046, \"sum\": 35046.0, \"min\": 35046}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 66, \"sum\": 66.0, \"min\": 66}}, \"EndTime\": 1554830924.047739, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 32}, \"StartTime\": 1554830921.817596}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:44 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=476.159922828 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:44.048] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 33, \"duration\": 2228, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:44 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:44 INFO 139701951670080] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:44 INFO 139661896189760] # Finished training epoch 17 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:44 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:44 INFO 139661896189760] Loss (name: value) total: 8.14664413381\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:44 INFO 139661896189760] Loss (name: value) kld: 0.172002498085\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:44 INFO 139661896189760] Loss (name: value) recons: 7.97464162621\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:44 INFO 139661896189760] Loss (name: value) logppx: 8.14664413381\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:44 INFO 139661896189760] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=8.14664413381\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:44.847] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 16, \"duration\": 4336, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:45 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:45 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:45 INFO 139661896189760] Loss (name: value) total: 8.55593062181\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:45 INFO 139661896189760] Loss (name: value) kld: 0.139378878398\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:45 INFO 139661896189760] Loss (name: value) recons: 8.41655195187\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:45 INFO 139661896189760] Loss (name: value) logppx: 8.55593062181\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:45 INFO 139661896189760] #validation_score (17): 8.55593062181\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:45 INFO 139661896189760] patience losses:[8.6099223992763427, 8.6162535056089737, 8.5886044233273235, 8.5811918209760609, 8.5739307110126202] min patience loss:8.57393071101 current loss:8.55593062181 absolute loss difference:0.0180000892052\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:45 INFO 139661896189760] Timing: train: 4.07s, val: 0.28s, epoch: 4.35s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:45 INFO 139661896189760] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1207, \"sum\": 1207.0, \"min\": 1207}, \"Total Records Seen\": {\"count\": 1, \"max\": 36142, \"sum\": 36142.0, \"min\": 36142}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1554830925.123123, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 16}, \"StartTime\": 1554830920.776809}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:45 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=489.133566779 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:45.123] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 4346, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:45 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:45 INFO 139661896189760] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:45 INFO 139701951670080] # Finished training epoch 34 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:45 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:45 INFO 139701951670080] Loss (name: value) total: 7.8456705376\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:45 INFO 139701951670080] Loss (name: value) kld: 0.166076224822\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:45 INFO 139701951670080] Loss (name: value) recons: 7.67959435074\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:45 INFO 139701951670080] Loss (name: value) logppx: 7.8456705376\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:45 INFO 139701951670080] #quality_metric: host=algo-2, epoch=34, train total_loss <loss>=7.8456705376\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:45.926] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 33, \"duration\": 2162, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:46 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:46 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:46 INFO 139701951670080] Loss (name: value) total: 8.56045348339\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:46 INFO 139701951670080] Loss (name: value) kld: 0.13770188246\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:46 INFO 139701951670080] Loss (name: value) recons: 8.42275167612\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:46 INFO 139701951670080] Loss (name: value) logppx: 8.56045348339\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:46 INFO 139701951670080] #validation_score (34): 8.56045348339\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:46 INFO 139701951670080] patience losses:[8.6296598776792877, 8.5946973360501797, 8.6333042633839145, 8.5676865797776447, 8.6382405990209339] min patience loss:8.56768657978 current loss:8.56045348339 absolute loss difference:0.00723309639173\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:46 INFO 139701951670080] Timing: train: 1.88s, val: 0.27s, epoch: 2.15s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:46 INFO 139701951670080] #progress_metric: host=algo-2, completed 22 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1224, \"sum\": 1224.0, \"min\": 1224}, \"Total Records Seen\": {\"count\": 1, \"max\": 36108, \"sum\": 36108.0, \"min\": 36108}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 68, \"sum\": 68.0, \"min\": 68}}, \"EndTime\": 1554830926.200783, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 33}, \"StartTime\": 1554830924.048161}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:46 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=493.312875271 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:46.200] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 2152, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:46 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:46 INFO 139701951670080] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] # Finished training epoch 35 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] Loss (name: value) total: 7.83678025846\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] Loss (name: value) kld: 0.165163698682\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] Loss (name: value) recons: 7.67161651894\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] Loss (name: value) logppx: 7.83678025846\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] #quality_metric: host=algo-2, epoch=35, train total_loss <loss>=7.83678025846\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:48.290] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 34, \"duration\": 2364, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] # Finished training epoch 18 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] Loss (name: value) total: 8.13193532021\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] Loss (name: value) kld: 0.173681916877\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] Loss (name: value) recons: 7.95825342169\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] Loss (name: value) logppx: 8.13193532021\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=8.13193532021\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:49.302] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 17, \"duration\": 4454, \"num_examples\": 14}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] Loss (name: value) total: 8.61953797952\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] Loss (name: value) kld: 0.150058490802\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] Loss (name: value) recons: 8.46947940924\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] Loss (name: value) logppx: 8.61953797952\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] #validation_score (35): 8.61953797952\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] patience losses:[8.5946973360501797, 8.6333042633839145, 8.5676865797776447, 8.6382405990209339, 8.5604534833859169] min patience loss:8.56045348339 current loss:8.61953797952 absolute loss difference:0.0590844961313\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] Timing: train: 2.09s, val: 0.25s, epoch: 2.34s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] #progress_metric: host=algo-2, completed 23 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1260, \"sum\": 1260.0, \"min\": 1260}, \"Total Records Seen\": {\"count\": 1, \"max\": 37170, \"sum\": 37170.0, \"min\": 37170}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 70, \"sum\": 70.0, \"min\": 70}}, \"EndTime\": 1554830928.541304, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 34}, \"StartTime\": 1554830926.20105}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=453.77053183 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:48.541] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 2340, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:48 INFO 139701951670080] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] Loss (name: value) total: 8.54872737787\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] Loss (name: value) kld: 0.151032288258\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] Loss (name: value) recons: 8.39769502297\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] Loss (name: value) logppx: 8.54872737787\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] #validation_score (18): 8.54872737787\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] patience losses:[8.6162535056089737, 8.5886044233273235, 8.5811918209760609, 8.5739307110126202, 8.5559306218073914] min patience loss:8.55593062181 current loss:8.54872737787 absolute loss difference:0.0072032439403\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] Timing: train: 4.18s, val: 0.27s, epoch: 4.44s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1278, \"sum\": 1278.0, \"min\": 1278}, \"Total Records Seen\": {\"count\": 1, \"max\": 38268, \"sum\": 38268.0, \"min\": 38268}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1554830929.568148, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 17}, \"StartTime\": 1554830925.123401}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=478.300328907 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:49.568] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 4444, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:49 INFO 139661896189760] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] # Finished training epoch 36 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] Loss (name: value) total: 7.81800806964\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] Loss (name: value) kld: 0.164773438153\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] Loss (name: value) recons: 7.65323465135\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] Loss (name: value) logppx: 7.81800806964\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] #quality_metric: host=algo-2, epoch=36, train total_loss <loss>=7.81800806964\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:50.414] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 35, \"duration\": 2120, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] Loss (name: value) total: 8.55910621056\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] Loss (name: value) kld: 0.147504544869\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] Loss (name: value) recons: 8.4116016877\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] Loss (name: value) logppx: 8.55910621056\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] #validation_score (36): 8.55910621056\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] patience losses:[8.6333042633839145, 8.5676865797776447, 8.6382405990209339, 8.5604534833859169, 8.6195379795172276] min patience loss:8.56045348339 current loss:8.55910621056 absolute loss difference:0.00134727282402\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] Timing: train: 1.87s, val: 0.28s, epoch: 2.15s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] #progress_metric: host=algo-2, completed 24 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1296, \"sum\": 1296.0, \"min\": 1296}, \"Total Records Seen\": {\"count\": 1, \"max\": 38232, \"sum\": 38232.0, \"min\": 38232}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}}, \"EndTime\": 1554830930.696274, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 35}, \"StartTime\": 1554830928.54156}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=492.835347872 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:50.696] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 2154, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:50 INFO 139701951670080] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] # Finished training epoch 37 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] Loss (name: value) total: 7.80950615494\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] Loss (name: value) kld: 0.164447657267\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] Loss (name: value) recons: 7.64505849768\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] Loss (name: value) logppx: 7.80950615494\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] #quality_metric: host=algo-2, epoch=37, train total_loss <loss>=7.80950615494\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:52.668] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 36, \"duration\": 2253, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] Loss (name: value) total: 8.59201867519\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] Loss (name: value) kld: 0.141411377222\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] Loss (name: value) recons: 8.45060718243\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] Loss (name: value) logppx: 8.59201867519\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] #validation_score (37): 8.59201867519\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] patience losses:[8.5676865797776447, 8.6382405990209339, 8.5604534833859169, 8.6195379795172276, 8.5591062105618985] min patience loss:8.55910621056 current loss:8.59201867519 absolute loss difference:0.0329124646309\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] Timing: train: 1.97s, val: 0.26s, epoch: 2.24s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] #progress_metric: host=algo-2, completed 24 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1332, \"sum\": 1332.0, \"min\": 1332}, \"Total Records Seen\": {\"count\": 1, \"max\": 39294, \"sum\": 39294.0, \"min\": 39294}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 74, \"sum\": 74.0, \"min\": 74}}, \"EndTime\": 1554830932.933455, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 36}, \"StartTime\": 1554830930.696591}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=474.736384111 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:52.933] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 37, \"duration\": 2236, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:52 INFO 139701951670080] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] # Finished training epoch 19 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] Loss (name: value) total: 8.11911597453\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] Loss (name: value) kld: 0.173674186854\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] Loss (name: value) recons: 7.94544175555\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] Loss (name: value) logppx: 8.11911597453\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=8.11911597453\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:53.728] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 18, \"duration\": 4425, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] Loss (name: value) total: 8.52838987693\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] Loss (name: value) kld: 0.14528537897\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] Loss (name: value) recons: 8.3831044515\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] Loss (name: value) logppx: 8.52838987693\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] #validation_score (19): 8.52838987693\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] patience losses:[8.5886044233273235, 8.5811918209760609, 8.5739307110126202, 8.5559306218073914, 8.5487273778670865] min patience loss:8.54872737787 current loss:8.52838987693 absolute loss difference:0.020337500939\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] Timing: train: 4.16s, val: 0.25s, epoch: 4.41s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1349, \"sum\": 1349.0, \"min\": 1349}, \"Total Records Seen\": {\"count\": 1, \"max\": 40394, \"sum\": 40394.0, \"min\": 40394}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1554830933.975279, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 18}, \"StartTime\": 1554830929.568446}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=482.416526015 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:53.975] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 19, \"duration\": 4406, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:53 INFO 139661896189760] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:54 INFO 139701951670080] # Finished training epoch 38 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:54 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:54 INFO 139701951670080] Loss (name: value) total: 7.79277652458\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:54 INFO 139701951670080] Loss (name: value) kld: 0.166623093685\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:54 INFO 139701951670080] Loss (name: value) recons: 7.62615346555\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:54 INFO 139701951670080] Loss (name: value) logppx: 7.79277652458\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:54 INFO 139701951670080] #quality_metric: host=algo-2, epoch=38, train total_loss <loss>=7.79277652458\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:54.937] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 37, \"duration\": 2268, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:55 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:55 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:55 INFO 139701951670080] Loss (name: value) total: 8.54401663756\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:55 INFO 139701951670080] Loss (name: value) kld: 0.142452840927\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:55 INFO 139701951670080] Loss (name: value) recons: 8.401563752\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:55 INFO 139701951670080] Loss (name: value) logppx: 8.54401663756\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:55 INFO 139701951670080] #validation_score (38): 8.54401663756\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:55 INFO 139701951670080] patience losses:[8.6382405990209339, 8.5604534833859169, 8.6195379795172276, 8.5591062105618985, 8.5920186751928078] min patience loss:8.55910621056 current loss:8.54401663756 absolute loss difference:0.0150895730043\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:55 INFO 139701951670080] Timing: train: 2.00s, val: 0.24s, epoch: 2.25s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:55 INFO 139701951670080] #progress_metric: host=algo-2, completed 25 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1368, \"sum\": 1368.0, \"min\": 1368}, \"Total Records Seen\": {\"count\": 1, \"max\": 40356, \"sum\": 40356.0, \"min\": 40356}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 76, \"sum\": 76.0, \"min\": 76}}, \"EndTime\": 1554830935.179395, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 37}, \"StartTime\": 1554830932.933794}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:55 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=472.888182644 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:55.179] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 2245, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:55 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:55 INFO 139701951670080] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:56 INFO 139701951670080] # Finished training epoch 39 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:56 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:56 INFO 139701951670080] Loss (name: value) total: 7.79039248007\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:56 INFO 139701951670080] Loss (name: value) kld: 0.167594959339\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:56 INFO 139701951670080] Loss (name: value) recons: 7.62279752096\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:56 INFO 139701951670080] Loss (name: value) logppx: 7.79039248007\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:56 INFO 139701951670080] #quality_metric: host=algo-2, epoch=39, train total_loss <loss>=7.79039248007\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:56.860] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 38, \"duration\": 1922, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:57 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:57 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:57 INFO 139701951670080] Loss (name: value) total: 8.58678925343\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:57 INFO 139701951670080] Loss (name: value) kld: 0.152417470247\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:57 INFO 139701951670080] Loss (name: value) recons: 8.43437186999\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:57 INFO 139701951670080] Loss (name: value) logppx: 8.58678925343\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:57 INFO 139701951670080] #validation_score (39): 8.58678925343\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:57 INFO 139701951670080] patience losses:[8.5604534833859169, 8.6195379795172276, 8.5591062105618985, 8.5920186751928078, 8.5440166375575917] min patience loss:8.54401663756 current loss:8.58678925343 absolute loss difference:0.0427726158729\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:57 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:57 INFO 139701951670080] Timing: train: 1.68s, val: 0.23s, epoch: 1.91s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:57 INFO 139701951670080] #progress_metric: host=algo-2, completed 26 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1404, \"sum\": 1404.0, \"min\": 1404}, \"Total Records Seen\": {\"count\": 1, \"max\": 41418, \"sum\": 41418.0, \"min\": 41418}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 78, \"sum\": 78.0, \"min\": 78}}, \"EndTime\": 1554830937.094773, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 38}, \"StartTime\": 1554830935.179737}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:57 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=554.505205079 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:57.095] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 39, \"duration\": 1914, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:57 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:57 INFO 139701951670080] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:58 INFO 139701951670080] # Finished training epoch 40 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:58 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:58 INFO 139701951670080] Loss (name: value) total: 7.77111119871\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:58 INFO 139701951670080] Loss (name: value) kld: 0.169607739095\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:58 INFO 139701951670080] Loss (name: value) recons: 7.6015034499\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:58 INFO 139701951670080] Loss (name: value) logppx: 7.77111119871\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:58 INFO 139701951670080] #quality_metric: host=algo-2, epoch=40, train total_loss <loss>=7.77111119871\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:58.840] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 39, \"duration\": 1979, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:59 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:59 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:59 INFO 139701951670080] Loss (name: value) total: 8.54604437412\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:59 INFO 139701951670080] Loss (name: value) kld: 0.16052802954\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:59 INFO 139701951670080] Loss (name: value) recons: 8.38551627917\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:59 INFO 139701951670080] Loss (name: value) logppx: 8.54604437412\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:59 INFO 139701951670080] #validation_score (40): 8.54604437412\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:59 INFO 139701951670080] patience losses:[8.6195379795172276, 8.5591062105618985, 8.5920186751928078, 8.5440166375575917, 8.5867892534304886] min patience loss:8.54401663756 current loss:8.54604437412 absolute loss difference:0.00202773656601\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:59 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:59 INFO 139701951670080] Timing: train: 1.75s, val: 0.25s, epoch: 2.00s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:59 INFO 139701951670080] #progress_metric: host=algo-2, completed 26 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1440, \"sum\": 1440.0, \"min\": 1440}, \"Total Records Seen\": {\"count\": 1, \"max\": 42480, \"sum\": 42480.0, \"min\": 42480}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 80, \"sum\": 80.0, \"min\": 80}}, \"EndTime\": 1554830939.094086, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 39}, \"StartTime\": 1554830937.095068}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:59 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=531.160957712 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:28:59.094] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 1999, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:59 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:28:59 INFO 139701951670080] # Starting training for epoch 41\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] # Finished training epoch 20 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] Loss (name: value) total: 8.1110986629\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] Loss (name: value) kld: 0.17596267098\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] Loss (name: value) recons: 7.93513598375\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] Loss (name: value) logppx: 8.1110986629\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=8.1110986629\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:58.057] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 19, \"duration\": 4328, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] Loss (name: value) total: 8.53334753574\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] Loss (name: value) kld: 0.148339548478\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] Loss (name: value) recons: 8.38500804412\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] Loss (name: value) logppx: 8.53334753574\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] #validation_score (20): 8.53334753574\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] patience losses:[8.5811918209760609, 8.5739307110126202, 8.5559306218073914, 8.5487273778670865, 8.5283898769280846] min patience loss:8.52838987693 current loss:8.53334753574 absolute loss difference:0.00495765881661\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] Timing: train: 4.08s, val: 0.25s, epoch: 4.33s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1420, \"sum\": 1420.0, \"min\": 1420}, \"Total Records Seen\": {\"count\": 1, \"max\": 42520, \"sum\": 42520.0, \"min\": 42520}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1554830938.306423, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 19}, \"StartTime\": 1554830933.975564}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=490.878586372 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:28:58.306] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 4330, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:28:58 INFO 139661896189760] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:00 INFO 139701951670080] # Finished training epoch 41 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:00 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:00 INFO 139701951670080] Loss (name: value) total: 7.77327707079\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:00 INFO 139701951670080] Loss (name: value) kld: 0.167427215091\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:00 INFO 139701951670080] Loss (name: value) recons: 7.60584983826\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:00 INFO 139701951670080] Loss (name: value) logppx: 7.77327707079\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:00 INFO 139701951670080] #quality_metric: host=algo-2, epoch=41, train total_loss <loss>=7.77327707079\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:00.850] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 40, \"duration\": 2009, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:01 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:01 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:01 INFO 139701951670080] Loss (name: value) total: 8.56365896372\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:01 INFO 139701951670080] Loss (name: value) kld: 0.157928419113\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:01 INFO 139701951670080] Loss (name: value) recons: 8.40573065342\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:01 INFO 139701951670080] Loss (name: value) logppx: 8.56365896372\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:01 INFO 139701951670080] #validation_score (41): 8.56365896372\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:01 INFO 139701951670080] patience losses:[8.5591062105618985, 8.5920186751928078, 8.5440166375575917, 8.5867892534304886, 8.5460443741235981] min patience loss:8.54401663756 current loss:8.56365896372 absolute loss difference:0.0196423261594\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:01 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:01 INFO 139701951670080] Timing: train: 1.76s, val: 0.30s, epoch: 2.06s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:01 INFO 139701951670080] #progress_metric: host=algo-2, completed 27 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1476, \"sum\": 1476.0, \"min\": 1476}, \"Total Records Seen\": {\"count\": 1, \"max\": 43542, \"sum\": 43542.0, \"min\": 43542}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}}, \"EndTime\": 1554830941.155259, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 40}, \"StartTime\": 1554830939.094867}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:01 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=515.388867297 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:01.155] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 2060, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:01 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:01 INFO 139701951670080] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] # Finished training epoch 21 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] Loss (name: value) total: 8.1015498918\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] Loss (name: value) kld: 0.176904242923\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] Loss (name: value) recons: 7.92464567372\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] Loss (name: value) logppx: 8.1015498918\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=8.1015498918\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:02.373] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 20, \"duration\": 4315, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] # Finished training epoch 42 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] Loss (name: value) total: 7.74950480991\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] Loss (name: value) kld: 0.167957142327\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] Loss (name: value) recons: 7.58154766648\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] Loss (name: value) logppx: 7.74950480991\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] #quality_metric: host=algo-2, epoch=42, train total_loss <loss>=7.74950480991\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:03.172] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 41, \"duration\": 2322, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] Loss (name: value) total: 8.53013927753\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] Loss (name: value) kld: 0.143137575419\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] Loss (name: value) recons: 8.3870016636\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] Loss (name: value) logppx: 8.53013927753\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] #validation_score (21): 8.53013927753\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] patience losses:[8.5739307110126202, 8.5559306218073914, 8.5487273778670865, 8.5283898769280846, 8.5333475357446922] min patience loss:8.52838987693 current loss:8.53013927753 absolute loss difference:0.00174940060347\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] Timing: train: 4.07s, val: 0.27s, epoch: 4.33s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1491, \"sum\": 1491.0, \"min\": 1491}, \"Total Records Seen\": {\"count\": 1, \"max\": 44646, \"sum\": 44646.0, \"min\": 44646}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1554830942.640377, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 20}, \"StartTime\": 1554830938.306705}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=490.554854667 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:02.640] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 21, \"duration\": 4333, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:02 INFO 139661896189760] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] Loss (name: value) total: 8.53891844138\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] Loss (name: value) kld: 0.147078751295\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] Loss (name: value) recons: 8.39183971698\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] Loss (name: value) logppx: 8.53891844138\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] #validation_score (42): 8.53891844138\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] patience losses:[8.5920186751928078, 8.5440166375575917, 8.5867892534304886, 8.5460443741235981, 8.5636589637169465] min patience loss:8.54401663756 current loss:8.53891844138 absolute loss difference:0.00509819617638\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] Timing: train: 2.02s, val: 0.24s, epoch: 2.26s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] #progress_metric: host=algo-2, completed 28 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1512, \"sum\": 1512.0, \"min\": 1512}, \"Total Records Seen\": {\"count\": 1, \"max\": 44604, \"sum\": 44604.0, \"min\": 44604}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 84, \"sum\": 84.0, \"min\": 84}}, \"EndTime\": 1554830943.416987, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 41}, \"StartTime\": 1554830941.15566}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=469.598411967 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:03.417] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 42, \"duration\": 2261, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:03 INFO 139701951670080] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] # Finished training epoch 43 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] Loss (name: value) total: 7.75340154436\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] Loss (name: value) kld: 0.166184493127\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] Loss (name: value) recons: 7.58721700598\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] Loss (name: value) logppx: 7.75340154436\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] #quality_metric: host=algo-2, epoch=43, train total_loss <loss>=7.75340154436\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:05.322] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 42, \"duration\": 2149, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] Loss (name: value) total: 8.56799668532\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] Loss (name: value) kld: 0.159400427036\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] Loss (name: value) recons: 8.40859629313\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] Loss (name: value) logppx: 8.56799668532\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] #validation_score (43): 8.56799668532\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] patience losses:[8.5440166375575917, 8.5867892534304886, 8.5460443741235981, 8.5636589637169465, 8.5389184413812096] min patience loss:8.53891844138 current loss:8.56799668532 absolute loss difference:0.0290782439403\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] Timing: train: 1.91s, val: 0.25s, epoch: 2.16s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] #progress_metric: host=algo-2, completed 28 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1548, \"sum\": 1548.0, \"min\": 1548}, \"Total Records Seen\": {\"count\": 1, \"max\": 45666, \"sum\": 45666.0, \"min\": 45666}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 86, \"sum\": 86.0, \"min\": 86}}, \"EndTime\": 1554830945.576792, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 42}, \"StartTime\": 1554830943.417249}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=491.738619229 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:05.577] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 43, \"duration\": 2159, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:05 INFO 139701951670080] # Starting training for epoch 44\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:06 INFO 139661896189760] # Finished training epoch 22 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:06 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:06 INFO 139661896189760] Loss (name: value) total: 8.08617004251\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:06 INFO 139661896189760] Loss (name: value) kld: 0.175093978671\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:06 INFO 139661896189760] Loss (name: value) recons: 7.9110760541\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:06 INFO 139661896189760] Loss (name: value) logppx: 8.08617004251\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:06 INFO 139661896189760] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=8.08617004251\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:06.814] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 21, \"duration\": 4440, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:07 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:07 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:07 INFO 139661896189760] Loss (name: value) total: 8.5213560838\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:07 INFO 139661896189760] Loss (name: value) kld: 0.144606637955\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:07 INFO 139661896189760] Loss (name: value) recons: 8.37674943973\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:07 INFO 139661896189760] Loss (name: value) logppx: 8.5213560838\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:07 INFO 139661896189760] #validation_score (22): 8.5213560838\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:07 INFO 139661896189760] patience losses:[8.5559306218073914, 8.5487273778670865, 8.5283898769280846, 8.5333475357446922, 8.5301392775315499] min patience loss:8.52838987693 current loss:8.5213560838 absolute loss difference:0.00703379313151\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:07 INFO 139661896189760] Timing: train: 4.17s, val: 0.25s, epoch: 4.42s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:07 INFO 139661896189760] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1562, \"sum\": 1562.0, \"min\": 1562}, \"Total Records Seen\": {\"count\": 1, \"max\": 46772, \"sum\": 46772.0, \"min\": 46772}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1554830947.062378, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 21}, \"StartTime\": 1554830942.640761}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:07 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=480.800540915 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:07.062] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 4421, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:07 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:07 INFO 139661896189760] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] # Finished training epoch 44 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] Loss (name: value) total: 7.73497996154\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] Loss (name: value) kld: 0.166835298141\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] Loss (name: value) recons: 7.56814462874\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] Loss (name: value) logppx: 7.73497996154\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] #quality_metric: host=algo-2, epoch=44, train total_loss <loss>=7.73497996154\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:07.600] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 43, \"duration\": 2277, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] Loss (name: value) total: 8.52129493127\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] Loss (name: value) kld: 0.149640018512\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] Loss (name: value) recons: 8.37165492131\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] Loss (name: value) logppx: 8.52129493127\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] #validation_score (44): 8.52129493127\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] patience losses:[8.5867892534304886, 8.5460443741235981, 8.5636589637169465, 8.5389184413812096, 8.5679966853215141] min patience loss:8.53891844138 current loss:8.52129493127 absolute loss difference:0.0176235101162\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] Timing: train: 2.02s, val: 0.25s, epoch: 2.28s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] #progress_metric: host=algo-2, completed 29 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1584, \"sum\": 1584.0, \"min\": 1584}, \"Total Records Seen\": {\"count\": 1, \"max\": 46728, \"sum\": 46728.0, \"min\": 46728}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 88, \"sum\": 88.0, \"min\": 88}}, \"EndTime\": 1554830947.853335, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 43}, \"StartTime\": 1554830945.577057}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=466.515863694 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:07.853] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 2276, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:07 INFO 139701951670080] # Starting training for epoch 45\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] # Finished training epoch 45 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] Loss (name: value) total: 7.73606065114\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] Loss (name: value) kld: 0.167917236134\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] Loss (name: value) recons: 7.56814340662\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] Loss (name: value) logppx: 7.73606065114\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] #quality_metric: host=algo-2, epoch=45, train total_loss <loss>=7.73606065114\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:09.703] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 44, \"duration\": 2102, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] Loss (name: value) total: 8.54801264054\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] Loss (name: value) kld: 0.143282434879\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] Loss (name: value) recons: 8.40473026373\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] Loss (name: value) logppx: 8.54801264054\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] #validation_score (45): 8.54801264054\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] patience losses:[8.5460443741235981, 8.5636589637169465, 8.5389184413812096, 8.5679966853215141, 8.5212949312650235] min patience loss:8.52129493127 current loss:8.54801264054 absolute loss difference:0.0267177092723\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] Timing: train: 1.85s, val: 0.26s, epoch: 2.11s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] #progress_metric: host=algo-2, completed 30 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1620, \"sum\": 1620.0, \"min\": 1620}, \"Total Records Seen\": {\"count\": 1, \"max\": 47790, \"sum\": 47790.0, \"min\": 47790}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 90, \"sum\": 90.0, \"min\": 90}}, \"EndTime\": 1554830949.96862, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 44}, \"StartTime\": 1554830947.853702}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=502.101790809 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:09.968] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 45, \"duration\": 2114, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:09 INFO 139701951670080] # Starting training for epoch 46\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] # Finished training epoch 23 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] Loss (name: value) total: 8.07730666326\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] Loss (name: value) kld: 0.177319417761\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] Loss (name: value) recons: 7.89998724852\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] Loss (name: value) logppx: 8.07730666326\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=8.07730666326\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:11.120] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 22, \"duration\": 4305, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] Loss (name: value) total: 8.50770928799\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] Loss (name: value) kld: 0.140000932645\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] Loss (name: value) recons: 8.36770852896\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] Loss (name: value) logppx: 8.50770928799\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] #validation_score (23): 8.50770928799\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] patience losses:[8.5487273778670865, 8.5283898769280846, 8.5333475357446922, 8.5301392775315499, 8.5213560837965741] min patience loss:8.5213560838 current loss:8.50770928799 absolute loss difference:0.0136467958108\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] Timing: train: 4.06s, val: 0.25s, epoch: 4.31s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1633, \"sum\": 1633.0, \"min\": 1633}, \"Total Records Seen\": {\"count\": 1, \"max\": 48898, \"sum\": 48898.0, \"min\": 48898}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1554830951.37371, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 22}, \"StartTime\": 1554830947.062661}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=493.134282307 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:11.373] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 4310, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:11 INFO 139661896189760] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] # Finished training epoch 46 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] Loss (name: value) total: 7.7168306492\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] Loss (name: value) kld: 0.167801230925\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] Loss (name: value) recons: 7.54902942092\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] Loss (name: value) logppx: 7.7168306492\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] #quality_metric: host=algo-2, epoch=46, train total_loss <loss>=7.7168306492\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:11.708] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 45, \"duration\": 2004, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] Loss (name: value) total: 8.51903604361\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] Loss (name: value) kld: 0.146113405472\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] Loss (name: value) recons: 8.37292261368\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] Loss (name: value) logppx: 8.51903604361\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] #validation_score (46): 8.51903604361\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] patience losses:[8.5636589637169465, 8.5389184413812096, 8.5679966853215141, 8.5212949312650235, 8.54801264053736] min patience loss:8.52129493127 current loss:8.51903604361 absolute loss difference:0.00225888765775\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] Timing: train: 1.74s, val: 0.24s, epoch: 1.98s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] #progress_metric: host=algo-2, completed 30 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1656, \"sum\": 1656.0, \"min\": 1656}, \"Total Records Seen\": {\"count\": 1, \"max\": 48852, \"sum\": 48852.0, \"min\": 48852}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 92, \"sum\": 92.0, \"min\": 92}}, \"EndTime\": 1554830951.951376, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 45}, \"StartTime\": 1554830949.968996}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=535.674551991 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:11.951] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 46, \"duration\": 1982, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:11 INFO 139701951670080] # Starting training for epoch 47\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] # Finished training epoch 47 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] Loss (name: value) total: 7.7068856204\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] Loss (name: value) kld: 0.165489546458\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] Loss (name: value) recons: 7.54139605628\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] Loss (name: value) logppx: 7.7068856204\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] #quality_metric: host=algo-2, epoch=47, train total_loss <loss>=7.7068856204\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:13.717] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 46, \"duration\": 2009, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] Loss (name: value) total: 8.55387162429\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] Loss (name: value) kld: 0.155440208239\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] Loss (name: value) recons: 8.39843155298\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] Loss (name: value) logppx: 8.55387162429\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] #validation_score (47): 8.55387162429\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] patience losses:[8.5389184413812096, 8.5679966853215141, 8.5212949312650235, 8.54801264053736, 8.5190360436072723] min patience loss:8.51903604361 current loss:8.55387162429 absolute loss difference:0.0348355806791\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] Timing: train: 1.77s, val: 0.26s, epoch: 2.02s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] #progress_metric: host=algo-2, completed 31 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1692, \"sum\": 1692.0, \"min\": 1692}, \"Total Records Seen\": {\"count\": 1, \"max\": 49914, \"sum\": 49914.0, \"min\": 49914}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 94, \"sum\": 94.0, \"min\": 94}}, \"EndTime\": 1554830953.975081, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 46}, \"StartTime\": 1554830951.951674}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=524.811452477 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:13.975] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 2022, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:13 INFO 139701951670080] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:15 INFO 139701951670080] # Finished training epoch 48 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:15 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:15 INFO 139701951670080] Loss (name: value) total: 7.69480982886\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:15 INFO 139701951670080] Loss (name: value) kld: 0.169430554575\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:15 INFO 139701951670080] Loss (name: value) recons: 7.52537923742\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:15 INFO 139701951670080] Loss (name: value) logppx: 7.69480982886\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:15 INFO 139701951670080] #quality_metric: host=algo-2, epoch=48, train total_loss <loss>=7.69480982886\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:15.939] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 47, \"duration\": 2221, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:16 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:16 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:16 INFO 139701951670080] Loss (name: value) total: 8.5231461745\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:16 INFO 139701951670080] Loss (name: value) kld: 0.15037802794\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:16 INFO 139701951670080] Loss (name: value) recons: 8.37276799129\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:16 INFO 139701951670080] Loss (name: value) logppx: 8.5231461745\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:16 INFO 139701951670080] #validation_score (48): 8.5231461745\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:16 INFO 139701951670080] patience losses:[8.5679966853215141, 8.5212949312650235, 8.54801264053736, 8.5190360436072723, 8.5538716242863586] min patience loss:8.51903604361 current loss:8.5231461745 absolute loss difference:0.00411013089693\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:16 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:16 INFO 139701951670080] Timing: train: 1.96s, val: 0.27s, epoch: 2.23s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:16 INFO 139701951670080] #progress_metric: host=algo-2, completed 32 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1728, \"sum\": 1728.0, \"min\": 1728}, \"Total Records Seen\": {\"count\": 1, \"max\": 50976, \"sum\": 50976.0, \"min\": 50976}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 96, \"sum\": 96.0, \"min\": 96}}, \"EndTime\": 1554830956.206011, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 47}, \"StartTime\": 1554830953.975602}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:16 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=476.094423016 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:16.206] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 48, \"duration\": 2230, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:16 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:16 INFO 139701951670080] # Starting training for epoch 49\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] # Finished training epoch 24 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] Loss (name: value) total: 8.0691882926\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] Loss (name: value) kld: 0.176395930595\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] Loss (name: value) recons: 7.89279237846\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] Loss (name: value) logppx: 8.0691882926\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=8.0691882926\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:15.484] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 23, \"duration\": 4363, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] Loss (name: value) total: 8.50982274765\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] Loss (name: value) kld: 0.152401483365\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] Loss (name: value) recons: 8.35742132725\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] Loss (name: value) logppx: 8.50982274765\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] #validation_score (24): 8.50982274765\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] patience losses:[8.5283898769280846, 8.5333475357446922, 8.5301392775315499, 8.5213560837965741, 8.5077092879857776] min patience loss:8.50770928799 current loss:8.50982274765 absolute loss difference:0.00211345966046\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] Timing: train: 4.11s, val: 0.27s, epoch: 4.38s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1704, \"sum\": 1704.0, \"min\": 1704}, \"Total Records Seen\": {\"count\": 1, \"max\": 51024, \"sum\": 51024.0, \"min\": 51024}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1554830955.757748, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 23}, \"StartTime\": 1554830951.37399}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=484.953107842 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:15.757] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 4383, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:15 INFO 139661896189760] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:19 INFO 139661896189760] # Finished training epoch 25 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:19 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:19 INFO 139661896189760] Loss (name: value) total: 8.05790317876\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:19 INFO 139661896189760] Loss (name: value) kld: 0.181674212357\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:19 INFO 139661896189760] Loss (name: value) recons: 7.8762289486\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:19 INFO 139661896189760] Loss (name: value) logppx: 8.05790317876\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:19 INFO 139661896189760] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=8.05790317876\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:19.922] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 24, \"duration\": 4437, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:20 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:20 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:20 INFO 139661896189760] Loss (name: value) total: 8.50325837747\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:20 INFO 139661896189760] Loss (name: value) kld: 0.152084566997\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:20 INFO 139661896189760] Loss (name: value) recons: 8.35117390951\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:20 INFO 139661896189760] Loss (name: value) logppx: 8.50325837747\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:20 INFO 139661896189760] #validation_score (25): 8.50325837747\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:20 INFO 139661896189760] patience losses:[8.5333475357446922, 8.5301392775315499, 8.5213560837965741, 8.5077092879857776, 8.5098227476462345] min patience loss:8.50770928799 current loss:8.50325837747 absolute loss difference:0.00445091051933\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:20 INFO 139661896189760] Timing: train: 4.16s, val: 0.29s, epoch: 4.45s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:20 INFO 139661896189760] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1775, \"sum\": 1775.0, \"min\": 1775}, \"Total Records Seen\": {\"count\": 1, \"max\": 53150, \"sum\": 53150.0, \"min\": 53150}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1554830960.210269, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 24}, \"StartTime\": 1554830955.758056}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:20 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=477.499249996 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:20.210] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 25, \"duration\": 4452, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:20 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:20 INFO 139661896189760] # Starting training for epoch 26\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] # Finished training epoch 49 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] Loss (name: value) total: 7.698505225\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] Loss (name: value) kld: 0.169906467641\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] Loss (name: value) recons: 7.52859875008\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] Loss (name: value) logppx: 7.698505225\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] #quality_metric: host=algo-2, epoch=49, train total_loss <loss>=7.698505225\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:18.293] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 48, \"duration\": 2353, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] Loss (name: value) total: 8.54173392271\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] Loss (name: value) kld: 0.151773516337\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] Loss (name: value) recons: 8.3899604993\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] Loss (name: value) logppx: 8.54173392271\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] #validation_score (49): 8.54173392271\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] patience losses:[8.5212949312650235, 8.54801264053736, 8.5190360436072723, 8.5538716242863586, 8.5231461745042072] min patience loss:8.51903604361 current loss:8.54173392271 absolute loss difference:0.0226978791066\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] Timing: train: 2.09s, val: 0.25s, epoch: 2.34s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] #progress_metric: host=algo-2, completed 32 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1764, \"sum\": 1764.0, \"min\": 1764}, \"Total Records Seen\": {\"count\": 1, \"max\": 52038, \"sum\": 52038.0, \"min\": 52038}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 98, \"sum\": 98.0, \"min\": 98}}, \"EndTime\": 1554830958.542398, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 48}, \"StartTime\": 1554830956.206277}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=454.571703293 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:18.542] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 49, \"duration\": 2336, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:18 INFO 139701951670080] # Starting training for epoch 50\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] # Finished training epoch 50 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] Loss (name: value) total: 7.68691709307\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] Loss (name: value) kld: 0.171262998934\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] Loss (name: value) recons: 7.5156540906\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] Loss (name: value) logppx: 7.68691709307\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] #quality_metric: host=algo-2, epoch=50, train total_loss <loss>=7.68691709307\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:20.527] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 49, \"duration\": 2234, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] Loss (name: value) total: 8.50564207419\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] Loss (name: value) kld: 0.150894090457\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] Loss (name: value) recons: 8.3547479874\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] Loss (name: value) logppx: 8.50564207419\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] #validation_score (50): 8.50564207419\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] patience losses:[8.54801264053736, 8.5190360436072723, 8.5538716242863586, 8.5231461745042072, 8.541733922713842] min patience loss:8.51903604361 current loss:8.50564207419 absolute loss difference:0.0133939694136\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] Timing: train: 1.98s, val: 0.26s, epoch: 2.25s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] #progress_metric: host=algo-2, completed 33 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1800, \"sum\": 1800.0, \"min\": 1800}, \"Total Records Seen\": {\"count\": 1, \"max\": 53100, \"sum\": 53100.0, \"min\": 53100}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}}, \"EndTime\": 1554830960.790149, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 49}, \"StartTime\": 1554830958.542737}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=472.458581777 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:20.790] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 2247, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:20 INFO 139701951670080] # Starting training for epoch 51\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:22 INFO 139701951670080] # Finished training epoch 51 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:22 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:22 INFO 139701951670080] Loss (name: value) total: 7.68609204469\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:22 INFO 139701951670080] Loss (name: value) kld: 0.170166174571\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:22 INFO 139701951670080] Loss (name: value) recons: 7.51592580301\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:22 INFO 139701951670080] Loss (name: value) logppx: 7.68609204469\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:22 INFO 139701951670080] #quality_metric: host=algo-2, epoch=51, train total_loss <loss>=7.68609204469\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:22.851] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 50, \"duration\": 2323, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:23 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:23 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:23 INFO 139701951670080] Loss (name: value) total: 8.55253636287\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:23 INFO 139701951670080] Loss (name: value) kld: 0.159432174609\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:23 INFO 139701951670080] Loss (name: value) recons: 8.39310424022\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:23 INFO 139701951670080] Loss (name: value) logppx: 8.55253636287\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:23 INFO 139701951670080] #validation_score (51): 8.55253636287\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:23 INFO 139701951670080] patience losses:[8.5190360436072723, 8.5538716242863586, 8.5231461745042072, 8.541733922713842, 8.5056420741937107] min patience loss:8.50564207419 current loss:8.55253636287 absolute loss difference:0.0468942886744\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:23 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:23 INFO 139701951670080] Timing: train: 2.06s, val: 0.24s, epoch: 2.30s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:23 INFO 139701951670080] #progress_metric: host=algo-2, completed 34 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1836, \"sum\": 1836.0, \"min\": 1836}, \"Total Records Seen\": {\"count\": 1, \"max\": 54162, \"sum\": 54162.0, \"min\": 54162}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 102, \"sum\": 102.0, \"min\": 102}}, \"EndTime\": 1554830963.09398, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 50}, \"StartTime\": 1554830960.790891}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:23 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=461.08581276 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:23.094] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 51, \"duration\": 2301, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:23 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:23 INFO 139701951670080] # Starting training for epoch 52\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] # Finished training epoch 26 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] Loss (name: value) total: 8.04718884392\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] Loss (name: value) kld: 0.181659762736\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] Loss (name: value) recons: 7.86552910603\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] Loss (name: value) logppx: 8.04718884392\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=8.04718884392\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:24.346] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 25, \"duration\": 4424, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:24 INFO 139701951670080] # Finished training epoch 52 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:24 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:24 INFO 139701951670080] Loss (name: value) total: 7.67203973841\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:24 INFO 139701951670080] Loss (name: value) kld: 0.169707067808\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:24 INFO 139701951670080] Loss (name: value) recons: 7.50233263086\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:24 INFO 139701951670080] Loss (name: value) logppx: 7.67203973841\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:24 INFO 139701951670080] #quality_metric: host=algo-2, epoch=52, train total_loss <loss>=7.67203973841\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:24.862] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 51, \"duration\": 2011, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:25 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:25 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:25 INFO 139701951670080] Loss (name: value) total: 8.51376084548\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:25 INFO 139701951670080] Loss (name: value) kld: 0.150797392772\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:25 INFO 139701951670080] Loss (name: value) recons: 8.36296355419\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:25 INFO 139701951670080] Loss (name: value) logppx: 8.51376084548\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:25 INFO 139701951670080] #validation_score (52): 8.51376084548\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:25 INFO 139701951670080] patience losses:[8.5538716242863586, 8.5231461745042072, 8.541733922713842, 8.5056420741937107, 8.5525363628680893] min patience loss:8.50564207419 current loss:8.51376084548 absolute loss difference:0.00811877128405\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:25 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:25 INFO 139701951670080] Timing: train: 1.77s, val: 0.25s, epoch: 2.02s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:25 INFO 139701951670080] #progress_metric: host=algo-2, completed 34 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1872, \"sum\": 1872.0, \"min\": 1872}, \"Total Records Seen\": {\"count\": 1, \"max\": 55224, \"sum\": 55224.0, \"min\": 55224}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 104, \"sum\": 104.0, \"min\": 104}}, \"EndTime\": 1554830965.110839, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 51}, \"StartTime\": 1554830963.094321}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:25 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=526.600741699 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:25.111] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 52, \"duration\": 2016, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:25 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:25 INFO 139701951670080] # Starting training for epoch 53\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] Loss (name: value) total: 8.50533807217\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] Loss (name: value) kld: 0.147319337038\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] Loss (name: value) recons: 8.35801876753\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] Loss (name: value) logppx: 8.50533807217\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] #validation_score (26): 8.50533807217\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] patience losses:[8.5301392775315499, 8.5213560837965741, 8.5077092879857776, 8.5098227476462345, 8.5032583774664463] min patience loss:8.50325837747 current loss:8.50533807217 absolute loss difference:0.00207969469902\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] Timing: train: 4.14s, val: 0.31s, epoch: 4.44s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1846, \"sum\": 1846.0, \"min\": 1846}, \"Total Records Seen\": {\"count\": 1, \"max\": 55276, \"sum\": 55276.0, \"min\": 55276}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}}, \"EndTime\": 1554830964.654432, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 25}, \"StartTime\": 1554830960.21055}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=478.392680393 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:24.654] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 4443, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:24 INFO 139661896189760] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:26 INFO 139701951670080] # Finished training epoch 53 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:26 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:26 INFO 139701951670080] Loss (name: value) total: 7.67013360483\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:26 INFO 139701951670080] Loss (name: value) kld: 0.171136345245\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:26 INFO 139701951670080] Loss (name: value) recons: 7.49899724325\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:26 INFO 139701951670080] Loss (name: value) logppx: 7.67013360483\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:26 INFO 139701951670080] #quality_metric: host=algo-2, epoch=53, train total_loss <loss>=7.67013360483\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:26.959] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 52, \"duration\": 2096, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:27 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:27 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:27 INFO 139701951670080] Loss (name: value) total: 8.54273869441\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:27 INFO 139701951670080] Loss (name: value) kld: 0.155754698851\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:27 INFO 139701951670080] Loss (name: value) recons: 8.3869840573\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:27 INFO 139701951670080] Loss (name: value) logppx: 8.54273869441\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:27 INFO 139701951670080] #validation_score (53): 8.54273869441\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:27 INFO 139701951670080] patience losses:[8.5231461745042072, 8.541733922713842, 8.5056420741937107, 8.5525363628680893, 8.5137608454777638] min patience loss:8.50564207419 current loss:8.54273869441 absolute loss difference:0.0370966202173\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:27 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:27 INFO 139701951670080] Timing: train: 1.85s, val: 0.24s, epoch: 2.09s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:27 INFO 139701951670080] #progress_metric: host=algo-2, completed 35 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1908, \"sum\": 1908.0, \"min\": 1908}, \"Total Records Seen\": {\"count\": 1, \"max\": 56286, \"sum\": 56286.0, \"min\": 56286}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 106, \"sum\": 106.0, \"min\": 106}}, \"EndTime\": 1554830967.200416, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 52}, \"StartTime\": 1554830965.111138}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:27 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=508.267464556 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:27.200] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 2089, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:27 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:27 INFO 139701951670080] # Starting training for epoch 54\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/09/2019 17:29:28 INFO 139701951670080] # Finished training epoch 54 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:28 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:28 INFO 139701951670080] Loss (name: value) total: 7.65536160646\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:28 INFO 139701951670080] Loss (name: value) kld: 0.169969867998\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:28 INFO 139701951670080] Loss (name: value) recons: 7.48539165214\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:28 INFO 139701951670080] Loss (name: value) logppx: 7.65536160646\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:28 INFO 139701951670080] #quality_metric: host=algo-2, epoch=54, train total_loss <loss>=7.65536160646\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:28.960] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 53, \"duration\": 2001, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:29 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:29 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:29 INFO 139701951670080] Loss (name: value) total: 8.51003500131\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:29 INFO 139701951670080] Loss (name: value) kld: 0.152397217506\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:29 INFO 139701951670080] Loss (name: value) recons: 8.35763757168\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:29 INFO 139701951670080] Loss (name: value) logppx: 8.51003500131\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:29 INFO 139701951670080] #validation_score (54): 8.51003500131\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:29 INFO 139701951670080] patience losses:[8.541733922713842, 8.5056420741937107, 8.5525363628680893, 8.5137608454777638, 8.5427386944110584] min patience loss:8.50564207419 current loss:8.51003500131 absolute loss difference:0.00439292712089\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:29 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:29 INFO 139701951670080] Timing: train: 1.76s, val: 0.24s, epoch: 2.00s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:29 INFO 139701951670080] #progress_metric: host=algo-2, completed 36 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1944, \"sum\": 1944.0, \"min\": 1944}, \"Total Records Seen\": {\"count\": 1, \"max\": 57348, \"sum\": 57348.0, \"min\": 57348}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 108, \"sum\": 108.0, \"min\": 108}}, \"EndTime\": 1554830969.203007, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 53}, \"StartTime\": 1554830967.200822}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:29 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=530.363268795 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:29.203] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 54, \"duration\": 2002, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:29 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:29 INFO 139701951670080] # Starting training for epoch 55\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:28 INFO 139661896189760] # Finished training epoch 27 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:28 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:28 INFO 139661896189760] Loss (name: value) total: 8.03853246125\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:28 INFO 139661896189760] Loss (name: value) kld: 0.180960171995\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:28 INFO 139661896189760] Loss (name: value) recons: 7.85757226362\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:28 INFO 139661896189760] Loss (name: value) logppx: 8.03853246125\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:28 INFO 139661896189760] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=8.03853246125\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:28.755] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 26, \"duration\": 4407, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:28 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:28 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:28 INFO 139661896189760] Loss (name: value) total: 8.50430090489\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:28 INFO 139661896189760] Loss (name: value) kld: 0.14481175496\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:28 INFO 139661896189760] Loss (name: value) recons: 8.35948916704\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:29 INFO 139661896189760] Loss (name: value) logppx: 8.50430090489\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:29 INFO 139661896189760] #validation_score (27): 8.50430090489\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:29 INFO 139661896189760] patience losses:[8.5213560837965741, 8.5077092879857776, 8.5098227476462345, 8.5032583774664463, 8.5053380721654648] min patience loss:8.50325837747 current loss:8.50430090489 absolute loss difference:0.00104252741887\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:29 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:29 INFO 139661896189760] Timing: train: 4.10s, val: 0.25s, epoch: 4.35s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:29 INFO 139661896189760] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1917, \"sum\": 1917.0, \"min\": 1917}, \"Total Records Seen\": {\"count\": 1, \"max\": 57402, \"sum\": 57402.0, \"min\": 57402}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}}, \"EndTime\": 1554830969.001002, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 26}, \"StartTime\": 1554830964.654735}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:29 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=489.134908317 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:29.001] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 27, \"duration\": 4346, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:29 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:29 INFO 139661896189760] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:30 INFO 139701951670080] # Finished training epoch 55 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:30 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:30 INFO 139701951670080] Loss (name: value) total: 7.65369240796\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:30 INFO 139701951670080] Loss (name: value) kld: 0.173246901344\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:30 INFO 139701951670080] Loss (name: value) recons: 7.48044550154\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:30 INFO 139701951670080] Loss (name: value) logppx: 7.65369240796\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:30 INFO 139701951670080] #quality_metric: host=algo-2, epoch=55, train total_loss <loss>=7.65369240796\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:30.975] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 54, \"duration\": 2014, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:31 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:31 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:31 INFO 139701951670080] Loss (name: value) total: 8.53044449244\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:31 INFO 139701951670080] Loss (name: value) kld: 0.155982130613\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:31 INFO 139701951670080] Loss (name: value) recons: 8.374462695\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:31 INFO 139701951670080] Loss (name: value) logppx: 8.53044449244\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:31 INFO 139701951670080] #validation_score (55): 8.53044449244\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:31 INFO 139701951670080] patience losses:[8.5056420741937107, 8.5525363628680893, 8.5137608454777638, 8.5427386944110584, 8.5100350013146038] min patience loss:8.50564207419 current loss:8.53044449244 absolute loss difference:0.0248024182442\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:31 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:31 INFO 139701951670080] Timing: train: 1.77s, val: 0.24s, epoch: 2.02s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:31 INFO 139701951670080] #progress_metric: host=algo-2, completed 36 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1980, \"sum\": 1980.0, \"min\": 1980}, \"Total Records Seen\": {\"count\": 1, \"max\": 58410, \"sum\": 58410.0, \"min\": 58410}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 110, \"sum\": 110.0, \"min\": 110}}, \"EndTime\": 1554830971.220138, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 54}, \"StartTime\": 1554830969.203423}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:31 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=526.552995973 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:31.220] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 55, \"duration\": 2016, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:31 INFO 139701951670080] \u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:31 INFO 139701951670080] # Starting training for epoch 56\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] # Finished training epoch 56 on 1062 examples from 36 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] Loss (name: value) total: 7.64226064329\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] Loss (name: value) kld: 0.174033415759\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] Loss (name: value) recons: 7.46822726638\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] Loss (name: value) logppx: 7.64226064329\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] #quality_metric: host=algo-2, epoch=56, train total_loss <loss>=7.64226064329\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:29:33.024] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 55, \"duration\": 2048, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] Loss (name: value) total: 8.51366005922\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] Loss (name: value) kld: 0.15107882084\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] Loss (name: value) recons: 8.36258134108\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] Loss (name: value) logppx: 8.51366005922\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] #validation_score (56): 8.51366005922\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] patience losses:[8.5525363628680893, 8.5137608454777638, 8.5427386944110584, 8.5100350013146038, 8.5304444924379013] min patience loss:8.51003500131 current loss:8.51366005922 absolute loss difference:0.00362505790515\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] Bad epoch: loss has not improved (enough). Bad count:6\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] Timing: train: 1.80s, val: 0.24s, epoch: 2.05s\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] #progress_metric: host=algo-2, completed 100 % epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2016, \"sum\": 2016.0, \"min\": 2016}, \"Total Records Seen\": {\"count\": 1, \"max\": 59472, \"sum\": 59472.0, \"min\": 59472}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1062, \"sum\": 1062.0, \"min\": 1062}, \"Reset Count\": {\"count\": 1, \"max\": 112, \"sum\": 112.0, \"min\": 112}}, \"EndTime\": 1554830973.267349, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 55}, \"StartTime\": 1554830971.220482}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:29:33 INFO 139701951670080] #throughput_metric: host=algo-2, train throughput=518.8095398 records/second\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] # Finished training epoch 28 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] Loss (name: value) total: 8.04016897712\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] Loss (name: value) kld: 0.184141595151\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] Loss (name: value) recons: 7.85602738645\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] Loss (name: value) logppx: 8.04016897712\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=8.04016897712\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:33.051] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 27, \"duration\": 4295, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] Loss (name: value) total: 8.50779391558\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] Loss (name: value) kld: 0.146081048403\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] Loss (name: value) recons: 8.36171288123\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] Loss (name: value) logppx: 8.50779391558\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] #validation_score (28): 8.50779391558\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] patience losses:[8.5077092879857776, 8.5098227476462345, 8.5032583774664463, 8.5053380721654648, 8.5043009048853158] min patience loss:8.50325837747 current loss:8.50779391558 absolute loss difference:0.00453553811098\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] Timing: train: 4.05s, val: 0.22s, epoch: 4.27s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1988, \"sum\": 1988.0, \"min\": 1988}, \"Total Records Seen\": {\"count\": 1, \"max\": 59528, \"sum\": 59528.0, \"min\": 59528}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}}, \"EndTime\": 1554830973.274208, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 27}, \"StartTime\": 1554830969.001367}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=497.540377891 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:33.274] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 4272, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:33 INFO 139661896189760] # Starting training for epoch 29\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] # Finished training epoch 29 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] Loss (name: value) total: 7.99112879077\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] Loss (name: value) kld: 0.187911583224\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] Loss (name: value) recons: 7.80321720464\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] Loss (name: value) logppx: 7.99112879077\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=7.99112879077\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:36.644] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 28, \"duration\": 3592, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] Loss (name: value) total: 8.48573674911\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] Loss (name: value) kld: 0.158854467441\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] Loss (name: value) recons: 8.32688223032\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] Loss (name: value) logppx: 8.48573674911\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] #validation_score (29): 8.48573674911\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] patience losses:[8.5098227476462345, 8.5032583774664463, 8.5053380721654648, 8.5043009048853158, 8.5077939155774231] min patience loss:8.50325837747 current loss:8.48573674911 absolute loss difference:0.0175216283554\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] Timing: train: 3.37s, val: 0.24s, epoch: 3.61s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2059, \"sum\": 2059.0, \"min\": 2059}, \"Total Records Seen\": {\"count\": 1, \"max\": 61654, \"sum\": 61654.0, \"min\": 61654}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 58, \"sum\": 58.0, \"min\": 58}}, \"EndTime\": 1554830976.887468, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 28}, \"StartTime\": 1554830973.274501}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=588.410140348 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:36.887] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 3612, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:36 INFO 139661896189760] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] # Finished training epoch 30 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] Loss (name: value) total: 7.93504425192\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] Loss (name: value) kld: 0.191541299462\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] Loss (name: value) recons: 7.7435030082\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] Loss (name: value) logppx: 7.93504425192\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=7.93504425192\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:40.331] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 29, \"duration\": 3687, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] Loss (name: value) total: 8.46688044621\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] Loss (name: value) kld: 0.157826829568\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] Loss (name: value) recons: 8.30905346993\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] Loss (name: value) logppx: 8.46688044621\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] #validation_score (30): 8.46688044621\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] patience losses:[8.5032583774664463, 8.5053380721654648, 8.5043009048853158, 8.5077939155774231, 8.4857367491110782] min patience loss:8.48573674911 current loss:8.46688044621 absolute loss difference:0.0188563028971\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] Timing: train: 3.44s, val: 0.22s, epoch: 3.67s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2130, \"sum\": 2130.0, \"min\": 2130}, \"Total Records Seen\": {\"count\": 1, \"max\": 63780, \"sum\": 63780.0, \"min\": 63780}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}}, \"EndTime\": 1554830980.55635, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 29}, \"StartTime\": 1554830976.887777}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=579.493174696 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:40.556] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 3668, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:40 INFO 139661896189760] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:43 INFO 139661896189760] # Finished training epoch 31 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:43 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:43 INFO 139661896189760] Loss (name: value) total: 7.90564492812\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:43 INFO 139661896189760] Loss (name: value) kld: 0.192450489796\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:43 INFO 139661896189760] Loss (name: value) recons: 7.71319448265\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:43 INFO 139661896189760] Loss (name: value) logppx: 7.90564492812\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:43 INFO 139661896189760] #quality_metric: host=algo-1, epoch=31, train total_loss <loss>=7.90564492812\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:43.953] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 30, \"duration\": 3621, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:44 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:44 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:44 INFO 139661896189760] Loss (name: value) total: 8.45023025121\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:44 INFO 139661896189760] Loss (name: value) kld: 0.150821431478\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:44 INFO 139661896189760] Loss (name: value) recons: 8.29940889799\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:44 INFO 139661896189760] Loss (name: value) logppx: 8.45023025121\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:44 INFO 139661896189760] #validation_score (31): 8.45023025121\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:44 INFO 139661896189760] patience losses:[8.5053380721654648, 8.5043009048853158, 8.5077939155774231, 8.4857367491110782, 8.466880446213942] min patience loss:8.46688044621 current loss:8.45023025121 absolute loss difference:0.0166501949995\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:44 INFO 139661896189760] Timing: train: 3.40s, val: 0.23s, epoch: 3.63s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:44 INFO 139661896189760] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2201, \"sum\": 2201.0, \"min\": 2201}, \"Total Records Seen\": {\"count\": 1, \"max\": 65906, \"sum\": 65906.0, \"min\": 65906}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 62, \"sum\": 62.0, \"min\": 62}}, \"EndTime\": 1554830984.18823, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 30}, \"StartTime\": 1554830980.556641}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:44 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=585.395620495 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:44.188] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 31, \"duration\": 3631, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:44 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:44 INFO 139661896189760] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] # Finished training epoch 32 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] Loss (name: value) total: 7.88481602915\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] Loss (name: value) kld: 0.197026775365\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] Loss (name: value) recons: 7.68778922927\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] Loss (name: value) logppx: 7.88481602915\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] #quality_metric: host=algo-1, epoch=32, train total_loss <loss>=7.88481602915\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:47.307] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 31, \"duration\": 3353, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] Loss (name: value) total: 8.45007558969\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] Loss (name: value) kld: 0.164292798898\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] Loss (name: value) recons: 8.28578276023\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] Loss (name: value) logppx: 8.45007558969\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] #validation_score (32): 8.45007558969\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] patience losses:[8.5043009048853158, 8.5077939155774231, 8.4857367491110782, 8.466880446213942, 8.4502302512144425] min patience loss:8.45023025121 current loss:8.45007558969 absolute loss difference:0.000154661520932\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] Timing: train: 3.12s, val: 0.22s, epoch: 3.34s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2272, \"sum\": 2272.0, \"min\": 2272}, \"Total Records Seen\": {\"count\": 1, \"max\": 68032, \"sum\": 68032.0, \"min\": 68032}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}}, \"EndTime\": 1554830987.530233, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 31}, \"StartTime\": 1554830984.188544}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=636.176109057 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:47.530] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 3341, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:47 INFO 139661896189760] # Starting training for epoch 33\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] # Finished training epoch 33 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] Loss (name: value) total: 7.86878202197\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] Loss (name: value) kld: 0.200186223939\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] Loss (name: value) recons: 7.6685957931\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] Loss (name: value) logppx: 7.86878202197\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] #quality_metric: host=algo-1, epoch=33, train total_loss <loss>=7.86878202197\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:50.682] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 32, \"duration\": 3373, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] Loss (name: value) total: 8.44843041836\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] Loss (name: value) kld: 0.158649943425\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] Loss (name: value) recons: 8.28978040646\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] Loss (name: value) logppx: 8.44843041836\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] #validation_score (33): 8.44843041836\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] patience losses:[8.5077939155774231, 8.4857367491110782, 8.466880446213942, 8.4502302512144425, 8.45007558969351] min patience loss:8.45007558969 current loss:8.44843041836 absolute loss difference:0.00164517133664\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] Timing: train: 3.15s, val: 0.22s, epoch: 3.37s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2343, \"sum\": 2343.0, \"min\": 2343}, \"Total Records Seen\": {\"count\": 1, \"max\": 70158, \"sum\": 70158.0, \"min\": 70158}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 66, \"sum\": 66.0, \"min\": 66}}, \"EndTime\": 1554830990.905424, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 32}, \"StartTime\": 1554830987.530517}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=629.916047177 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:50.905] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 33, \"duration\": 3374, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:50 INFO 139661896189760] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] # Finished training epoch 34 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] Loss (name: value) total: 7.85675309589\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] Loss (name: value) kld: 0.202337251023\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] Loss (name: value) recons: 7.65441582908\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] Loss (name: value) logppx: 7.85675309589\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] #quality_metric: host=algo-1, epoch=34, train total_loss <loss>=7.85675309589\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:54.335] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 33, \"duration\": 3652, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] Loss (name: value) total: 8.43883146628\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] Loss (name: value) kld: 0.163024355204\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] Loss (name: value) recons: 8.27580711169\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] Loss (name: value) logppx: 8.43883146628\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] #validation_score (34): 8.43883146628\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] patience losses:[8.4857367491110782, 8.466880446213942, 8.4502302512144425, 8.45007558969351, 8.448430418356871] min patience loss:8.44843041836 current loss:8.43883146628 absolute loss difference:0.00959895207332\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] Timing: train: 3.43s, val: 0.23s, epoch: 3.66s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2414, \"sum\": 2414.0, \"min\": 2414}, \"Total Records Seen\": {\"count\": 1, \"max\": 72284, \"sum\": 72284.0, \"min\": 72284}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 68, \"sum\": 68.0, \"min\": 68}}, \"EndTime\": 1554830994.563052, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 33}, \"StartTime\": 1554830990.905725}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=581.275788938 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:54.563] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 3657, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:54 INFO 139661896189760] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:57 INFO 139661896189760] # Finished training epoch 35 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:57 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:57 INFO 139661896189760] Loss (name: value) total: 7.84465994678\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:57 INFO 139661896189760] Loss (name: value) kld: 0.203250053567\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:57 INFO 139661896189760] Loss (name: value) recons: 7.64140986769\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:57 INFO 139661896189760] Loss (name: value) logppx: 7.84465994678\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:57 INFO 139661896189760] #quality_metric: host=algo-1, epoch=35, train total_loss <loss>=7.84465994678\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:57.825] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 34, \"duration\": 3488, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:58 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:58 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:58 INFO 139661896189760] Loss (name: value) total: 8.43217100486\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:58 INFO 139661896189760] Loss (name: value) kld: 0.163068458973\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:58 INFO 139661896189760] Loss (name: value) recons: 8.2691025954\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:58 INFO 139661896189760] Loss (name: value) logppx: 8.43217100486\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:58 INFO 139661896189760] #validation_score (35): 8.43217100486\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:58 INFO 139661896189760] patience losses:[8.466880446213942, 8.4502302512144425, 8.45007558969351, 8.448430418356871, 8.4388314662835544] min patience loss:8.43883146628 current loss:8.43217100486 absolute loss difference:0.00666046142578\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:58 INFO 139661896189760] Timing: train: 3.26s, val: 0.22s, epoch: 3.49s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:58 INFO 139661896189760] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2485, \"sum\": 2485.0, \"min\": 2485}, \"Total Records Seen\": {\"count\": 1, \"max\": 74410, \"sum\": 74410.0, \"min\": 74410}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 70, \"sum\": 70.0, \"min\": 70}}, \"EndTime\": 1554830998.048797, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 34}, \"StartTime\": 1554830994.563336}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:58 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=609.935027837 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:29:58.049] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 3485, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:58 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:29:58 INFO 139661896189760] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] # Finished training epoch 36 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] Loss (name: value) total: 7.8313770133\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] Loss (name: value) kld: 0.20255693908\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] Loss (name: value) recons: 7.6288201059\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] Loss (name: value) logppx: 7.8313770133\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] #quality_metric: host=algo-1, epoch=36, train total_loss <loss>=7.8313770133\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:01.504] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 35, \"duration\": 3678, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] Loss (name: value) total: 8.43779539451\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] Loss (name: value) kld: 0.165420695452\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] Loss (name: value) recons: 8.27237470578\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] Loss (name: value) logppx: 8.43779539451\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] #validation_score (36): 8.43779539451\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] patience losses:[8.4502302512144425, 8.45007558969351, 8.448430418356871, 8.4388314662835544, 8.4321710048577732] min patience loss:8.43217100486 current loss:8.43779539451 absolute loss difference:0.00562438964844\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] Timing: train: 3.45s, val: 0.23s, epoch: 3.69s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2556, \"sum\": 2556.0, \"min\": 2556}, \"Total Records Seen\": {\"count\": 1, \"max\": 76536, \"sum\": 76536.0, \"min\": 76536}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}}, \"EndTime\": 1554831001.739193, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 35}, \"StartTime\": 1554830998.049136}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=576.117580809 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:01.739] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 3689, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:01 INFO 139661896189760] # Starting training for epoch 37\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] # Finished training epoch 37 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] Loss (name: value) total: 7.82466785986\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] Loss (name: value) kld: 0.205948635222\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] Loss (name: value) recons: 7.618719289\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] Loss (name: value) logppx: 7.82466785986\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] #quality_metric: host=algo-1, epoch=37, train total_loss <loss>=7.82466785986\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:05.180] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 36, \"duration\": 3676, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] Loss (name: value) total: 8.43726583628\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] Loss (name: value) kld: 0.170829372528\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] Loss (name: value) recons: 8.26643641545\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] Loss (name: value) logppx: 8.43726583628\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] #validation_score (37): 8.43726583628\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] patience losses:[8.45007558969351, 8.448430418356871, 8.4388314662835544, 8.4321710048577732, 8.4377953945062103] min patience loss:8.43217100486 current loss:8.43726583628 absolute loss difference:0.00509483141777\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] Timing: train: 3.44s, val: 0.23s, epoch: 3.67s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2627, \"sum\": 2627.0, \"min\": 2627}, \"Total Records Seen\": {\"count\": 1, \"max\": 78662, \"sum\": 78662.0, \"min\": 78662}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 74, \"sum\": 74.0, \"min\": 74}}, \"EndTime\": 1554831005.408336, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 36}, \"StartTime\": 1554831001.739492}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=579.44881531 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:05.408] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 37, \"duration\": 3668, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:05 INFO 139661896189760] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:08 INFO 139661896189760] # Finished training epoch 38 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:08 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:08 INFO 139661896189760] Loss (name: value) total: 7.81709541715\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:08 INFO 139661896189760] Loss (name: value) kld: 0.20809909984\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:08 INFO 139661896189760] Loss (name: value) recons: 7.60899635279\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:08 INFO 139661896189760] Loss (name: value) logppx: 7.81709541715\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:08 INFO 139661896189760] #quality_metric: host=algo-1, epoch=38, train total_loss <loss>=7.81709541715\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:08.774] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 37, \"duration\": 3593, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:09 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:09 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:09 INFO 139661896189760] Loss (name: value) total: 8.43236655211\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:09 INFO 139661896189760] Loss (name: value) kld: 0.1731540252\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:09 INFO 139661896189760] Loss (name: value) recons: 8.25921259171\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:09 INFO 139661896189760] Loss (name: value) logppx: 8.43236655211\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:09 INFO 139661896189760] #validation_score (38): 8.43236655211\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:09 INFO 139661896189760] patience losses:[8.448430418356871, 8.4388314662835544, 8.4321710048577732, 8.4377953945062103, 8.4372658362755413] min patience loss:8.43217100486 current loss:8.43236655211 absolute loss difference:0.0001955472506\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:09 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:09 INFO 139661896189760] Timing: train: 3.37s, val: 0.26s, epoch: 3.62s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:09 INFO 139661896189760] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2698, \"sum\": 2698.0, \"min\": 2698}, \"Total Records Seen\": {\"count\": 1, \"max\": 80788, \"sum\": 80788.0, \"min\": 80788}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 76, \"sum\": 76.0, \"min\": 76}}, \"EndTime\": 1554831009.031904, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 37}, \"StartTime\": 1554831005.408676}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:09 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=586.746757676 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:09.032] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 3623, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:09 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:09 INFO 139661896189760] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] # Finished training epoch 39 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] Loss (name: value) total: 7.81006939669\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] Loss (name: value) kld: 0.208411888002\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] Loss (name: value) recons: 7.60165754148\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] Loss (name: value) logppx: 7.81006939669\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] #quality_metric: host=algo-1, epoch=39, train total_loss <loss>=7.81006939669\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:12.494] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 38, \"duration\": 3719, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] Loss (name: value) total: 8.42915981977\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] Loss (name: value) kld: 0.165646432608\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] Loss (name: value) recons: 8.26351341834\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] Loss (name: value) logppx: 8.42915981977\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] #validation_score (39): 8.42915981977\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] patience losses:[8.4388314662835544, 8.4321710048577732, 8.4377953945062103, 8.4372658362755413, 8.4323665521083733] min patience loss:8.43217100486 current loss:8.42915981977 absolute loss difference:0.00301118508363\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] Timing: train: 3.46s, val: 0.23s, epoch: 3.69s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2769, \"sum\": 2769.0, \"min\": 2769}, \"Total Records Seen\": {\"count\": 1, \"max\": 82914, \"sum\": 82914.0, \"min\": 82914}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 78, \"sum\": 78.0, \"min\": 78}}, \"EndTime\": 1554831012.727496, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 38}, \"StartTime\": 1554831009.032209}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=575.302978048 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:12.727] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 39, \"duration\": 3695, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:12 INFO 139661896189760] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] # Finished training epoch 40 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] Loss (name: value) total: 7.8039884612\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] Loss (name: value) kld: 0.210510896293\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] Loss (name: value) recons: 7.59347758405\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] Loss (name: value) logppx: 7.8039884612\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] #quality_metric: host=algo-1, epoch=40, train total_loss <loss>=7.8039884612\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:16.187] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 39, \"duration\": 3692, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] Loss (name: value) total: 8.42435369247\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] Loss (name: value) kld: 0.168818906637\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] Loss (name: value) recons: 8.25553483229\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] Loss (name: value) logppx: 8.42435369247\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] #validation_score (40): 8.42435369247\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] patience losses:[8.4321710048577732, 8.4377953945062103, 8.4372658362755413, 8.4323665521083733, 8.4291598197741386] min patience loss:8.42915981977 current loss:8.42435369247 absolute loss difference:0.00480612730369\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] Timing: train: 3.46s, val: 0.23s, epoch: 3.69s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2840, \"sum\": 2840.0, \"min\": 2840}, \"Total Records Seen\": {\"count\": 1, \"max\": 85040, \"sum\": 85040.0, \"min\": 85040}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 80, \"sum\": 80.0, \"min\": 80}}, \"EndTime\": 1554831016.419226, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 39}, \"StartTime\": 1554831012.727793}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=575.903931646 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:16.419] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 3691, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:16 INFO 139661896189760] # Starting training for epoch 41\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/09/2019 17:30:19 INFO 139661896189760] # Finished training epoch 41 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:19 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:19 INFO 139661896189760] Loss (name: value) total: 7.79924741933\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:19 INFO 139661896189760] Loss (name: value) kld: 0.212242428126\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:19 INFO 139661896189760] Loss (name: value) recons: 7.58700500632\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:19 INFO 139661896189760] Loss (name: value) logppx: 7.79924741933\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:19 INFO 139661896189760] #quality_metric: host=algo-1, epoch=41, train total_loss <loss>=7.79924741933\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:19.908] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 40, \"duration\": 3720, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:20 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:20 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:20 INFO 139661896189760] Loss (name: value) total: 8.42087230193\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:20 INFO 139661896189760] Loss (name: value) kld: 0.179111872575\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:20 INFO 139661896189760] Loss (name: value) recons: 8.24176044953\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:20 INFO 139661896189760] Loss (name: value) logppx: 8.42087230193\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:20 INFO 139661896189760] #validation_score (41): 8.42087230193\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:20 INFO 139661896189760] patience losses:[8.4377953945062103, 8.4372658362755413, 8.4323665521083733, 8.4291598197741386, 8.4243536924704525] min patience loss:8.42435369247 current loss:8.42087230193 absolute loss difference:0.00348139053736\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:20 INFO 139661896189760] Timing: train: 3.49s, val: 0.24s, epoch: 3.73s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:20 INFO 139661896189760] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2911, \"sum\": 2911.0, \"min\": 2911}, \"Total Records Seen\": {\"count\": 1, \"max\": 87166, \"sum\": 87166.0, \"min\": 87166}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}}, \"EndTime\": 1554831020.14791, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 40}, \"StartTime\": 1554831016.419579}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:20 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=570.206011549 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:20.148] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 3728, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:20 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:20 INFO 139661896189760] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] # Finished training epoch 42 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] Loss (name: value) total: 7.78975132329\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] Loss (name: value) kld: 0.211405981874\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] Loss (name: value) recons: 7.57834533548\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] Loss (name: value) logppx: 7.78975132329\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] #quality_metric: host=algo-1, epoch=42, train total_loss <loss>=7.78975132329\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:23.606] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 41, \"duration\": 3697, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] Loss (name: value) total: 8.41101938883\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] Loss (name: value) kld: 0.16372379034\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] Loss (name: value) recons: 8.2472955557\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] Loss (name: value) logppx: 8.41101938883\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] #validation_score (42): 8.41101938883\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] patience losses:[8.4372658362755413, 8.4323665521083733, 8.4291598197741386, 8.4243536924704525, 8.4208723019330929] min patience loss:8.42087230193 current loss:8.41101938883 absolute loss difference:0.00985291309846\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] Timing: train: 3.46s, val: 0.26s, epoch: 3.71s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2982, \"sum\": 2982.0, \"min\": 2982}, \"Total Records Seen\": {\"count\": 1, \"max\": 89292, \"sum\": 89292.0, \"min\": 89292}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 84, \"sum\": 84.0, \"min\": 84}}, \"EndTime\": 1554831023.862462, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 41}, \"StartTime\": 1554831020.14821}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=572.367436788 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:23.862] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 42, \"duration\": 3714, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:23 INFO 139661896189760] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] # Finished training epoch 43 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] Loss (name: value) total: 7.7853361676\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] Loss (name: value) kld: 0.211482806609\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] Loss (name: value) recons: 7.57385332708\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] Loss (name: value) logppx: 7.7853361676\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] #quality_metric: host=algo-1, epoch=43, train total_loss <loss>=7.7853361676\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:27.345] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 42, \"duration\": 3739, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] Loss (name: value) total: 8.43184290177\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] Loss (name: value) kld: 0.176893380361\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] Loss (name: value) recons: 8.25494971642\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] Loss (name: value) logppx: 8.43184290177\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] #validation_score (43): 8.43184290177\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] patience losses:[8.4323665521083733, 8.4291598197741386, 8.4243536924704525, 8.4208723019330929, 8.4110193888346352] min patience loss:8.41101938883 current loss:8.43184290177 absolute loss difference:0.0208235129332\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] Timing: train: 3.48s, val: 0.22s, epoch: 3.70s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3053, \"sum\": 3053.0, \"min\": 3053}, \"Total Records Seen\": {\"count\": 1, \"max\": 91418, \"sum\": 91418.0, \"min\": 91418}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 86, \"sum\": 86.0, \"min\": 86}}, \"EndTime\": 1554831027.564584, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 42}, \"StartTime\": 1554831023.862819}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=574.292544635 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:27.564] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 43, \"duration\": 3701, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:27 INFO 139661896189760] # Starting training for epoch 44\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] # Finished training epoch 44 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] Loss (name: value) total: 7.77957409066\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] Loss (name: value) kld: 0.212648208488\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] Loss (name: value) recons: 7.56692587266\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] Loss (name: value) logppx: 7.77957409066\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] #quality_metric: host=algo-1, epoch=44, train total_loss <loss>=7.77957409066\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:31.037] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 43, \"duration\": 3691, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] Loss (name: value) total: 8.43005907108\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] Loss (name: value) kld: 0.179763306104\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] Loss (name: value) recons: 8.25029570751\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] Loss (name: value) logppx: 8.43005907108\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] #validation_score (44): 8.43005907108\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] patience losses:[8.4291598197741386, 8.4243536924704525, 8.4208723019330929, 8.4110193888346352, 8.431842901767828] min patience loss:8.41101938883 current loss:8.43005907108 absolute loss difference:0.0190396822416\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] Timing: train: 3.47s, val: 0.24s, epoch: 3.71s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3124, \"sum\": 3124.0, \"min\": 3124}, \"Total Records Seen\": {\"count\": 1, \"max\": 93544, \"sum\": 93544.0, \"min\": 93544}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 88, \"sum\": 88.0, \"min\": 88}}, \"EndTime\": 1554831031.273195, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 43}, \"StartTime\": 1554831027.564897}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=573.282226676 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:31.273] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 3708, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:31 INFO 139661896189760] # Starting training for epoch 45\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] # Finished training epoch 45 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] Loss (name: value) total: 7.77295310867\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] Loss (name: value) kld: 0.211221278217\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] Loss (name: value) recons: 7.56173183817\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] Loss (name: value) logppx: 7.77295310867\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] #quality_metric: host=algo-1, epoch=45, train total_loss <loss>=7.77295310867\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:34.738] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 44, \"duration\": 3700, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] Loss (name: value) total: 8.41057954446\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] Loss (name: value) kld: 0.172320546248\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] Loss (name: value) recons: 8.23825898782\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] Loss (name: value) logppx: 8.41057954446\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] #validation_score (45): 8.41057954446\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] patience losses:[8.4243536924704525, 8.4208723019330929, 8.4110193888346352, 8.431842901767828, 8.4300590710762222] min patience loss:8.41101938883 current loss:8.41057954446 absolute loss difference:0.000439844376002\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] Timing: train: 3.46s, val: 0.22s, epoch: 3.69s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3195, \"sum\": 3195.0, \"min\": 3195}, \"Total Records Seen\": {\"count\": 1, \"max\": 95670, \"sum\": 95670.0, \"min\": 95670}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 90, \"sum\": 90.0, \"min\": 90}}, \"EndTime\": 1554831034.963322, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 44}, \"StartTime\": 1554831031.273525}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=576.160054182 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:34.963] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 45, \"duration\": 3689, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:34 INFO 139661896189760] # Starting training for epoch 46\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] # Finished training epoch 46 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] Loss (name: value) total: 7.77190882723\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] Loss (name: value) kld: 0.215349636951\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] Loss (name: value) recons: 7.55655923763\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] Loss (name: value) logppx: 7.77190882723\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] #quality_metric: host=algo-1, epoch=46, train total_loss <loss>=7.77190882723\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:38.404] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 45, \"duration\": 3664, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] Loss (name: value) total: 8.41733519725\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] Loss (name: value) kld: 0.177670250795\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] Loss (name: value) recons: 8.23966490917\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] Loss (name: value) logppx: 8.41733519725\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] #validation_score (46): 8.41733519725\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] patience losses:[8.4208723019330929, 8.4110193888346352, 8.431842901767828, 8.4300590710762222, 8.4105795444586331] min patience loss:8.41057954446 current loss:8.41733519725 absolute loss difference:0.00675565279447\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] Timing: train: 3.44s, val: 0.24s, epoch: 3.68s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3266, \"sum\": 3266.0, \"min\": 3266}, \"Total Records Seen\": {\"count\": 1, \"max\": 97796, \"sum\": 97796.0, \"min\": 97796}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 92, \"sum\": 92.0, \"min\": 92}}, \"EndTime\": 1554831038.642231, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 45}, \"StartTime\": 1554831034.96364}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=577.914474061 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:38.642] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 46, \"duration\": 3678, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:38 INFO 139661896189760] # Starting training for epoch 47\u001b[0m\n",
      "\n",
      "2019-04-09 17:30:48 Uploading - Uploading generated training model\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] # Finished training epoch 47 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] Loss (name: value) total: 7.76253485881\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] Loss (name: value) kld: 0.210953361104\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] Loss (name: value) recons: 7.55158153409\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] Loss (name: value) logppx: 7.76253485881\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] #quality_metric: host=algo-1, epoch=47, train total_loss <loss>=7.76253485881\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:42.085] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 46, \"duration\": 3680, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] Loss (name: value) total: 8.41955304268\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] Loss (name: value) kld: 0.173353306452\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] Loss (name: value) recons: 8.24619985727\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] Loss (name: value) logppx: 8.41955304268\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] #validation_score (47): 8.41955304268\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] patience losses:[8.4110193888346352, 8.431842901767828, 8.4300590710762222, 8.4105795444586331, 8.4173351972531041] min patience loss:8.41057954446 current loss:8.41955304268 absolute loss difference:0.00897349822216\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] Timing: train: 3.44s, val: 0.24s, epoch: 3.68s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3337, \"sum\": 3337.0, \"min\": 3337}, \"Total Records Seen\": {\"count\": 1, \"max\": 99922, \"sum\": 99922.0, \"min\": 99922}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 94, \"sum\": 94.0, \"min\": 94}}, \"EndTime\": 1554831042.327698, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 46}, \"StartTime\": 1554831038.642517}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=576.882314482 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:42.327] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 3685, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] \u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:42 INFO 139661896189760] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:30:46.068] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 56, \"duration\": 73043, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] Loss (name: value) total: 8.40675995656\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] Loss (name: value) kld: 0.160669060242\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] Loss (name: value) recons: 8.24609097212\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] Loss (name: value) logppx: 8.40675995656\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] #quality_metric: host=algo-2, epoch=56, validation total_loss <loss>=8.40675995656\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] Loss of server-side model: 8.40675995656\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] Best model based on early stopping at epoch 56. Best loss: 8.40675995656\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] Topics from epoch:final (num_topics:3) [wetc 0.34, tu 1.00]:\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] [0.61, 1.00] sont ses ciapres lexpediteur destinataires etablis lintention erreur toutes recevez aurait titre ete jointes declinent lhypothese linternet lintegrite dassurer modifie\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] [0.27, 1.00] clicking web chart represent affiliates contained sources prohibited accurate solicitation based officer instruction delete corp reliable offer immediately andor link\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] [0.15, 1.00] tail flip head coin flipping bidoffer resource pending pao eva yippee request approval playing create sooo hookie double walk volunteer\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] Serializing model to /opt/ml/model/model_algo-2\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] Saved checkpoint to \"/tmp/tmpQ02pz6/state-0001.params\"\u001b[0m\n",
      "\u001b[32m[04/09/2019 17:30:46 INFO 139701951670080] Test data is not provided.\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:30:46.350] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 75129, \"num_examples\": 36}\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:30:46.350] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"duration\": 209966, \"num_epochs\": 57, \"num_examples\": 2017}\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:30:46.350] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 57, \"duration\": 282, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[32m[2019-04-09 17:30:46.350] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"duration\": 209980, \"num_epochs\": 58, \"num_examples\": 799}\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 210613.8210296631, \"sum\": 210613.8210296631, \"min\": 210613.8210296631}, \"finalize.time\": {\"count\": 1, \"max\": 277.79293060302734, \"sum\": 277.79293060302734, \"min\": 277.79293060302734}, \"initialize.time\": {\"count\": 1, \"max\": 16211.827039718628, \"sum\": 16211.827039718628, \"min\": 16211.827039718628}, \"model.serialize.time\": {\"count\": 1, \"max\": 5.173921585083008, \"sum\": 5.173921585083008, \"min\": 5.173921585083008}, \"setuptime\": {\"count\": 1, \"max\": 547.6069450378418, \"sum\": 547.6069450378418, \"min\": 547.6069450378418}, \"early_stop.time\": {\"count\": 56, \"max\": 304.84604835510254, \"sum\": 14330.455541610718, \"min\": 225.33488273620605}, \"update.time\": {\"count\": 56, \"max\": 2357.555150985718, \"sum\": 120700.10590553284, \"min\": 1914.8459434509277}, \"epochs\": {\"count\": 1, \"max\": 150, \"sum\": 150.0, \"min\": 150}, \"model.score.time\": {\"count\": 57, \"max\": 304.23498153686523, \"sum\": 14451.45058631897, \"min\": 220.689058303833}}, \"EndTime\": 1554831046.35185, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1554830836.316038}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:45 INFO 139661896189760] # Finished training epoch 48 on 2126 examples from 71 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:45 INFO 139661896189760] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:45 INFO 139661896189760] Loss (name: value) total: 7.76042633773\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:45 INFO 139661896189760] Loss (name: value) kld: 0.212546843318\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:45 INFO 139661896189760] Loss (name: value) recons: 7.5478794868\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:45 INFO 139661896189760] Loss (name: value) logppx: 7.76042633773\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:45 INFO 139661896189760] #quality_metric: host=algo-1, epoch=48, train total_loss <loss>=7.76042633773\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:45.835] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 47, \"duration\": 3749, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Loss (name: value) total: 8.41213253706\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Loss (name: value) kld: 0.170817825122\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Loss (name: value) recons: 8.24131461902\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Loss (name: value) logppx: 8.41213253706\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] #validation_score (48): 8.41213253706\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] patience losses:[8.431842901767828, 8.4300590710762222, 8.4105795444586331, 8.4173351972531041, 8.4195530426807892] min patience loss:8.41057954446 current loss:8.41213253706 absolute loss difference:0.00155299260066\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Bad epoch: loss has not improved (enough). Bad count:6\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Timing: train: 3.51s, val: 0.23s, epoch: 3.74s\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3408, \"sum\": 3408.0, \"min\": 3408}, \"Total Records Seen\": {\"count\": 1, \"max\": 102048, \"sum\": 102048.0, \"min\": 102048}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2126, \"sum\": 2126.0, \"min\": 2126}, \"Reset Count\": {\"count\": 1, \"max\": 96, \"sum\": 96.0, \"min\": 96}}, \"EndTime\": 1554831046.065416, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 47}, \"StartTime\": 1554831042.32802}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] #throughput_metric: host=algo-1, train throughput=568.823342686 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:46.069] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 48, \"duration\": 233, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Finished scoring on 390 examples from 13 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Loss (name: value) total: 8.41261264116\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Loss (name: value) kld: 0.160669060242\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Loss (name: value) recons: 8.2519434611\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Loss (name: value) logppx: 8.41261264116\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] #quality_metric: host=algo-1, epoch=48, validation total_loss <loss>=8.41261264116\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Loss of server-side model: 8.41261264116\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Best model based on early stopping at epoch 45. Best loss: 8.41057954446\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Topics from epoch:final (num_topics:3) [wetc 0.34, tu 1.00]:\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] [0.61, 1.00] sont ses ciapres lexpediteur destinataires etablis lintention erreur toutes recevez aurait titre ete jointes declinent linternet lhypothese lintegrite dassurer modifie\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] [0.24, 1.00] clicking web chart represent contained sources affiliates prohibited accurate solicitation based officer instruction delete offer corp unsubscribe andor link receive\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] [0.17, 1.00] tail flip head coin flipping bidoffer pao resource eva yippee pending request approval playing double walk play game hookie sooo\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Saved checkpoint to \"/tmp/tmpP3fR6p/state-0001.params\"\u001b[0m\n",
      "\u001b[31m[04/09/2019 17:30:46 INFO 139661896189760] Test data is not provided.\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:46.337] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 48, \"duration\": 4009, \"num_examples\": 71}\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:46.337] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"duration\": 209948, \"num_epochs\": 49, \"num_examples\": 3409}\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:46.337] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 49, \"duration\": 268, \"num_examples\": 14}\u001b[0m\n",
      "\u001b[31m[2019-04-09 17:30:46.337] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"duration\": 209927, \"num_epochs\": 50, \"num_examples\": 687}\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 210609.0748310089, \"sum\": 210609.0748310089, \"min\": 210609.0748310089}, \"finalize.time\": {\"count\": 1, \"max\": 266.0198211669922, \"sum\": 266.0198211669922, \"min\": 266.0198211669922}, \"initialize.time\": {\"count\": 1, \"max\": 16200.185060501099, \"sum\": 16200.185060501099, \"min\": 16200.185060501099}, \"model.serialize.time\": {\"count\": 1, \"max\": 5.148887634277344, \"sum\": 5.148887634277344, \"min\": 5.148887634277344}, \"setuptime\": {\"count\": 1, \"max\": 561.0899925231934, \"sum\": 561.0899925231934, \"min\": 561.0899925231934}, \"early_stop.time\": {\"count\": 48, \"max\": 307.5909614562988, \"sum\": 11885.593175888062, \"min\": 218.6908721923828}, \"update.time\": {\"count\": 48, \"max\": 4452.049970626831, \"sum\": 193470.44444084167, \"min\": 3341.5188789367676}, \"epochs\": {\"count\": 1, \"max\": 150, \"sum\": 150.0, \"min\": 150}, \"model.score.time\": {\"count\": 49, \"max\": 307.096004486084, \"sum\": 11978.602886199951, \"min\": 218.29509735107422}}, \"EndTime\": 1554831046.339024, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1554830836.339296}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-04-09 17:30:55 Completed - Training job completed\n",
      "Billable seconds: 467\n"
     ]
    }
   ],
   "source": [
    "ntm.fit({'train': s3_train, 'validation': s3_val, 'auxiliary': s3_aux})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: ntm-2019-04-09-17-24-19-713\n"
     ]
    }
   ],
   "source": [
    "print('Training job name: {}'.format(ntm.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hosting and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: ntm-2019-04-09-18-52-54-810\n",
      "INFO:sagemaker:Creating endpoint with name ntm-2019-04-09-17-24-19-713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: ntm-2019-04-09-17-24-19-713\n"
     ]
    }
   ],
   "source": [
    "print('Endpoint name: {}'.format(ntm_predictor.endpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert test vectors from compressed sparse matrix to dense matrix\n",
    "test_data = np.array(test_vectors.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'topic_weights': [0.0780238435, 0.5128391385, 0.4091370702]}, {'topic_weights': [0.0647637397, 0.5575146079, 0.3777216673]}, {'topic_weights': [0.0770179257, 0.2474122494, 0.6755698323]}, {'topic_weights': [0.0907187015, 0.6687077284, 0.240573585]}]}\n"
     ]
    }
   ],
   "source": [
    "results = ntm_predictor.predict(test_data[1:5])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07802384 0.51283914 0.40913707]\n",
      " [0.06476374 0.55751461 0.37772167]\n",
      " [0.07701793 0.24741225 0.67556983]\n",
      " [0.0907187  0.66870773 0.24057359]]\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Topic ID')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fs=12\n",
    "df=pd.DataFrame(predictions.T)\n",
    "df.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "plt.xlabel('Topic ID', fontsize=fs+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word Cloud from Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/6d/7932788aea1293015548631495a1b44e588127fedcf4221a5dcdfa411e7b/mxnet-1.4.0.post0-py2.py3-none-manylinux1_x86_64.whl (28.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 28.4MB 1.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from mxnet) (2.20.1)\n",
      "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
      "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
      "Collecting numpy<1.15.0,>=1.8.2 (from mxnet)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c4/395ebb218053ba44d64935b3729bc88241ec279915e72100c5979db10945/numpy-1.14.6-cp36-cp36m-manylinux1_x86_64.whl (13.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.8MB 7.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.20.0->mxnet) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.20.0->mxnet) (2.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.20.0->mxnet) (1.23)\n",
      "Installing collected packages: graphviz, numpy, mxnet\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\n",
      "Successfully installed graphviz-0.8.4 mxnet-1.4.0.post0 numpy-1.14.6\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet\n",
    "!pip install wordcloud\n",
    "import wordcloud as wc\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enronemails/output/ntm-2019-04-09-17-24-19-713/output/model.tar.gz'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Determine the location of the model output\n",
    "current_job_name = ntm.latest_training_job.job_name\n",
    "\n",
    "model_path = os.path.join(output_prefix, current_job_name, 'output/model.tar.gz')\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the model\n",
    "boto3.resource('s3').Bucket(bucket).download_file(model_path, 'downloaded_model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_algo-2\r\n",
      "model_algo-1\r\n"
     ]
    }
   ],
   "source": [
    "#unzip the model output\n",
    "!tar -xzvf 'downloaded_model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  model_algo-1\r\n",
      " extracting: meta.json               \r\n",
      " extracting: symbol.json             \r\n",
      " extracting: params                  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip -o model_algo-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mx.ndarray.load('params')\n",
    "W = model['arg:projection_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-1.6004856 -2.60862   -1.9256244]\n",
       " [-1.5195873 -2.801778  -2.5669572]\n",
       " [-1.5795462 -2.8432672 -2.3337717]\n",
       " ...\n",
       " [-1.5052562 -2.8511627 -2.35806  ]\n",
       " [-1.5185319 -2.8104594 -2.4379675]\n",
       " [-1.5643051 -2.8513684 -2.3724968]]\n",
       "<NDArray 17524x3 @cpu(0)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving word distributions for each of the latent topics \n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17524"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vocabulary list\n",
    "vocab_list = pd.read_table(vocab_op_fn, header=None)\n",
    "vocab_list = vocab_list[0].tolist()\n",
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through the vocabulary list to create dictionary of key values for each of the words in vocabulary\n",
    "word_to_id = {}\n",
    "\n",
    "for i, v in enumerate(vocab_list):\n",
    "    #print(\"Index and Value\", i, v)\n",
    "    word_to_id[v] = i\n",
    "    \n",
    "limit = 24\n",
    "n_col = 4\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aaa': 0,\n",
       " 'aactive': 1,\n",
       " 'aadvantage': 2,\n",
       " 'aaron': 3,\n",
       " 'aarp': 4,\n",
       " 'aba': 5,\n",
       " 'abacus': 6,\n",
       " 'abandon': 7,\n",
       " 'abandoned': 8,\n",
       " 'abandoning': 9,\n",
       " 'abated': 10,\n",
       " 'abb': 11,\n",
       " 'abbott': 12,\n",
       " 'abbreviated': 13,\n",
       " 'abbreviation': 14,\n",
       " 'abc': 15,\n",
       " 'abel': 16,\n",
       " 'aberration': 17,\n",
       " 'abide': 18,\n",
       " 'abilene': 19,\n",
       " 'abilities': 20,\n",
       " 'ability': 21,\n",
       " 'able': 22,\n",
       " 'abn': 23,\n",
       " 'aboard': 24,\n",
       " 'abolish': 25,\n",
       " 'abolishing': 26,\n",
       " 'abound': 27,\n",
       " 'abovemarket': 28,\n",
       " 'abraham': 29,\n",
       " 'abreast': 30,\n",
       " 'abroad': 31,\n",
       " 'abrogate': 32,\n",
       " 'abrupt': 33,\n",
       " 'abruptly': 34,\n",
       " 'absence': 35,\n",
       " 'absences': 36,\n",
       " 'absent': 37,\n",
       " 'absentee': 38,\n",
       " 'absolute': 39,\n",
       " 'absolutely': 40,\n",
       " 'absorb': 41,\n",
       " 'absorbed': 42,\n",
       " 'absorbing': 43,\n",
       " 'abstract': 44,\n",
       " 'abt': 45,\n",
       " 'abundance': 46,\n",
       " 'abundant': 47,\n",
       " 'abuse': 48,\n",
       " 'abuses': 49,\n",
       " 'academia': 50,\n",
       " 'academic': 51,\n",
       " 'academy': 52,\n",
       " 'acc': 53,\n",
       " 'accelerate': 54,\n",
       " 'accelerated': 55,\n",
       " 'accelerates': 56,\n",
       " 'accelerating': 57,\n",
       " 'acceleration': 58,\n",
       " 'accent': 59,\n",
       " 'accenture': 60,\n",
       " 'accept': 61,\n",
       " 'acceptable': 62,\n",
       " 'acceptance': 63,\n",
       " 'acceptances': 64,\n",
       " 'accepted': 65,\n",
       " 'accepting': 66,\n",
       " 'access': 67,\n",
       " 'accessed': 68,\n",
       " 'accessibility': 69,\n",
       " 'accessible': 70,\n",
       " 'accessing': 71,\n",
       " 'accessories': 72,\n",
       " 'accessory': 73,\n",
       " 'accident': 74,\n",
       " 'accidental': 75,\n",
       " 'accidentally': 76,\n",
       " 'acclaimed': 77,\n",
       " 'accolades': 78,\n",
       " 'accommodate': 79,\n",
       " 'accommodating': 80,\n",
       " 'accommodation': 81,\n",
       " 'accomodate': 82,\n",
       " 'accompanied': 83,\n",
       " 'accompany': 84,\n",
       " 'accompanying': 85,\n",
       " 'accomplish': 86,\n",
       " 'accomplished': 87,\n",
       " 'accomplishing': 88,\n",
       " 'accomplishment': 89,\n",
       " 'accord': 90,\n",
       " 'accordance': 91,\n",
       " 'according': 92,\n",
       " 'accordion': 93,\n",
       " 'account': 94,\n",
       " 'accountability': 95,\n",
       " 'accountable': 96,\n",
       " 'accountant': 97,\n",
       " 'accounted': 98,\n",
       " 'accounting': 99,\n",
       " 'accredited': 100,\n",
       " 'accrual': 101,\n",
       " 'accrue': 102,\n",
       " 'accrued': 103,\n",
       " 'acct': 104,\n",
       " 'accum': 105,\n",
       " 'accumulate': 106,\n",
       " 'accuracy': 107,\n",
       " 'accurate': 108,\n",
       " 'accurately': 109,\n",
       " 'accusation': 110,\n",
       " 'accused': 111,\n",
       " 'accuses': 112,\n",
       " 'acevedo': 113,\n",
       " 'achievable': 114,\n",
       " 'achieve': 115,\n",
       " 'achieved': 116,\n",
       " 'achievement': 117,\n",
       " 'achieves': 118,\n",
       " 'achieving': 119,\n",
       " 'achilles': 120,\n",
       " 'acid': 121,\n",
       " 'acidity': 122,\n",
       " 'ackerman': 123,\n",
       " 'acknowledge': 124,\n",
       " 'acknowledged': 125,\n",
       " 'acknowledges': 126,\n",
       " 'acknowledging': 127,\n",
       " 'acknowledgment': 128,\n",
       " 'acn': 129,\n",
       " 'acquaintance': 130,\n",
       " 'acquire': 131,\n",
       " 'acquired': 132,\n",
       " 'acquirer': 133,\n",
       " 'acquires': 134,\n",
       " 'acquiring': 135,\n",
       " 'acquisition': 136,\n",
       " 'acre': 137,\n",
       " 'acreage': 138,\n",
       " 'acres': 139,\n",
       " 'acrobat': 140,\n",
       " 'acronym': 141,\n",
       " 'acrosstheboard': 142,\n",
       " 'act': 143,\n",
       " 'acted': 144,\n",
       " 'acting': 145,\n",
       " 'action': 146,\n",
       " 'actionable': 147,\n",
       " 'activate': 148,\n",
       " 'activated': 149,\n",
       " 'activation': 150,\n",
       " 'active': 151,\n",
       " 'actively': 152,\n",
       " 'actives': 153,\n",
       " 'activism': 154,\n",
       " 'activist': 155,\n",
       " 'activities': 156,\n",
       " 'activity': 157,\n",
       " 'actor': 158,\n",
       " 'actress': 159,\n",
       " 'actual': 160,\n",
       " 'actually': 161,\n",
       " 'acute': 162,\n",
       " 'ada': 163,\n",
       " 'adam': 164,\n",
       " 'adapt': 165,\n",
       " 'adaptation': 166,\n",
       " 'adapted': 167,\n",
       " 'adapter': 168,\n",
       " 'adaptive': 169,\n",
       " 'add': 170,\n",
       " 'adddrop': 171,\n",
       " 'added': 172,\n",
       " 'addedvalue': 173,\n",
       " 'addendum': 174,\n",
       " 'adder': 175,\n",
       " 'addictive': 176,\n",
       " 'addin': 177,\n",
       " 'adding': 178,\n",
       " 'addison': 179,\n",
       " 'addition': 180,\n",
       " 'additional': 181,\n",
       " 'additionally': 182,\n",
       " 'additive': 183,\n",
       " 'addon': 184,\n",
       " 'address': 185,\n",
       " 'addressed': 186,\n",
       " 'addressee': 187,\n",
       " 'addressees': 188,\n",
       " 'addresses': 189,\n",
       " 'addressing': 190,\n",
       " 'addtion': 191,\n",
       " 'addtional': 192,\n",
       " 'adept': 193,\n",
       " 'adequacy': 194,\n",
       " 'adequate': 195,\n",
       " 'adequately': 196,\n",
       " 'ader': 197,\n",
       " 'adhere': 198,\n",
       " 'adherence': 199,\n",
       " 'aditya': 200,\n",
       " 'adjacent': 201,\n",
       " 'adjoining': 202,\n",
       " 'adjourn': 203,\n",
       " 'adjunct': 204,\n",
       " 'adjust': 205,\n",
       " 'adjustable': 206,\n",
       " 'adjusted': 207,\n",
       " 'adjusting': 208,\n",
       " 'adjustment': 209,\n",
       " 'adler': 210,\n",
       " 'admin': 211,\n",
       " 'administered': 212,\n",
       " 'administration': 213,\n",
       " 'administrative': 214,\n",
       " 'administrator': 215,\n",
       " 'admiration': 216,\n",
       " 'admired': 217,\n",
       " 'admirer': 218,\n",
       " 'admission': 219,\n",
       " 'admit': 220,\n",
       " 'admitted': 221,\n",
       " 'admittedly': 222,\n",
       " 'admitting': 223,\n",
       " 'admonished': 224,\n",
       " 'admonition': 225,\n",
       " 'adobe': 226,\n",
       " 'adopt': 227,\n",
       " 'adopted': 228,\n",
       " 'adoption': 229,\n",
       " 'adore': 230,\n",
       " 'adr': 231,\n",
       " 'adrian': 232,\n",
       " 'adtran': 233,\n",
       " 'adult': 234,\n",
       " 'advance': 235,\n",
       " 'advanced': 236,\n",
       " 'advancement': 237,\n",
       " 'advancer': 238,\n",
       " 'advances': 239,\n",
       " 'advancing': 240,\n",
       " 'advantage': 241,\n",
       " 'advantageous': 242,\n",
       " 'advantages': 243,\n",
       " 'advent': 244,\n",
       " 'adventure': 245,\n",
       " 'adventures': 246,\n",
       " 'adverse': 247,\n",
       " 'adversely': 248,\n",
       " 'adversity': 249,\n",
       " 'advertise': 250,\n",
       " 'advertised': 251,\n",
       " 'advertisement': 252,\n",
       " 'advertiser': 253,\n",
       " 'advertises': 254,\n",
       " 'advertising': 255,\n",
       " 'advice': 256,\n",
       " 'advice_____________________': 257,\n",
       " 'advise': 258,\n",
       " 'advised': 259,\n",
       " 'adviser': 260,\n",
       " 'advises': 261,\n",
       " 'advising': 262,\n",
       " 'advisor': 263,\n",
       " 'advisory': 264,\n",
       " 'advocate': 265,\n",
       " 'advocates': 266,\n",
       " 'advocating': 267,\n",
       " 'aec': 268,\n",
       " 'aeco': 269,\n",
       " 'aegis': 270,\n",
       " 'aep': 271,\n",
       " 'aerial': 272,\n",
       " 'aeros': 273,\n",
       " 'aerospace': 274,\n",
       " 'aes': 275,\n",
       " 'aess': 276,\n",
       " 'af56661': 277,\n",
       " 'afar': 278,\n",
       " 'affair': 279,\n",
       " 'affect': 280,\n",
       " 'affected': 281,\n",
       " 'affecting': 282,\n",
       " 'affidavit': 283,\n",
       " 'affiliate': 284,\n",
       " 'affiliated': 285,\n",
       " 'affiliates': 286,\n",
       " 'affiliation': 287,\n",
       " 'affirm': 288,\n",
       " 'affirmation': 289,\n",
       " 'affirmative': 290,\n",
       " 'affirmed': 291,\n",
       " 'afflicted': 292,\n",
       " 'affluent': 293,\n",
       " 'afford': 294,\n",
       " 'affordable': 295,\n",
       " 'afghanistan': 296,\n",
       " 'afl': 297,\n",
       " 'afloat': 298,\n",
       " 'aforementioned': 299,\n",
       " 'afp': 300,\n",
       " 'afpextel': 301,\n",
       " 'afraid': 302,\n",
       " 'africa': 303,\n",
       " 'african': 304,\n",
       " 'afterhour': 305,\n",
       " 'aftermath': 306,\n",
       " 'afternoon': 307,\n",
       " 'afterschool': 308,\n",
       " 'aftertax': 309,\n",
       " 'afx': 310,\n",
       " 'aga': 311,\n",
       " 'against': 312,\n",
       " 'agame': 313,\n",
       " 'agas': 314,\n",
       " 'age': 315,\n",
       " 'aged': 316,\n",
       " 'agencies': 317,\n",
       " 'agency': 318,\n",
       " 'agenda': 319,\n",
       " 'agendas': 320,\n",
       " 'agent': 321,\n",
       " 'ages': 322,\n",
       " 'agg': 323,\n",
       " 'aggie': 324,\n",
       " 'aggies': 325,\n",
       " 'aggravate': 326,\n",
       " 'aggravation': 327,\n",
       " 'aggregate': 328,\n",
       " 'aggregated': 329,\n",
       " 'aggregation': 330,\n",
       " 'aggressive': 331,\n",
       " 'aggressively': 332,\n",
       " 'agile': 333,\n",
       " 'agilent': 334,\n",
       " 'agility': 335,\n",
       " 'aging': 336,\n",
       " 'ago': 337,\n",
       " 'agra': 338,\n",
       " 'agree': 339,\n",
       " 'agreeable': 340,\n",
       " 'agreed': 341,\n",
       " 'agreeing': 342,\n",
       " 'agreement': 343,\n",
       " 'agrees': 344,\n",
       " 'agressive': 345,\n",
       " 'agricultural': 346,\n",
       " 'agriculture': 347,\n",
       " 'agt': 348,\n",
       " 'agua': 349,\n",
       " 'ahanchian': 350,\n",
       " 'ahead': 351,\n",
       " 'ahman': 352,\n",
       " 'ahold': 353,\n",
       " 'ahover': 354,\n",
       " 'aid': 355,\n",
       " 'aide': 356,\n",
       " 'aided': 357,\n",
       " 'aides': 358,\n",
       " 'aig': 359,\n",
       " 'aikman': 360,\n",
       " 'ailing': 361,\n",
       " 'ailment': 362,\n",
       " 'aim': 363,\n",
       " 'aimed': 364,\n",
       " 'aiming': 365,\n",
       " 'ain': 366,\n",
       " 'aint': 367,\n",
       " 'air': 368,\n",
       " 'airborne': 369,\n",
       " 'aircard': 370,\n",
       " 'airconditioning': 371,\n",
       " 'aircraft': 372,\n",
       " 'aired': 373,\n",
       " 'airfare': 374,\n",
       " 'airfares': 375,\n",
       " 'airline': 376,\n",
       " 'airlines': 377,\n",
       " 'airplane': 378,\n",
       " 'airplanes': 379,\n",
       " 'airport': 380,\n",
       " 'airportassessed': 381,\n",
       " 'airquality': 382,\n",
       " 'airtime': 383,\n",
       " 'airway': 384,\n",
       " 'aisle': 385,\n",
       " 'ajay': 386,\n",
       " 'aka': 387,\n",
       " 'akamai': 388,\n",
       " 'aker': 389,\n",
       " 'akili': 390,\n",
       " 'akin': 391,\n",
       " 'alabama': 392,\n",
       " 'aladdin': 393,\n",
       " 'alamo': 394,\n",
       " 'alamos': 395,\n",
       " 'alan': 396,\n",
       " 'alarm': 397,\n",
       " 'alas': 398,\n",
       " 'alaska': 399,\n",
       " 'alaskan': 400,\n",
       " 'alaywan': 401,\n",
       " 'albany': 402,\n",
       " 'albeit': 403,\n",
       " 'albert': 404,\n",
       " 'alberta': 405,\n",
       " 'alberto': 406,\n",
       " 'albright': 407,\n",
       " 'album': 408,\n",
       " 'albuquerque': 409,\n",
       " 'alcatel': 410,\n",
       " 'alcoa': 411,\n",
       " 'alcohol': 412,\n",
       " 'alden': 413,\n",
       " 'alder': 414,\n",
       " 'aldine': 415,\n",
       " 'aldrich': 416,\n",
       " 'alejandro': 417,\n",
       " 'alert': 418,\n",
       " 'alerted': 419,\n",
       " 'alerting': 420,\n",
       " 'alex': 421,\n",
       " 'alexander': 422,\n",
       " 'alexandra': 423,\n",
       " 'alexandria': 424,\n",
       " 'alexis': 425,\n",
       " 'alfalfa': 426,\n",
       " 'alfred': 427,\n",
       " 'alfredo': 428,\n",
       " 'algebraic': 429,\n",
       " 'alhambra': 430,\n",
       " 'ali': 431,\n",
       " 'alice': 432,\n",
       " 'align': 433,\n",
       " 'aligncenter': 434,\n",
       " 'aligned': 435,\n",
       " 'aligning': 436,\n",
       " 'alignleft': 437,\n",
       " 'alignment': 438,\n",
       " 'alike': 439,\n",
       " 'alink': 440,\n",
       " 'alison': 441,\n",
       " 'alive': 442,\n",
       " 'allamerica': 443,\n",
       " 'allamerican': 444,\n",
       " 'allan': 445,\n",
       " 'allay': 446,\n",
       " 'allegation': 447,\n",
       " 'allege': 448,\n",
       " 'alleged': 449,\n",
       " 'allegedly': 450,\n",
       " 'alleges': 451,\n",
       " 'allegheny': 452,\n",
       " 'allegiance': 453,\n",
       " 'alleging': 454,\n",
       " 'allen': 455,\n",
       " 'allenhouect': 456,\n",
       " 'alleviate': 457,\n",
       " 'alley': 458,\n",
       " 'alliance': 459,\n",
       " 'alliances': 460,\n",
       " 'allies': 461,\n",
       " 'allin': 462,\n",
       " 'allinclusive': 463,\n",
       " 'allinone': 464,\n",
       " 'allison': 465,\n",
       " 'allnew': 466,\n",
       " 'allocate': 467,\n",
       " 'allocated': 468,\n",
       " 'allocates': 469,\n",
       " 'allocation': 470,\n",
       " 'allot': 471,\n",
       " 'allotment': 472,\n",
       " 'allotted': 473,\n",
       " 'allowable': 474,\n",
       " 'allowance': 475,\n",
       " 'allowances': 476,\n",
       " 'allowed': 477,\n",
       " 'allowing': 478,\n",
       " 'alloy': 479,\n",
       " 'allpurpose': 480,\n",
       " 'allred': 481,\n",
       " 'allstar': 482,\n",
       " 'allstock': 483,\n",
       " 'alltime': 484,\n",
       " 'alluded': 485,\n",
       " 'alluring': 486,\n",
       " 'alma': 487,\n",
       " 'almeida': 488,\n",
       " 'almond': 489,\n",
       " 'aloha': 490,\n",
       " 'alongside': 491,\n",
       " 'alot': 492,\n",
       " 'aloud': 493,\n",
       " 'alpert': 494,\n",
       " 'alphanumeric': 495,\n",
       " 'alpharetta': 496,\n",
       " 'alqaeda': 497,\n",
       " 'alright': 498,\n",
       " 'alstott': 499,\n",
       " 'alt': 500,\n",
       " 'altar': 501,\n",
       " 'altec': 502,\n",
       " 'alter': 503,\n",
       " 'alteration': 504,\n",
       " 'altered': 505,\n",
       " 'alternate': 506,\n",
       " 'alternates': 507,\n",
       " 'alternating': 508,\n",
       " 'alternative': 509,\n",
       " 'alternativeenergy': 510,\n",
       " 'alternatively': 511,\n",
       " 'alternatives': 512,\n",
       " 'alto': 513,\n",
       " 'altogether': 514,\n",
       " 'altra': 515,\n",
       " 'alum': 516,\n",
       " 'aluminum': 517,\n",
       " 'alumni': 518,\n",
       " 'alvarez': 519,\n",
       " 'alvin': 520,\n",
       " 'alyson': 521,\n",
       " 'alzheimer': 522,\n",
       " 'amabile': 523,\n",
       " 'amalgamated': 524,\n",
       " 'amanda': 525,\n",
       " 'amani': 526,\n",
       " 'amar': 527,\n",
       " 'amarillo': 528,\n",
       " 'amassed': 529,\n",
       " 'amateur': 530,\n",
       " 'amaze': 531,\n",
       " 'amazed': 532,\n",
       " 'amazing': 533,\n",
       " 'amazingly': 534,\n",
       " 'amazon': 535,\n",
       " 'amazoncom': 536,\n",
       " 'ambassador': 537,\n",
       " 'ambiguity': 538,\n",
       " 'ambition': 539,\n",
       " 'ambitious': 540,\n",
       " 'ambler': 541,\n",
       " 'ambulance': 542,\n",
       " 'amc': 543,\n",
       " 'amen': 544,\n",
       " 'amend': 545,\n",
       " 'amended': 546,\n",
       " 'amending': 547,\n",
       " 'amendment': 548,\n",
       " 'amenities': 549,\n",
       " 'amerada': 550,\n",
       " 'ameren': 551,\n",
       " 'amerex': 552,\n",
       " 'america': 553,\n",
       " 'america_corp': 554,\n",
       " 'american': 555,\n",
       " 'americas': 556,\n",
       " 'ameritrade': 557,\n",
       " 'amex': 558,\n",
       " 'amgen': 559,\n",
       " 'ami': 560,\n",
       " 'amid': 561,\n",
       " 'amidst': 562,\n",
       " 'amie': 563,\n",
       " 'amir': 564,\n",
       " 'ammunition': 565,\n",
       " 'amnesty': 566,\n",
       " 'amoco': 567,\n",
       " 'amortization': 568,\n",
       " 'amortize': 569,\n",
       " 'amortized': 570,\n",
       " 'amos': 571,\n",
       " 'amount': 572,\n",
       " 'amounted': 573,\n",
       " 'amp': 574,\n",
       " 'ampersand': 575,\n",
       " 'ample': 576,\n",
       " 'amplified': 577,\n",
       " 'amplifier': 578,\n",
       " 'amr': 579,\n",
       " 'amro': 580,\n",
       " 'amsterdam': 581,\n",
       " 'amt': 582,\n",
       " 'amusing': 583,\n",
       " 'amy': 584,\n",
       " 'anadarko': 585,\n",
       " 'anaheim': 586,\n",
       " 'analog': 587,\n",
       " 'analogous': 588,\n",
       " 'analogy': 589,\n",
       " 'analyses': 590,\n",
       " 'analysis': 591,\n",
       " 'analysis_____________________': 592,\n",
       " 'analyst': 593,\n",
       " 'analystassociate': 594,\n",
       " 'analytic': 595,\n",
       " 'analytical': 596,\n",
       " 'analyze': 597,\n",
       " 'analyzed': 598,\n",
       " 'analyzer': 599,\n",
       " 'analyzes': 600,\n",
       " 'analyzing': 601,\n",
       " 'anatol': 602,\n",
       " 'anchor': 603,\n",
       " 'anchordesk': 604,\n",
       " 'anchored': 605,\n",
       " 'ancient': 606,\n",
       " 'ancillary': 607,\n",
       " 'ander': 608,\n",
       " 'andersen': 609,\n",
       " 'anderson': 610,\n",
       " 'andor': 611,\n",
       " 'andover': 612,\n",
       " 'andre': 613,\n",
       " 'andrea': 614,\n",
       " 'andrew': 615,\n",
       " 'andy': 616,\n",
       " 'anecdotal': 617,\n",
       " 'anecdotes': 618,\n",
       " 'anemia': 619,\n",
       " 'anemic': 620,\n",
       " 'anew': 621,\n",
       " 'angel': 622,\n",
       " 'angela': 623,\n",
       " 'angelides': 624,\n",
       " 'angelo': 625,\n",
       " 'angelos': 626,\n",
       " 'anger': 627,\n",
       " 'angered': 628,\n",
       " 'angie': 629,\n",
       " 'angle': 630,\n",
       " 'angles': 631,\n",
       " 'angry': 632,\n",
       " 'angst': 633,\n",
       " 'animal': 634,\n",
       " 'animated': 635,\n",
       " 'animation': 636,\n",
       " 'animosity': 637,\n",
       " 'ankle': 638,\n",
       " 'ankles': 639,\n",
       " 'ann': 640,\n",
       " 'anna': 641,\n",
       " 'annapolis': 642,\n",
       " 'anne': 643,\n",
       " 'annex': 644,\n",
       " 'annexes': 645,\n",
       " 'annie': 646,\n",
       " 'anniversaries': 647,\n",
       " 'anniversary': 648,\n",
       " 'announce': 649,\n",
       " 'announced': 650,\n",
       " 'announcement': 651,\n",
       " 'announcer': 652,\n",
       " 'announces': 653,\n",
       " 'announcing': 654,\n",
       " 'annoyance': 655,\n",
       " 'annoying': 656,\n",
       " 'annual': 657,\n",
       " 'annualized': 658,\n",
       " 'annually': 659,\n",
       " 'annuities': 660,\n",
       " 'annuity': 661,\n",
       " 'anointed': 662,\n",
       " 'anomaly': 663,\n",
       " 'anonymity': 664,\n",
       " 'anonymous': 665,\n",
       " 'anp': 666,\n",
       " 'answer': 667,\n",
       " 'answerable': 668,\n",
       " 'answered': 669,\n",
       " 'answering': 670,\n",
       " 'ant': 671,\n",
       " 'ante': 672,\n",
       " 'anthem': 673,\n",
       " 'anthony': 674,\n",
       " 'anthrax': 675,\n",
       " 'anti': 676,\n",
       " 'anticipate': 677,\n",
       " 'anticipated': 678,\n",
       " 'anticipates': 679,\n",
       " 'anticipating': 680,\n",
       " 'anticipation': 681,\n",
       " 'antigua': 682,\n",
       " 'antioch': 683,\n",
       " 'antique': 684,\n",
       " 'antiques': 685,\n",
       " 'antiterror': 686,\n",
       " 'antiterrorism': 687,\n",
       " 'antiterrorist': 688,\n",
       " 'antitrust': 689,\n",
       " 'antivirus': 690,\n",
       " 'anton': 691,\n",
       " 'antonio': 692,\n",
       " 'antwan': 693,\n",
       " 'anxiety': 694,\n",
       " 'anxious': 695,\n",
       " 'anymore': 696,\n",
       " 'anyones': 697,\n",
       " 'anytime': 698,\n",
       " 'anyway': 699,\n",
       " 'anz': 700,\n",
       " 'aol': 701,\n",
       " 'aon': 702,\n",
       " 'apa': 703,\n",
       " 'apache': 704,\n",
       " 'apartment': 705,\n",
       " 'apb': 706,\n",
       " 'apex': 707,\n",
       " 'api': 708,\n",
       " 'apiece': 709,\n",
       " 'apollo': 710,\n",
       " 'apologies': 711,\n",
       " 'apologist': 712,\n",
       " 'apologize': 713,\n",
       " 'apologized': 714,\n",
       " 'apology': 715,\n",
       " 'app': 716,\n",
       " 'appalachian': 717,\n",
       " 'apparel': 718,\n",
       " 'apparent': 719,\n",
       " 'apparently': 720,\n",
       " 'appeal': 721,\n",
       " 'appealed': 722,\n",
       " 'appealing': 723,\n",
       " 'appearance': 724,\n",
       " 'appearances': 725,\n",
       " 'appeared': 726,\n",
       " 'appearing': 727,\n",
       " 'appease': 728,\n",
       " 'appellate': 729,\n",
       " 'appendices': 730,\n",
       " 'appendix': 731,\n",
       " 'appetite': 732,\n",
       " 'appetizer': 733,\n",
       " 'applaud': 734,\n",
       " 'applauded': 735,\n",
       " 'applause': 736,\n",
       " 'apple': 737,\n",
       " 'applebaum': 738,\n",
       " 'apples': 739,\n",
       " 'appleton': 740,\n",
       " 'applewhite': 741,\n",
       " 'applewhites': 742,\n",
       " 'appliance': 743,\n",
       " 'appliances': 744,\n",
       " 'applicability': 745,\n",
       " 'applicable': 746,\n",
       " 'applicant': 747,\n",
       " 'application': 748,\n",
       " 'applied': 749,\n",
       " 'applies': 750,\n",
       " 'apply': 751,\n",
       " 'applying': 752,\n",
       " 'appoint': 753,\n",
       " 'appointed': 754,\n",
       " 'appointee': 755,\n",
       " 'appointment': 756,\n",
       " 'appraisal': 757,\n",
       " 'appraised': 758,\n",
       " 'appreciate': 759,\n",
       " 'appreciated': 760,\n",
       " 'appreciates': 761,\n",
       " 'appreciation': 762,\n",
       " 'approach': 763,\n",
       " 'approached': 764,\n",
       " 'approaches': 765,\n",
       " 'approaching': 766,\n",
       " 'appropriately': 767,\n",
       " 'appropriation': 768,\n",
       " 'approval': 769,\n",
       " 'approve': 770,\n",
       " 'approved': 771,\n",
       " 'approver': 772,\n",
       " 'approves': 773,\n",
       " 'approving': 774,\n",
       " 'approx': 775,\n",
       " 'approximate': 776,\n",
       " 'approximately': 777,\n",
       " 'appt': 778,\n",
       " 'apr': 779,\n",
       " 'apr01': 780,\n",
       " 'april': 781,\n",
       " 'apses': 782,\n",
       " 'apt': 783,\n",
       " 'apx': 784,\n",
       " 'aquila': 785,\n",
       " 'arab': 786,\n",
       " 'aramco': 787,\n",
       " 'arb': 788,\n",
       " 'arbiter': 789,\n",
       " 'arbitrage': 790,\n",
       " 'arbitrageur': 791,\n",
       " 'arbitrarily': 792,\n",
       " 'arbitration': 793,\n",
       " 'arbitrator': 794,\n",
       " 'arbor': 795,\n",
       " 'arc': 796,\n",
       " 'arcade': 797,\n",
       " 'archer': 798,\n",
       " 'architect': 799,\n",
       " 'architectural': 800,\n",
       " 'architecture': 801,\n",
       " 'archival': 802,\n",
       " 'archive': 803,\n",
       " 'archived': 804,\n",
       " 'archives': 805,\n",
       " 'area': 806,\n",
       " 'areas': 807,\n",
       " 'arena': 808,\n",
       " 'arent': 809,\n",
       " 'argentina': 810,\n",
       " 'argentine': 811,\n",
       " 'arguably': 812,\n",
       " 'argue': 813,\n",
       " 'argued': 814,\n",
       " 'argues': 815,\n",
       " 'arguing': 816,\n",
       " 'argument': 817,\n",
       " 'argus': 818,\n",
       " 'ari': 819,\n",
       " 'aria': 820,\n",
       " 'arial': 821,\n",
       " 'arise': 822,\n",
       " 'arisen': 823,\n",
       " 'arises': 824,\n",
       " 'arising': 825,\n",
       " 'ariz': 826,\n",
       " 'arizona': 827,\n",
       " 'arizonas': 828,\n",
       " 'ark': 829,\n",
       " 'arkansas': 830,\n",
       " 'arlington': 831,\n",
       " 'arm': 832,\n",
       " 'armando': 833,\n",
       " 'armed': 834,\n",
       " 'armen': 835,\n",
       " 'armies': 836,\n",
       " 'armstrong': 837,\n",
       " 'army': 838,\n",
       " 'arnold': 839,\n",
       " 'arnoldougcooenron': 840,\n",
       " 'aromas': 841,\n",
       " 'aromatic': 842,\n",
       " 'aron': 843,\n",
       " 'arora': 844,\n",
       " 'arose': 845,\n",
       " 'aroundtheclock': 846,\n",
       " 'aroused': 847,\n",
       " 'arpt': 848,\n",
       " 'arrange': 849,\n",
       " 'arranged': 850,\n",
       " 'arrangement': 851,\n",
       " 'arranging': 852,\n",
       " 'arrangment': 853,\n",
       " 'array': 854,\n",
       " 'arredondo': 855,\n",
       " 'arrest': 856,\n",
       " 'arrested': 857,\n",
       " 'arris': 858,\n",
       " 'arrival': 859,\n",
       " 'arrive': 860,\n",
       " 'arrived': 861,\n",
       " 'arrives': 862,\n",
       " 'arriving': 863,\n",
       " 'arrogance': 864,\n",
       " 'arrogant': 865,\n",
       " 'arrow': 866,\n",
       " 'arroyo': 867,\n",
       " 'arsenal': 868,\n",
       " 'arslanian': 869,\n",
       " 'art': 870,\n",
       " 'arter': 871,\n",
       " 'artesa': 872,\n",
       " 'arthritis': 873,\n",
       " 'arthroscopic': 874,\n",
       " 'arthur': 875,\n",
       " 'article': 876,\n",
       " 'articles': 877,\n",
       " 'articulate': 878,\n",
       " 'articulated': 879,\n",
       " 'artifact': 880,\n",
       " 'artificial': 881,\n",
       " 'artificially': 882,\n",
       " 'artist': 883,\n",
       " 'artwork': 884,\n",
       " 'as400': 885,\n",
       " 'asa': 886,\n",
       " 'asap': 887,\n",
       " 'asc': 888,\n",
       " 'ascend': 889,\n",
       " 'ascension': 890,\n",
       " 'ascent': 891,\n",
       " 'ascertain': 892,\n",
       " 'aschehoug': 893,\n",
       " 'asem': 894,\n",
       " 'ash': 895,\n",
       " 'ashamed': 896,\n",
       " 'ashcroft': 897,\n",
       " 'ashford': 898,\n",
       " 'ashland': 899,\n",
       " 'ashley': 900,\n",
       " 'ashore': 901,\n",
       " 'ashton': 902,\n",
       " 'asia': 903,\n",
       " 'asian': 904,\n",
       " 'asiapacific': 905,\n",
       " 'asias': 906,\n",
       " 'ask': 907,\n",
       " 'asked': 908,\n",
       " 'asking': 909,\n",
       " 'asleep': 910,\n",
       " 'asp': 911,\n",
       " 'aspect': 912,\n",
       " 'asphalt': 913,\n",
       " 'aspiration': 914,\n",
       " 'ass': 915,\n",
       " 'assailed': 916,\n",
       " 'assault': 917,\n",
       " 'assemble': 918,\n",
       " 'assembled': 919,\n",
       " 'assembly': 920,\n",
       " 'assemblywoman': 921,\n",
       " 'assert': 922,\n",
       " 'asserted': 923,\n",
       " 'asserting': 924,\n",
       " 'assertion': 925,\n",
       " 'assertive': 926,\n",
       " 'asses': 927,\n",
       " 'assess': 928,\n",
       " 'assessed': 929,\n",
       " 'assesses': 930,\n",
       " 'assessing': 931,\n",
       " 'assessment': 932,\n",
       " 'asset': 933,\n",
       " 'assetbased': 934,\n",
       " 'asshole': 935,\n",
       " 'assign': 936,\n",
       " 'assigned': 937,\n",
       " 'assignment': 938,\n",
       " 'assist': 939,\n",
       " 'assistance': 940,\n",
       " 'assistant': 941,\n",
       " 'assisted': 942,\n",
       " 'assisting': 943,\n",
       " 'assoc': 944,\n",
       " 'associate': 945,\n",
       " 'associates': 946,\n",
       " 'association': 947,\n",
       " 'assorted': 948,\n",
       " 'asst': 949,\n",
       " 'assuage': 950,\n",
       " 'assume': 951,\n",
       " 'assumed': 952,\n",
       " 'assumes': 953,\n",
       " 'assuming': 954,\n",
       " 'assumption': 955,\n",
       " 'assurance': 956,\n",
       " 'assurances': 957,\n",
       " 'assure': 958,\n",
       " 'assured': 959,\n",
       " 'assuring': 960,\n",
       " 'astoria': 961,\n",
       " 'astounding': 962,\n",
       " 'astray': 963,\n",
       " 'astrodome': 964,\n",
       " 'astronomical': 965,\n",
       " 'astros': 966,\n",
       " 'asylum': 967,\n",
       " 'ata': 968,\n",
       " 'atc': 969,\n",
       " 'ate': 970,\n",
       " 'athe': 971,\n",
       " 'atheist': 972,\n",
       " 'athen': 973,\n",
       " 'athlete': 974,\n",
       " 'athletes': 975,\n",
       " 'athletic': 976,\n",
       " 'athleticism': 977,\n",
       " 'atkin': 978,\n",
       " 'atl': 979,\n",
       " 'atlanta': 980,\n",
       " 'atlantabased': 981,\n",
       " 'atlantas': 982,\n",
       " 'atlantic': 983,\n",
       " 'atlantis': 984,\n",
       " 'atlas': 985,\n",
       " 'atm': 986,\n",
       " 'atmosphere': 987,\n",
       " 'atmospheric': 988,\n",
       " 'atom': 989,\n",
       " 'atop': 990,\n",
       " 'att': 991,\n",
       " 'att1htm': 992,\n",
       " 'atta': 993,\n",
       " 'attach': 994,\n",
       " 'attached': 995,\n",
       " 'attaching': 996,\n",
       " 'attack': 997,\n",
       " 'attacked': 998,\n",
       " 'attacking': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1152 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For each of the topics synthesized, get the word distribution\n",
    "plt.figure(figsize=(20,16))\n",
    "for ind in range(num_topics):\n",
    "\n",
    "    if counter >= limit:\n",
    "        break\n",
    "\n",
    "    title_str = 'Topic{}'.format(ind)\n",
    "\n",
    "    #Use softmax function to assign probability for each of the word associated with the topic\n",
    "    # The sum of all the probabilities of words associated with each topic should add up to 1\n",
    "    pvals = mx.nd.softmax(mx.nd.array(W[:, ind])).asnumpy()\n",
    "    #print(\"Printing pvals: \", len(pvals))\n",
    "    \n",
    "    word_freq = dict()\n",
    "    for k in word_to_id.keys():\n",
    "        i = word_to_id[k]\n",
    "        word_freq[k] =pvals[i]\n",
    "\n",
    "    wordcloud = wc.WordCloud(background_color='white').fit_words(word_freq)\n",
    "\n",
    "    plt.subplot(limit // n_col, n_col, counter+1)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title_str)\n",
    "    #plt.close()\n",
    "\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
