{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Inference Pipeline in SageMaker to conduct feature processing and train NTM algorithm\n",
    "Through this notebook, we will demonstrate how SageMaker platform can be used to automate feature processing through Glue, model training, deployment, and inference. Being able to automate these key stages in machine learning life cycle, will enable data scientists and machine learning engineers to relatively quickly create production-ready solutions for business problems.\n",
    "\n",
    "#### We will follow the below process to illustrate key ideas\n",
    "-  Create processed dataset using Amazon Glue ETL service to run SparkML jobs\n",
    "-  Identify topics in the processed dataset via training NTM algorithm\n",
    "-  Create inference pipeline consisting of SparkML and NTM models for real time predictions\n",
    "-  Create inference pipeline consisting of SparkML and NTM models for batch predictions\n",
    "\n",
    "\n",
    "#### About the Dataset\n",
    "To illustrate the concepts, we will use [ABC Millions](https://www.kaggle.com/therohk/million-headlines) Headlines dataset. The dataset contains approximately a million news headlines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features by using AWS Glue to run SparkML jobs\n",
    "\n",
    "AWS Glue is a serverless ETL service, which can execute PySpark/Spark jobs. We will run SparkML jobs using AWS Glue. We will need to assign the current notebook a role, so it can access the Glue service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the current notebook to access AWS Glue service\n",
    "\n",
    "We will first retrieve the current execution role of the notebook. We will then navigate to [IAM Dashboard](http://console.aws.amazon.com/iam/home) to edit the Role to include AWS Glue specific permission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::109099157774:role/service-role/AmazonSageMaker-ExecutionRole-20180923T113724\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Glue as an trusted entity to this role\n",
    "\n",
    "If you have created a role to call Glue service, then you can simply pass the role while invoking Glue APIs. Otherwise, you can pass the execution role of the current sagemaker session when invoking Glue APIs.  \n",
    "\n",
    "On the IAM Dashboard, click on __Roles__ on the left-side nav and search for this Role. Click on the target Role to navigate to **Summary** page. Click on **Trust Relationships** tab to add AWS Glue as an additional trusted entity.\n",
    "\n",
    "Click on **Edit trust relationship** to add the following entry to \"Service\" key:\n",
    "\n",
    "\"glue.amazonaws.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the feature processing script using SparkML\n",
    "\n",
    "We are assuming that the source data is already uploaded to your S3 bucket. We will upload the feature processing script (abcheadlines_processing.py) to s3, so that Glue can run the script as a Pyspark job. \n",
    "\n",
    "\n",
    "The feature processing script conducts the following main functions:\n",
    "- Filter the dataset to include only ~100k abc news headlines\n",
    "- Use SparkML feature transformers to tokenize the headlines, remove stop words, get word & document frequency\n",
    "- The processed data to saved to the designated S3 bucket\n",
    "- The SparkML PipelineModel is serialized using MLeap\n",
    "\n",
    "[MLeap](http://mleap-docs.combust.ml/) is a serialization format and execution engine for machine learning pipelines. It serializes the pipeline to an MLeap bundle, which enables data scientists to take models to wherever they go. It supports Spark, scikit-learn and tensorflow for training pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_bucket = 'ai-in-aws'\n",
    "script_location = sess.upload_data(path='abcheadlines_processing.py', bucket=default_bucket, key_prefix='sagemaker/inference-pipeline/codes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload MLeap Dependencies to S3\n",
    "MLeap related software packages need to be made available to Glue job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-02-22 19:47:24--  https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/python/python.zip\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.200.96\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.200.96|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 36872 (36K) [application/zip]\n",
      "Saving to: ‘python.zip.3’\n",
      "\n",
      "python.zip.3        100%[===================>]  36.01K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2019-02-22 19:47:24 (466 KB/s) - ‘python.zip.3’ saved [36872/36872]\n",
      "\n",
      "--2019-02-22 19:47:24--  https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/jar/mleap_spark_assembly.jar\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.200.96\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.200.96|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 17319576 (17M) [application/java-archive]\n",
      "Saving to: ‘mleap_spark_assembly.jar.3’\n",
      "\n",
      "mleap_spark_assembl 100%[===================>]  16.52M  8.46MB/s    in 2.0s    \n",
      "\n",
      "2019-02-22 19:47:27 (8.46 MB/s) - ‘mleap_spark_assembly.jar.3’ saved [17319576/17319576]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/python/python.zip\n",
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/jar/mleap_spark_assembly.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_dep_location = sess.upload_data(path='python.zip', bucket=default_bucket, key_prefix='sagemaker/inference-pipeline/dependencies/python')\n",
    "jar_dep_location = sess.upload_data(path='mleap_spark_assembly.jar', bucket=default_bucket, key_prefix='sagemaker/inference-pipeline/dependencies/jar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Designate output location for processed data and SparkML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "# Input location of the data, We uploaded our train.csv file to input key previously\n",
    "s3_input_bucket = default_bucket\n",
    "s3_input_key_prefix = 'sagemaker/inference-pipeline/input'\n",
    "s3_input_fn = 'abcnews-date-text.csv'\n",
    "\n",
    "\n",
    "# Output location of the data. The input data will be split, transformed, and \n",
    "# uploaded to output/train and output/validation\n",
    "s3_output_bucket = default_bucket\n",
    "s3_output_key_prefix = 'sagemaker/inference-pipeline/output/' + timestamp_prefix \n",
    "\n",
    "# the MLeap serialized SparkML model will be uploaded to output/mleap\n",
    "s3_model_bucket = default_bucket\n",
    "s3_model_key_prefix = s3_output_key_prefix + '/mleap'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke Glue API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkml-abcnews-2019-02-22-19-53-09\n"
     ]
    }
   ],
   "source": [
    "boto_session = sess.boto_session\n",
    "s3 = boto_session.resource('s3')\n",
    "\n",
    "glue_client = boto_session.client('glue')\n",
    "job_name = 'sparkml-abcnews-' + timestamp_prefix\n",
    "\n",
    "response = glue_client.create_job(\n",
    "    Name=job_name,\n",
    "    Description='PySpark job to featurize the Enron Emails dataset',\n",
    "    Role=role, # you can pass your existing AWS Glue role here if you have used Glue before\n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 1\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': script_location\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python',\n",
    "        '--extra-jars' : jar_dep_location,\n",
    "        '--extra-py-files': python_dep_location\n",
    "    },\n",
    "    AllocatedCapacity=10,\n",
    "    Timeout=60,\n",
    ")\n",
    "glue_job_name = response['Name']\n",
    "\n",
    "print(glue_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jr_53f5d67be969ae0409654c35a8a6b78cc592a19de2939f8fb47e561217a40277\n"
     ]
    }
   ],
   "source": [
    "job_run_id = glue_client.start_job_run(JobName=job_name,\n",
    "                                       Arguments = {\n",
    "                                        '--S3_INPUT_BUCKET': s3_input_bucket,\n",
    "                                        '--S3_INPUT_KEY_PREFIX': s3_input_key_prefix,\n",
    "                                        '--S3_INPUT_FILENAME': s3_input_fn,  \n",
    "                                        '--S3_OUTPUT_BUCKET': s3_output_bucket,\n",
    "                                        '--S3_OUTPUT_KEY_PREFIX': s3_output_key_prefix,\n",
    "                                        '--S3_MODEL_BUCKET': s3_model_bucket,\n",
    "                                        '--S3_MODEL_KEY_PREFIX': s3_model_key_prefix\n",
    "                                       })['JobRunId']\n",
    "print(job_run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Glue Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "job_run_status = glue_client.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n",
    "while job_run_status not in ('FAILED', 'SUCCEEDED', 'STOPPED'):\n",
    "    job_run_status = glue_client.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n",
    "    print (job_run_status)\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Abc News Headlines dataset (processed) from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Acknowledgements, Copyright Information, and Availability**\n",
    "# Source: https://www.kaggle.com/therohk/million-headlines\n",
    "# Source: SageMaker AWS Labs\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix\n",
    "import io\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.session import s3_input\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from sagemaker.sparkml.model import SparkMLModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to cut this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_bucket = 'ai-in-aws'\n",
    "s3_output_key_prefix = 'sagemaker/inference-pipeline/output/' + '2019-02-22-19-53-09' \n",
    "\n",
    "# the MLeap serialized SparkML model will be uploaded to output/mleap\n",
    "s3_model_bucket = default_bucket\n",
    "s3_model_key_prefix = s3_output_key_prefix + '/mleap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CSV file in the bucket\n",
    "def get_csv_files(client, bucket):\n",
    "    csv_files = []\n",
    "    content = client.list_objects(Bucket=bucket).get('Contents')\n",
    "    for obj in content:\n",
    "        key = obj.get('Key')\n",
    "        if '.csv' in key:\n",
    "            csv_files.append(key)\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the processed dataset\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(default_bucket)\n",
    "\n",
    "files = my_bucket.objects.filter(Prefix=s3_output_key_prefix)\n",
    "\n",
    "for f in files:\n",
    "    if '.csv' in f.key:\n",
    "        #print(f.key)\n",
    "        abcnews_df = pd.read_csv(os.path.join('s3://', s3_output_bucket, f.key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110365, 200)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abcnews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#convert dataframe (dense vector) to compressed sparse row matrix\n",
    "abcnews_csr = csr_matrix(abcnews_df, dtype=np.float32)\n",
    "print(abcnews_csr[:16].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110365"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abcnews_csr.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training, Validation and Test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(0.8 * abcnews_csr.shape[0])\n",
    "\n",
    "# split train and test\n",
    "train_vectors = abcnews_csr[:n_train, :] \n",
    "test_vectors = abcnews_csr[n_train:, :] \n",
    "\n",
    "# further split test set into validation set and test set\n",
    "n_test = test_vectors.shape[0]\n",
    "val_vectors = test_vectors[:n_test//2, :]\n",
    "test_vectors = test_vectors[n_test//2:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88292, 200) (11037, 200) (11036, 200)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors.shape, test_vectors.shape, val_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store CSR formatted headlines on S3\n",
    "\n",
    "CSR = Compressed Sparse Row \n",
    "\n",
    "The NTM algorithm, as well as other first-party SageMaker algorithms, accepts data in RecordIO Protobuf format. The SageMaker Python API provides helper functions for easily converting your data into this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Location s3://ai-in-aws/sagemaker/inference-pipeline/input/train\n",
      "Validation set Location s3://ai-in-aws/sagemaker/inference-pipeline/input/val\n",
      "Trained model will be saved at s3://ai-in-aws/sagemaker/inference-pipeline/output\n",
      "Auxiliary will be saved at s3://ai-in-aws/sagemaker/inference-pipeline/input/aux\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "train_prefix = os.path.join(s3_input_key_prefix,'train')\n",
    "val_prefix = os.path.join(s3_input_key_prefix, 'val')\n",
    "\n",
    "output_prefix = 'sagemaker/inference-pipeline/output'\n",
    "aux_prefix = os.path.join(s3_input_key_prefix, 'aux')\n",
    "\n",
    "s3_train_data = os.path.join('s3://', default_bucket, train_prefix)\n",
    "s3_val_data = os.path.join('s3://', default_bucket, val_prefix)\n",
    "s3_aux_data = os.path.join('s3://', default_bucket, aux_prefix)\n",
    "output_path = os.path.join('s3://', default_bucket, output_prefix)\n",
    "\n",
    "print('Training Location', s3_train_data)\n",
    "print('Validation set Location', s3_val_data)\n",
    "print('Trained model will be saved at', output_path)\n",
    "print('Auxiliary will be saved at', s3_aux_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the training data for parallel processing. write_spmatrix_to_sparse_tensor from SageMaker API is used to convert sparse matrix into RecordIO Protobuf format.\n",
    "\n",
    "[Protocol buffers format](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html): \n",
    "An array containing numeric values is treated as an instance containing single dense vector.\n",
    "dataElement = [1.5, 16.0, 14.0, 23.0]\n",
    "\n",
    "It will be converted to the following representation by the SDK.\n",
    "converted = {\n",
    "  \"features\": {\n",
    "    \"values\": dataElement\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_convert_upload(sparray, bucket, prefix, fname_template='data_part{}.pbr', n_parts=5):\n",
    "\n",
    "    chunk_size = sparray.shape[0] // n_parts\n",
    "    for i in range(n_parts):\n",
    "        # Calculate start and end indices\n",
    "        start = i*chunk_size\n",
    "        end = (i+1)*chunk_size\n",
    "        if i+1 == n_parts:\n",
    "            end = sparray.shape[0]\n",
    "\n",
    "        # Convert to record protobuf\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(array=sparray[start:end], file=buf, labels=None)\n",
    "        buf.seek(0)\n",
    "\n",
    "        # Upload to s3 location specified by bucket and prefix\n",
    "        fname = os.path.join(prefix, fname_template.format(i))\n",
    "        boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload training and validation vectors\n",
    "split_convert_upload(train_vectors, bucket=default_bucket, prefix=train_prefix, fname_template='data_part{}.pbr', n_parts=8)\n",
    "split_convert_upload(val_vectors, bucket=default_bucket, prefix=val_prefix, fname_template='val_part{}.pbr', n_parts=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the vocabulary file from s3 bucket\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "files = my_bucket.objects.filter(Prefix=s3_output_key_prefix)\n",
    "\n",
    "for f in files:\n",
    "    if '.txt' in f.key:\n",
    "        s3.Bucket(default_bucket).download_file(f.key, 'vocab.txt')\n",
    "\n",
    "        \n",
    "# s3.Bucket(default_bucket).download_file(os.path.join(s3_output_key_prefix, vocabFN), 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload vocabulary file to auxiliary path\n",
    "vocabFN_location = sess.upload_data(path='vocab.txt', bucket=default_bucket, key_prefix=aux_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "SageMaker uses Amazon Elastic Container Registry (ECR) docker container to host the NTM training image. The ECR containers are currently available for SageMaker NTM training in different regions\n",
    "\n",
    "Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, which allows Python developers to write software that makes use of services like Amazon S3 and Amazon EC2\n",
    "\n",
    "SageMaker SDK - A library for training and deploying machine learning models on Amazon SageMaker - aws/sagemaker-python-sdk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'ntm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "ntm = sagemaker.estimator.Estimator(container,\n",
    "                                   role,\n",
    "                                   train_instance_count=2,\n",
    "                                   train_instance_type='ml.c4.xlarge',\n",
    "                                   output_path=output_path,\n",
    "                                   sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Hyperparameters\n",
    "\n",
    "__feature_dim__ - it should be set to vocabulary size <br>\n",
    "__num_topics__ - topics to extract <br>\n",
    "__mini_batch_size__ - this is the batch_size for each worker instance. <br>\n",
    "__epochs__ - the maximal number of epochs to train for. <br>\n",
    "__num_patience_epochs__ and tolerance control the early shopping behavior. Improvements smaller than the tolerance are not considered as improvement <br>\n",
    "__optimizer and learning_rate__ - Adadelta optimizer and learning rate is not required <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://ai-in-aws/sagemaker/inference-pipeline/input/train'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "vocab_size = 200\n",
    "ntm.set_hyperparameters(num_topics=num_topics, feature_dim=vocab_size, mini_batch_size=30, epochs=150, num_patience_epochs=3, tolerance=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train = s3_input(s3_train_data, distribution='ShardedByS3Key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_aux = s3_input(s3_aux_data, distribution='FullyReplicated', content_type='text/plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: ntm-2019-02-17-23-21-55-061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-17 23:21:55 Starting - Starting the training job...\n",
      "2019-02-17 23:21:56 Starting - Launching requested ML instances......\n",
      "2019-02-17 23:23:02 Starting - Preparing the instances for training.........\n",
      "2019-02-17 23:24:44 Downloading - Downloading input data\n",
      "2019-02-17 23:24:44 Training - Training image download completed. Training in progress.\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:43 INFO 140262573676352] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:43 INFO 140262573676352] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'3', u'num_topics': u'5', u'epochs': u'150', u'feature_dim': u'200', u'mini_batch_size': u'30', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:43 INFO 140262573676352] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'200', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'3', u'epochs': u'150', u'mini_batch_size': u'30', u'num_topics': u'5', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:43 INFO 140262573676352] nvidia-smi took: 0.0251729488373 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:44 INFO 140262573676352] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:44 INFO 140262573676352] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/df3e0191-3ede-4fc2-87cf-2fbf5610d571', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-02-17-23-21-55-061', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/49d9b43e-e074-4488-9031-0c426adcac3d', 'PWD': '/', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:44 INFO 140262573676352] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/df3e0191-3ede-4fc2-87cf-2fbf5610d571', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-02-17-23-21-55-061', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/49d9b43e-e074-4488-9031-0c426adcac3d', 'DMLC_ROLE': 'scheduler', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:44 INFO 140262573676352] Launching parameter server for role server\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:44 INFO 140262573676352] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/df3e0191-3ede-4fc2-87cf-2fbf5610d571', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-02-17-23-21-55-061', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/49d9b43e-e074-4488-9031-0c426adcac3d', 'PWD': '/', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:44 INFO 140262573676352] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/df3e0191-3ede-4fc2-87cf-2fbf5610d571', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-02-17-23-21-55-061', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/49d9b43e-e074-4488-9031-0c426adcac3d', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:44 INFO 140262573676352] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/df3e0191-3ede-4fc2-87cf-2fbf5610d571', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '2', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-02-17-23-21-55-061', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/49d9b43e-e074-4488-9031-0c426adcac3d', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31mProcess 40 is a shell:scheduler.\u001b[0m\n",
      "\u001b[31mProcess 41 is a shell:server.\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:44 INFO 140262573676352] Using default worker.\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:24:44.579] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/train\", \"num_examples\": 30, \"features\": [{\"name\": \"values\", \"shape\": [200], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:24:44.585] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:24:44.593] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/test\", \"num_examples\": 30, \"features\": [{\"name\": \"values\", \"shape\": [200], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:44 INFO 140262573676352] Initializing\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:44 INFO 140262573676352] /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:44 INFO 140262573676352] vocab.txt\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:44 INFO 140262573676352] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:44 INFO 140262573676352] Loading pre-trained token embedding vectors from /opt/amazon/lib/python2.7/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n",
      "\u001b[32mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:46 INFO 140471953696576] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:46 INFO 140471953696576] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'3', u'num_topics': u'5', u'epochs': u'150', u'feature_dim': u'200', u'mini_batch_size': u'30', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:46 INFO 140471953696576] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'200', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'3', u'epochs': u'150', u'mini_batch_size': u'30', u'num_topics': u'5', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:46 INFO 140471953696576] nvidia-smi took: 0.0251729488373 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:46 INFO 140471953696576] Launching parameter server for role server\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:46 INFO 140471953696576] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/dc33f3e9-8e5a-46d2-aa2d-e18c2bcede38', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-02-17-23-21-55-061', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/e055286e-2b0d-4aa0-8423-a09e102f4769', 'PWD': '/', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:46 INFO 140471953696576] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/dc33f3e9-8e5a-46d2-aa2d-e18c2bcede38', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-02-17-23-21-55-061', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/e055286e-2b0d-4aa0-8423-a09e102f4769', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:46 INFO 140471953696576] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/dc33f3e9-8e5a-46d2-aa2d-e18c2bcede38', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '2', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'ntm-2019-02-17-23-21-55-061', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/e055286e-2b0d-4aa0-8423-a09e102f4769', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[32mProcess 40 is a shell:server.\u001b[0m\n",
      "\u001b[32mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:46 INFO 140471953696576] Using default worker.\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:24:46.126] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/train\", \"num_examples\": 30, \"features\": [{\"name\": \"values\", \"shape\": [200], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:24:46.132] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:24:46.133] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/test\", \"num_examples\": 30, \"features\": [{\"name\": \"values\", \"shape\": [200], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:46 INFO 140471953696576] Initializing\u001b[0m\n",
      "\u001b[32m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:46 INFO 140471953696576] /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:46 INFO 140471953696576] vocab.txt\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:46 INFO 140471953696576] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:46 INFO 140471953696576] Loading pre-trained token embedding vectors from /opt/amazon/lib/python2.7/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[02/17/2019 23:24:55 WARNING 140262573676352] 0 out of 200 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:55 INFO 140262573676352] Vocab embedding shape\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:55 INFO 140262573676352] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:55 INFO 140262573676352] Create Store: dist_async\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:57 WARNING 140471953696576] 0 out of 200 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:57 INFO 140471953696576] Vocab embedding shape\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:57 INFO 140471953696576] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:57 INFO 140471953696576] Create Store: dist_async\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1550445897.253148, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1550445897.253118}\n",
      "\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:24:57.253] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 11128, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:57 INFO 140471953696576] \u001b[0m\n",
      "\u001b[32m[02/17/2019 23:24:57 INFO 140471953696576] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1550445897.25334, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1550445897.253299}\n",
      "\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:24:57.253] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 12683, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:57 INFO 140262573676352] \u001b[0m\n",
      "\u001b[31m[02/17/2019 23:24:57 INFO 140262573676352] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:20 INFO 140471953696576] # Finished training epoch 1 on 44148 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:20 INFO 140471953696576] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:20 INFO 140471953696576] Loss (name: value) total: 3.87503636695\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:20 INFO 140471953696576] Loss (name: value) kld: 0.0653061621735\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:20 INFO 140471953696576] Loss (name: value) recons: 3.80973021017\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:20 INFO 140471953696576] Loss (name: value) logppx: 3.87503636695\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:20 INFO 140471953696576] #quality_metric: host=algo-2, epoch=1, train total_loss <loss>=3.87503636695\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:20 INFO 140471953696576] Timing: train: 23.05s, val: 0.00s, epoch: 23.05s\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:20 INFO 140471953696576] #progress_metric: host=algo-2, completed 0 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Total Records Seen\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1550445920.304175, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1550445897.253518}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:20 INFO 140471953696576] #throughput_metric: host=algo-2, train throughput=1915.2483905 records/second\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:25:20.304] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 23050, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:20 INFO 140471953696576] \u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:20 INFO 140471953696576] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:20 INFO 140262573676352] # Finished training epoch 1 on 44144 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:20 INFO 140262573676352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:20 INFO 140262573676352] Loss (name: value) total: 3.85471145364\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:20 INFO 140262573676352] Loss (name: value) kld: 0.0635055837773\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:20 INFO 140262573676352] Loss (name: value) recons: 3.79120586964\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:20 INFO 140262573676352] Loss (name: value) logppx: 3.85471145364\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:20 INFO 140262573676352] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=3.85471145364\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:20 INFO 140262573676352] Timing: train: 22.96s, val: 0.00s, epoch: 22.96s\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:20 INFO 140262573676352] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Total Records Seen\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1550445920.21043, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1550445897.253813}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:20 INFO 140262573676352] #throughput_metric: host=algo-1, train throughput=1922.91860349 records/second\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:25:20.210] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 22956, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:20 INFO 140262573676352] \u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:20 INFO 140262573676352] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:43 INFO 140262573676352] # Finished training epoch 2 on 44144 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:43 INFO 140262573676352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:43 INFO 140262573676352] Loss (name: value) total: 3.80011166585\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:43 INFO 140262573676352] Loss (name: value) kld: 0.131058116912\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:43 INFO 140262573676352] Loss (name: value) recons: 3.66905355108\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:43 INFO 140262573676352] Loss (name: value) logppx: 3.80011166585\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:43 INFO 140262573676352] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=3.80011166585\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:43 INFO 140262573676352] Timing: train: 22.89s, val: 0.00s, epoch: 22.89s\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:43 INFO 140262573676352] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Total Records Seen\": {\"count\": 1, \"max\": 88288, \"sum\": 88288.0, \"min\": 88288}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1550445943.105716, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1550445920.210937}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:43 INFO 140262573676352] #throughput_metric: host=algo-1, train throughput=1928.10269996 records/second\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:25:43.106] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 22894, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:43 INFO 140262573676352] \u001b[0m\n",
      "\u001b[31m[02/17/2019 23:25:43 INFO 140262573676352] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:43 INFO 140471953696576] # Finished training epoch 2 on 44148 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:43 INFO 140471953696576] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:43 INFO 140471953696576] Loss (name: value) total: 3.81890590191\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:43 INFO 140471953696576] Loss (name: value) kld: 0.133452398532\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:43 INFO 140471953696576] Loss (name: value) recons: 3.68545350225\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:43 INFO 140471953696576] Loss (name: value) logppx: 3.81890590191\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:43 INFO 140471953696576] #quality_metric: host=algo-2, epoch=2, train total_loss <loss>=3.81890590191\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:43 INFO 140471953696576] Timing: train: 22.89s, val: 0.00s, epoch: 22.89s\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:43 INFO 140471953696576] #progress_metric: host=algo-2, completed 1 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2944, \"sum\": 2944.0, \"min\": 2944}, \"Total Records Seen\": {\"count\": 1, \"max\": 88296, \"sum\": 88296.0, \"min\": 88296}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1550445943.195535, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1550445920.304447}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:43 INFO 140471953696576] #throughput_metric: host=algo-2, train throughput=1928.59902824 records/second\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:25:43.195] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 22891, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:43 INFO 140471953696576] \u001b[0m\n",
      "\u001b[32m[02/17/2019 23:25:43 INFO 140471953696576] # Starting training for epoch 3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[02/17/2019 23:26:05 INFO 140262573676352] # Finished training epoch 3 on 44144 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:05 INFO 140262573676352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:05 INFO 140262573676352] Loss (name: value) total: 3.59882226068\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:05 INFO 140262573676352] Loss (name: value) kld: 0.253629735852\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:05 INFO 140262573676352] Loss (name: value) recons: 3.34519252578\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:05 INFO 140262573676352] Loss (name: value) logppx: 3.59882226068\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:05 INFO 140262573676352] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=3.59882226068\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:05 INFO 140262573676352] Timing: train: 22.68s, val: 0.00s, epoch: 22.69s\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:05 INFO 140262573676352] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4416, \"sum\": 4416.0, \"min\": 4416}, \"Total Records Seen\": {\"count\": 1, \"max\": 132432, \"sum\": 132432.0, \"min\": 132432}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1550445965.79354, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1550445943.1062}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:05 INFO 140262573676352] #throughput_metric: host=algo-1, train throughput=1945.73777525 records/second\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:26:05.793] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 3, \"duration\": 22687, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:05 INFO 140262573676352] \u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:05 INFO 140262573676352] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:05 INFO 140471953696576] # Finished training epoch 3 on 44148 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:05 INFO 140471953696576] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:05 INFO 140471953696576] Loss (name: value) total: 3.62006867027\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:05 INFO 140471953696576] Loss (name: value) kld: 0.255234079975\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:05 INFO 140471953696576] Loss (name: value) recons: 3.36483459343\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:05 INFO 140471953696576] Loss (name: value) logppx: 3.62006867027\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:05 INFO 140471953696576] #quality_metric: host=algo-2, epoch=3, train total_loss <loss>=3.62006867027\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:05 INFO 140471953696576] Timing: train: 22.79s, val: 0.00s, epoch: 22.79s\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:05 INFO 140471953696576] #progress_metric: host=algo-2, completed 2 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4416, \"sum\": 4416.0, \"min\": 4416}, \"Total Records Seen\": {\"count\": 1, \"max\": 132444, \"sum\": 132444.0, \"min\": 132444}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1550445965.987801, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1550445943.195796}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:05 INFO 140471953696576] #throughput_metric: host=algo-2, train throughput=1936.97664943 records/second\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:26:05.988] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 3, \"duration\": 22792, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:05 INFO 140471953696576] \u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:05 INFO 140471953696576] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:29 INFO 140262573676352] # Finished training epoch 4 on 44144 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:29 INFO 140262573676352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:29 INFO 140262573676352] Loss (name: value) total: 3.4414298137\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:29 INFO 140262573676352] Loss (name: value) kld: 0.344984488643\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:29 INFO 140262573676352] Loss (name: value) recons: 3.09644532826\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:29 INFO 140262573676352] Loss (name: value) logppx: 3.4414298137\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:29 INFO 140262573676352] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=3.4414298137\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:29 INFO 140262573676352] patience losses:[3.854711453638215, 3.8001116658466452, 3.5988222606804059] min patience loss:3.59882226068 current loss:3.4414298137 absolute loss difference:0.157392446978\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:29 INFO 140262573676352] Timing: train: 23.48s, val: 0.00s, epoch: 23.49s\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:29 INFO 140262573676352] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5888, \"sum\": 5888.0, \"min\": 5888}, \"Total Records Seen\": {\"count\": 1, \"max\": 176576, \"sum\": 176576.0, \"min\": 176576}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1550445989.280933, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1550445965.793848}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:29 INFO 140262573676352] #throughput_metric: host=algo-1, train throughput=1879.49003224 records/second\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:26:29.281] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 23486, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:29 INFO 140262573676352] \u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:29 INFO 140262573676352] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:29 INFO 140471953696576] # Finished training epoch 4 on 44148 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:29 INFO 140471953696576] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:29 INFO 140471953696576] Loss (name: value) total: 3.46254668167\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:29 INFO 140471953696576] Loss (name: value) kld: 0.346268488532\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:29 INFO 140471953696576] Loss (name: value) recons: 3.11627819158\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:29 INFO 140471953696576] Loss (name: value) logppx: 3.46254668167\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:29 INFO 140471953696576] #quality_metric: host=algo-2, epoch=4, train total_loss <loss>=3.46254668167\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:29 INFO 140471953696576] patience losses:[3.8750363669533661, 3.8189059019088747, 3.6200686702693718] min patience loss:3.62006867027 current loss:3.46254668167 absolute loss difference:0.157521988603\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:29 INFO 140471953696576] Timing: train: 23.62s, val: 0.00s, epoch: 23.62s\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:29 INFO 140471953696576] #progress_metric: host=algo-2, completed 2 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5888, \"sum\": 5888.0, \"min\": 5888}, \"Total Records Seen\": {\"count\": 1, \"max\": 176592, \"sum\": 176592.0, \"min\": 176592}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1550445989.609856, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1550445965.988175}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:29 INFO 140471953696576] #throughput_metric: host=algo-2, train throughput=1868.95030212 records/second\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:26:29.610] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 23621, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:29 INFO 140471953696576] \u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:29 INFO 140471953696576] # Starting training for epoch 5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[02/17/2019 23:26:51 INFO 140262573676352] # Finished training epoch 5 on 44144 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:51 INFO 140262573676352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:51 INFO 140262573676352] Loss (name: value) total: 3.25320331351\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:51 INFO 140262573676352] Loss (name: value) kld: 0.446331709764\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:51 INFO 140262573676352] Loss (name: value) recons: 2.80687160276\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:51 INFO 140262573676352] Loss (name: value) logppx: 3.25320331351\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:51 INFO 140262573676352] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=3.25320331351\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:51 INFO 140262573676352] patience losses:[3.8001116658466452, 3.5988222606804059, 3.4414298137029014] min patience loss:3.4414298137 current loss:3.25320331351 absolute loss difference:0.188226500197\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:51 INFO 140262573676352] Timing: train: 22.65s, val: 0.00s, epoch: 22.65s\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:51 INFO 140262573676352] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7360, \"sum\": 7360.0, \"min\": 7360}, \"Total Records Seen\": {\"count\": 1, \"max\": 220720, \"sum\": 220720.0, \"min\": 220720}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1550446011.932829, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1550445989.281185}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:51 INFO 140262573676352] #throughput_metric: host=algo-1, train throughput=1948.80803815 records/second\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:26:51.933] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 22651, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:51 INFO 140262573676352] \u001b[0m\n",
      "\u001b[31m[02/17/2019 23:26:51 INFO 140262573676352] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:52 INFO 140471953696576] # Finished training epoch 5 on 44148 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:52 INFO 140471953696576] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:52 INFO 140471953696576] Loss (name: value) total: 3.27282691987\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:52 INFO 140471953696576] Loss (name: value) kld: 0.446567744254\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:52 INFO 140471953696576] Loss (name: value) recons: 2.82625917309\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:52 INFO 140471953696576] Loss (name: value) logppx: 3.27282691987\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:52 INFO 140471953696576] #quality_metric: host=algo-2, epoch=5, train total_loss <loss>=3.27282691987\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:52 INFO 140471953696576] patience losses:[3.8189059019088747, 3.6200686702693718, 3.4625466816667196] min patience loss:3.46254668167 current loss:3.27282691987 absolute loss difference:0.1897197618\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:52 INFO 140471953696576] Timing: train: 22.68s, val: 0.00s, epoch: 22.68s\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:52 INFO 140471953696576] #progress_metric: host=algo-2, completed 3 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7360, \"sum\": 7360.0, \"min\": 7360}, \"Total Records Seen\": {\"count\": 1, \"max\": 220740, \"sum\": 220740.0, \"min\": 220740}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1550446012.289318, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1550445989.610106}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:52 INFO 140471953696576] #throughput_metric: host=algo-2, train throughput=1946.442082 records/second\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:26:52.291] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 22681, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:52 INFO 140471953696576] \u001b[0m\n",
      "\u001b[32m[02/17/2019 23:26:52 INFO 140471953696576] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:14 INFO 140262573676352] # Finished training epoch 6 on 44144 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:14 INFO 140262573676352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:14 INFO 140262573676352] Loss (name: value) total: 3.12473920653\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:14 INFO 140262573676352] Loss (name: value) kld: 0.51852934679\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:14 INFO 140262573676352] Loss (name: value) recons: 2.60620986033\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:14 INFO 140262573676352] Loss (name: value) logppx: 3.12473920653\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:14 INFO 140262573676352] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=3.12473920653\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:14 INFO 140262573676352] patience losses:[3.5988222606804059, 3.4414298137029014, 3.2532033135061678] min patience loss:3.25320331351 current loss:3.12473920653 absolute loss difference:0.128464106978\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:14 INFO 140262573676352] Timing: train: 22.35s, val: 0.00s, epoch: 22.35s\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:14 INFO 140262573676352] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8832, \"sum\": 8832.0, \"min\": 8832}, \"Total Records Seen\": {\"count\": 1, \"max\": 264864, \"sum\": 264864.0, \"min\": 264864}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1550446034.283528, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1550446011.933137}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:14 INFO 140262573676352] #throughput_metric: host=algo-1, train throughput=1975.07674099 records/second\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:27:14.283] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 22350, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:14 INFO 140262573676352] \u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:14 INFO 140262573676352] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:14 INFO 140471953696576] # Finished training epoch 6 on 44148 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:14 INFO 140471953696576] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:14 INFO 140471953696576] Loss (name: value) total: 3.14459729981\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:14 INFO 140471953696576] Loss (name: value) kld: 0.519944461025\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:14 INFO 140471953696576] Loss (name: value) recons: 2.62465284009\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:14 INFO 140471953696576] Loss (name: value) logppx: 3.14459729981\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:14 INFO 140471953696576] #quality_metric: host=algo-2, epoch=6, train total_loss <loss>=3.14459729981\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:14 INFO 140471953696576] patience losses:[3.6200686702693718, 3.4625466816667196, 3.2728269198666449] min patience loss:3.27282691987 current loss:3.14459729981 absolute loss difference:0.128229620059\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:14 INFO 140471953696576] Timing: train: 22.36s, val: 0.00s, epoch: 22.37s\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:14 INFO 140471953696576] #progress_metric: host=algo-2, completed 4 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8832, \"sum\": 8832.0, \"min\": 8832}, \"Total Records Seen\": {\"count\": 1, \"max\": 264888, \"sum\": 264888.0, \"min\": 264888}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1550446034.66056, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1550446012.292067}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:14 INFO 140471953696576] #throughput_metric: host=algo-2, train throughput=1973.65667448 records/second\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:27:14.660] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 22368, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:14 INFO 140471953696576] \u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:14 INFO 140471953696576] # Starting training for epoch 7\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[02/17/2019 23:27:37 INFO 140262573676352] # Finished training epoch 7 on 44144 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:37 INFO 140262573676352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:37 INFO 140262573676352] Loss (name: value) total: 3.02782568465\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:37 INFO 140262573676352] Loss (name: value) kld: 0.568294474504\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:37 INFO 140262573676352] Loss (name: value) recons: 2.45953120641\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:37 INFO 140262573676352] Loss (name: value) logppx: 3.02782568465\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:37 INFO 140262573676352] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=3.02782568465\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:37 INFO 140262573676352] patience losses:[3.4414298137029014, 3.2532033135061678, 3.1247392065283179] min patience loss:3.12473920653 current loss:3.02782568465 absolute loss difference:0.0969135218772\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:37 INFO 140262573676352] Timing: train: 22.87s, val: 0.00s, epoch: 22.88s\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:37 INFO 140262573676352] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Total Batches Seen\": {\"count\": 1, \"max\": 10304, \"sum\": 10304.0, \"min\": 10304}, \"Total Records Seen\": {\"count\": 1, \"max\": 309008, \"sum\": 309008.0, \"min\": 309008}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1550446057.161009, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1550446034.283795}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:37 INFO 140262573676352] #throughput_metric: host=algo-1, train throughput=1929.59268268 records/second\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:27:37.161] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 7, \"duration\": 22877, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:37 INFO 140262573676352] \u001b[0m\n",
      "\u001b[31m[02/17/2019 23:27:37 INFO 140262573676352] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:37 INFO 140471953696576] # Finished training epoch 7 on 44148 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:37 INFO 140471953696576] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:37 INFO 140471953696576] Loss (name: value) total: 3.04690454585\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:37 INFO 140471953696576] Loss (name: value) kld: 0.570111180064\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:37 INFO 140471953696576] Loss (name: value) recons: 2.47679336978\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:37 INFO 140471953696576] Loss (name: value) logppx: 3.04690454585\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:37 INFO 140471953696576] #quality_metric: host=algo-2, epoch=7, train total_loss <loss>=3.04690454585\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:37 INFO 140471953696576] patience losses:[3.4625466816667196, 3.2728269198666449, 3.1445972998073137] min patience loss:3.14459729981 current loss:3.04690454585 absolute loss difference:0.0976927539577\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:37 INFO 140471953696576] Timing: train: 22.87s, val: 0.00s, epoch: 22.87s\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:37 INFO 140471953696576] #progress_metric: host=algo-2, completed 4 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Total Batches Seen\": {\"count\": 1, \"max\": 10304, \"sum\": 10304.0, \"min\": 10304}, \"Total Records Seen\": {\"count\": 1, \"max\": 309036, \"sum\": 309036.0, \"min\": 309036}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1550446057.534425, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1550446034.66081}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:37 INFO 140471953696576] #throughput_metric: host=algo-2, train throughput=1930.07216092 records/second\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:27:37.534] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 7, \"duration\": 22873, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:37 INFO 140471953696576] \u001b[0m\n",
      "\u001b[32m[02/17/2019 23:27:37 INFO 140471953696576] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:00 INFO 140262573676352] # Finished training epoch 8 on 44144 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:00 INFO 140262573676352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:00 INFO 140262573676352] Loss (name: value) total: 2.98861535312\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:00 INFO 140262573676352] Loss (name: value) kld: 0.589569519395\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:00 INFO 140262573676352] Loss (name: value) recons: 2.3990458314\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:00 INFO 140262573676352] Loss (name: value) logppx: 2.98861535312\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:00 INFO 140262573676352] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=2.98861535312\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:00 INFO 140262573676352] patience losses:[3.2532033135061678, 3.1247392065283179, 3.0278256846510847] min patience loss:3.02782568465 current loss:2.98861535312 absolute loss difference:0.0392103315264\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:00 INFO 140262573676352] Timing: train: 22.89s, val: 0.00s, epoch: 22.90s\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:00 INFO 140262573676352] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Total Batches Seen\": {\"count\": 1, \"max\": 11776, \"sum\": 11776.0, \"min\": 11776}, \"Total Records Seen\": {\"count\": 1, \"max\": 353152, \"sum\": 353152.0, \"min\": 353152}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1550446080.059205, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1550446057.16127}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:00 INFO 140262573676352] #throughput_metric: host=algo-1, train throughput=1927.82886971 records/second\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:28:00.059] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 22898, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:00 INFO 140262573676352] \u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:00 INFO 140262573676352] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:00 INFO 140471953696576] # Finished training epoch 8 on 44148 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:00 INFO 140471953696576] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:00 INFO 140471953696576] Loss (name: value) total: 3.00928643234\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:00 INFO 140471953696576] Loss (name: value) kld: 0.591195695318\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:00 INFO 140471953696576] Loss (name: value) recons: 2.4180907357\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:00 INFO 140471953696576] Loss (name: value) logppx: 3.00928643234\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:00 INFO 140471953696576] #quality_metric: host=algo-2, epoch=8, train total_loss <loss>=3.00928643234\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:00 INFO 140471953696576] patience losses:[3.2728269198666449, 3.1445972998073137, 3.0469045458496482] min patience loss:3.04690454585 current loss:3.00928643234 absolute loss difference:0.0376181135143\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:00 INFO 140471953696576] Timing: train: 22.91s, val: 0.00s, epoch: 22.92s\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:00 INFO 140471953696576] #progress_metric: host=algo-2, completed 5 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Total Batches Seen\": {\"count\": 1, \"max\": 11776, \"sum\": 11776.0, \"min\": 11776}, \"Total Records Seen\": {\"count\": 1, \"max\": 353184, \"sum\": 353184.0, \"min\": 353184}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1550446080.452041, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1550446057.534846}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:00 INFO 140471953696576] #throughput_metric: host=algo-2, train throughput=1926.39107575 records/second\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:28:00.452] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 22917, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:00 INFO 140471953696576] \u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:00 INFO 140471953696576] # Starting training for epoch 9\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[02/17/2019 23:28:22 INFO 140262573676352] # Finished training epoch 9 on 44144 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:22 INFO 140262573676352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:22 INFO 140262573676352] Loss (name: value) total: 2.97319692356\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:22 INFO 140262573676352] Loss (name: value) kld: 0.60796623528\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:22 INFO 140262573676352] Loss (name: value) recons: 2.36523068759\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:22 INFO 140262573676352] Loss (name: value) logppx: 2.97319692356\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:22 INFO 140262573676352] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=2.97319692356\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:22 INFO 140262573676352] patience losses:[3.1247392065283179, 3.0278256846510847, 2.9886153531247293] min patience loss:2.98861535312 current loss:2.97319692356 absolute loss difference:0.0154184295647\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:22 INFO 140262573676352] Timing: train: 22.46s, val: 0.00s, epoch: 22.46s\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:22 INFO 140262573676352] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Total Batches Seen\": {\"count\": 1, \"max\": 13248, \"sum\": 13248.0, \"min\": 13248}, \"Total Records Seen\": {\"count\": 1, \"max\": 397296, \"sum\": 397296.0, \"min\": 397296}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1550446102.519385, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1550446080.059738}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:22 INFO 140262573676352] #throughput_metric: host=algo-1, train throughput=1965.46552071 records/second\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:28:22.519] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 9, \"duration\": 22459, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:22 INFO 140262573676352] \u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:22 INFO 140262573676352] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:23 INFO 140471953696576] # Finished training epoch 9 on 44148 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:23 INFO 140471953696576] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:23 INFO 140471953696576] Loss (name: value) total: 2.99213286422\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:23 INFO 140471953696576] Loss (name: value) kld: 0.609535762884\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:23 INFO 140471953696576] Loss (name: value) recons: 2.38259710445\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:23 INFO 140471953696576] Loss (name: value) logppx: 2.99213286422\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:23 INFO 140471953696576] #quality_metric: host=algo-2, epoch=9, train total_loss <loss>=2.99213286422\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:23 INFO 140471953696576] patience losses:[3.1445972998073137, 3.0469045458496482, 3.0092864323353421] min patience loss:3.00928643234 current loss:2.99213286422 absolute loss difference:0.0171535681123\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:23 INFO 140471953696576] Timing: train: 22.60s, val: 0.00s, epoch: 22.60s\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:23 INFO 140471953696576] #progress_metric: host=algo-2, completed 6 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Total Batches Seen\": {\"count\": 1, \"max\": 13248, \"sum\": 13248.0, \"min\": 13248}, \"Total Records Seen\": {\"count\": 1, \"max\": 397332, \"sum\": 397332.0, \"min\": 397332}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1550446103.056434, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1550446080.452304}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:23 INFO 140471953696576] #throughput_metric: host=algo-2, train throughput=1953.04317799 records/second\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:28:23.057] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 9, \"duration\": 22604, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:23 INFO 140471953696576] \u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:23 INFO 140471953696576] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:46 INFO 140262573676352] # Finished training epoch 10 on 44144 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:46 INFO 140262573676352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:46 INFO 140262573676352] Loss (name: value) total: 2.96377818187\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:46 INFO 140262573676352] Loss (name: value) kld: 0.624120367264\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:46 INFO 140262573676352] Loss (name: value) recons: 2.33965781184\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:46 INFO 140262573676352] Loss (name: value) logppx: 2.96377818187\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:46 INFO 140262573676352] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=2.96377818187\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:46 INFO 140262573676352] patience losses:[3.0278256846510847, 2.9886153531247293, 2.9731969235599904] min patience loss:2.97319692356 current loss:2.96377818187 absolute loss difference:0.00941874168921\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:46 INFO 140262573676352] Timing: train: 23.84s, val: 0.00s, epoch: 23.85s\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:46 INFO 140262573676352] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Total Batches Seen\": {\"count\": 1, \"max\": 14720, \"sum\": 14720.0, \"min\": 14720}, \"Total Records Seen\": {\"count\": 1, \"max\": 441440, \"sum\": 441440.0, \"min\": 441440}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1550446126.367804, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1550446102.519695}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:46 INFO 140262573676352] #throughput_metric: host=algo-1, train throughput=1851.03409323 records/second\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:28:46.368] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 23848, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:46 INFO 140262573676352] \u001b[0m\n",
      "\u001b[31m[02/17/2019 23:28:46 INFO 140262573676352] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:47 INFO 140471953696576] # Finished training epoch 10 on 44148 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:47 INFO 140471953696576] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:47 INFO 140471953696576] Loss (name: value) total: 2.98009155358\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:47 INFO 140471953696576] Loss (name: value) kld: 0.626898378091\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:47 INFO 140471953696576] Loss (name: value) recons: 2.35319317813\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:47 INFO 140471953696576] Loss (name: value) logppx: 2.98009155358\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:47 INFO 140471953696576] #quality_metric: host=algo-2, epoch=10, train total_loss <loss>=2.98009155358\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:47 INFO 140471953696576] patience losses:[3.0469045458496482, 3.0092864323353421, 2.9921328642230103] min patience loss:2.99213286422 current loss:2.98009155358 absolute loss difference:0.0120413106421\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:47 INFO 140471953696576] Timing: train: 23.96s, val: 0.00s, epoch: 23.96s\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:47 INFO 140471953696576] #progress_metric: host=algo-2, completed 6 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Total Batches Seen\": {\"count\": 1, \"max\": 14720, \"sum\": 14720.0, \"min\": 14720}, \"Total Records Seen\": {\"count\": 1, \"max\": 441480, \"sum\": 441480.0, \"min\": 441480}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1550446127.01661, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1550446103.057457}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:47 INFO 140471953696576] #throughput_metric: host=algo-2, train throughput=1842.62456133 records/second\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:28:47.016] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 23957, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:47 INFO 140471953696576] \u001b[0m\n",
      "\u001b[32m[02/17/2019 23:28:47 INFO 140471953696576] # Starting training for epoch 11\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[02/17/2019 23:29:08 INFO 140262573676352] # Finished training epoch 11 on 44144 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:08 INFO 140262573676352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:08 INFO 140262573676352] Loss (name: value) total: 2.95378539407\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:08 INFO 140262573676352] Loss (name: value) kld: 0.638607970118\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:08 INFO 140262573676352] Loss (name: value) recons: 2.31517742553\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:08 INFO 140262573676352] Loss (name: value) logppx: 2.95378539407\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:08 INFO 140262573676352] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=2.95378539407\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:08 INFO 140262573676352] patience losses:[2.9886153531247293, 2.9731969235599904, 2.9637781818707785] min patience loss:2.96377818187 current loss:2.95378539407 absolute loss difference:0.00999278780343\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:08 INFO 140262573676352] Timing: train: 22.36s, val: 0.00s, epoch: 22.36s\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:08 INFO 140262573676352] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Total Batches Seen\": {\"count\": 1, \"max\": 16192, \"sum\": 16192.0, \"min\": 16192}, \"Total Records Seen\": {\"count\": 1, \"max\": 485584, \"sum\": 485584.0, \"min\": 485584}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1550446148.730447, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1550446126.36814}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:08 INFO 140262573676352] #throughput_metric: host=algo-1, train throughput=1974.01475794 records/second\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:29:08.730] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 22362, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:08 INFO 140262573676352] \u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:08 INFO 140262573676352] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:09 INFO 140471953696576] # Finished training epoch 11 on 44148 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:09 INFO 140471953696576] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:09 INFO 140471953696576] Loss (name: value) total: 2.97348198502\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:09 INFO 140471953696576] Loss (name: value) kld: 0.64104687077\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:09 INFO 140471953696576] Loss (name: value) recons: 2.33243510917\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:09 INFO 140471953696576] Loss (name: value) logppx: 2.97348198502\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:09 INFO 140471953696576] #quality_metric: host=algo-2, epoch=11, train total_loss <loss>=2.97348198502\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:09 INFO 140471953696576] patience losses:[3.0092864323353421, 2.9921328642230103, 2.9800915535809338] min patience loss:2.98009155358 current loss:2.97348198502 absolute loss difference:0.00660956856133\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:09 INFO 140471953696576] Timing: train: 22.45s, val: 0.01s, epoch: 22.46s\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:09 INFO 140471953696576] #progress_metric: host=algo-2, completed 7 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Total Batches Seen\": {\"count\": 1, \"max\": 16192, \"sum\": 16192.0, \"min\": 16192}, \"Total Records Seen\": {\"count\": 1, \"max\": 485628, \"sum\": 485628.0, \"min\": 485628}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1550446149.478815, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1550446127.016909}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:09 INFO 140471953696576] #throughput_metric: host=algo-2, train throughput=1965.41155172 records/second\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:29:09.479] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 22462, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:09 INFO 140471953696576] \u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:09 INFO 140471953696576] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:31 INFO 140262573676352] # Finished training epoch 12 on 44144 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:31 INFO 140262573676352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:31 INFO 140262573676352] Loss (name: value) total: 2.95334146144\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:31 INFO 140262573676352] Loss (name: value) kld: 0.654337975815\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:31 INFO 140262573676352] Loss (name: value) recons: 2.29900348195\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:31 INFO 140262573676352] Loss (name: value) logppx: 2.95334146144\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:31 INFO 140262573676352] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=2.95334146144\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:31 INFO 140262573676352] patience losses:[2.9731969235599904, 2.9637781818707785, 2.9537853940673497] min patience loss:2.95378539407 current loss:2.95334146144 absolute loss difference:0.000443932630014\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:31 INFO 140262573676352] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:31 INFO 140262573676352] Timing: train: 22.64s, val: 0.01s, epoch: 22.65s\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:31 INFO 140262573676352] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Total Batches Seen\": {\"count\": 1, \"max\": 17664, \"sum\": 17664.0, \"min\": 17664}, \"Total Records Seen\": {\"count\": 1, \"max\": 529728, \"sum\": 529728.0, \"min\": 529728}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1550446171.381138, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1550446148.731009}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:31 INFO 140262573676352] #throughput_metric: host=algo-1, train throughput=1948.92865577 records/second\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:29:31.381] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 22650, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:31 INFO 140262573676352] \u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:31 INFO 140262573676352] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:32 INFO 140471953696576] # Finished training epoch 12 on 44148 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:32 INFO 140471953696576] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:32 INFO 140471953696576] Loss (name: value) total: 2.97542397976\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:32 INFO 140471953696576] Loss (name: value) kld: 0.657266930437\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:32 INFO 140471953696576] Loss (name: value) recons: 2.31815705148\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:32 INFO 140471953696576] Loss (name: value) logppx: 2.97542397976\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:32 INFO 140471953696576] #quality_metric: host=algo-2, epoch=12, train total_loss <loss>=2.97542397976\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:32 INFO 140471953696576] patience losses:[2.9921328642230103, 2.9800915535809338, 2.973481985019601] min patience loss:2.97348198502 current loss:2.97542397976 absolute loss difference:0.00194199473962\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:32 INFO 140471953696576] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:32 INFO 140471953696576] Timing: train: 22.82s, val: 0.00s, epoch: 22.82s\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:32 INFO 140471953696576] #progress_metric: host=algo-2, completed 8 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Total Batches Seen\": {\"count\": 1, \"max\": 17664, \"sum\": 17664.0, \"min\": 17664}, \"Total Records Seen\": {\"count\": 1, \"max\": 529776, \"sum\": 529776.0, \"min\": 529776}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1550446172.296003, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1550446149.479608}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:32 INFO 140471953696576] #throughput_metric: host=algo-2, train throughput=1934.91000634 records/second\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:29:32.296] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 22816, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:32 INFO 140471953696576] \u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:32 INFO 140471953696576] # Starting training for epoch 13\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[02/17/2019 23:29:54 INFO 140262573676352] # Finished training epoch 13 on 44144 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:54 INFO 140262573676352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:54 INFO 140262573676352] Loss (name: value) total: 2.95553899701\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:54 INFO 140262573676352] Loss (name: value) kld: 0.661925343996\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:54 INFO 140262573676352] Loss (name: value) recons: 2.29361365152\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:54 INFO 140262573676352] Loss (name: value) logppx: 2.95553899701\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:54 INFO 140262573676352] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=2.95553899701\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:54 INFO 140262573676352] patience losses:[2.9637781818707785, 2.9537853940673497, 2.9533414614373359] min patience loss:2.95334146144 current loss:2.95553899701 absolute loss difference:0.00219753557357\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:54 INFO 140262573676352] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:54 INFO 140262573676352] Timing: train: 23.11s, val: 0.00s, epoch: 23.11s\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:54 INFO 140262573676352] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Total Batches Seen\": {\"count\": 1, \"max\": 19136, \"sum\": 19136.0, \"min\": 19136}, \"Total Records Seen\": {\"count\": 1, \"max\": 573872, \"sum\": 573872.0, \"min\": 573872}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1550446194.487274, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1550446171.381451}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:54 INFO 140262573676352] #throughput_metric: host=algo-1, train throughput=1910.50200454 records/second\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:29:54.487] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 13, \"duration\": 23105, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:54 INFO 140262573676352] \u001b[0m\n",
      "\u001b[31m[02/17/2019 23:29:54 INFO 140262573676352] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:55 INFO 140471953696576] # Finished training epoch 13 on 44148 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:55 INFO 140471953696576] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:55 INFO 140471953696576] Loss (name: value) total: 2.97570020401\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:55 INFO 140471953696576] Loss (name: value) kld: 0.664065537267\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:55 INFO 140471953696576] Loss (name: value) recons: 2.31163467061\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:55 INFO 140471953696576] Loss (name: value) logppx: 2.97570020401\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:55 INFO 140471953696576] #quality_metric: host=algo-2, epoch=13, train total_loss <loss>=2.97570020401\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:55 INFO 140471953696576] patience losses:[2.9800915535809338, 2.973481985019601, 2.9754239797592161] min patience loss:2.97348198502 current loss:2.97570020401 absolute loss difference:0.00221821898999\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:55 INFO 140471953696576] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:55 INFO 140471953696576] Timing: train: 23.18s, val: 0.00s, epoch: 23.18s\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:55 INFO 140471953696576] #progress_metric: host=algo-2, completed 8 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Total Batches Seen\": {\"count\": 1, \"max\": 19136, \"sum\": 19136.0, \"min\": 19136}, \"Total Records Seen\": {\"count\": 1, \"max\": 573924, \"sum\": 573924.0, \"min\": 573924}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1550446195.4766, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1550446172.296403}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:55 INFO 140471953696576] #throughput_metric: host=algo-2, train throughput=1904.54046247 records/second\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:29:55.476] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 13, \"duration\": 23180, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:55 INFO 140471953696576] \u001b[0m\n",
      "\u001b[32m[02/17/2019 23:29:55 INFO 140471953696576] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:18 INFO 140262573676352] # Finished training epoch 14 on 44144 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:18 INFO 140262573676352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:18 INFO 140262573676352] Loss (name: value) total: 2.95813674037\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:18 INFO 140262573676352] Loss (name: value) kld: 0.667595581581\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:18 INFO 140262573676352] Loss (name: value) recons: 2.29054116028\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:18 INFO 140262573676352] Loss (name: value) logppx: 2.95813674037\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:18 INFO 140262573676352] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=2.95813674037\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:18 INFO 140262573676352] patience losses:[2.9537853940673497, 2.9533414614373359, 2.9555389970109083] min patience loss:2.95334146144 current loss:2.95813674037 absolute loss difference:0.00479527893274\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:18 INFO 140262573676352] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:18 INFO 140262573676352] Timing: train: 23.65s, val: 0.00s, epoch: 23.65s\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:18 INFO 140262573676352] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Total Batches Seen\": {\"count\": 1, \"max\": 20608, \"sum\": 20608.0, \"min\": 20608}, \"Total Records Seen\": {\"count\": 1, \"max\": 618016, \"sum\": 618016.0, \"min\": 618016}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1550446218.133813, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1550446194.487554}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:18 INFO 140262573676352] #throughput_metric: host=algo-1, train throughput=1866.83832774 records/second\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:30:18.134] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 23646, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:18 INFO 140262573676352] \u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:18 INFO 140262573676352] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:19 INFO 140471953696576] # Finished training epoch 14 on 44148 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:19 INFO 140471953696576] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:19 INFO 140471953696576] Loss (name: value) total: 2.97644856745\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:19 INFO 140471953696576] Loss (name: value) kld: 0.670347416271\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:19 INFO 140471953696576] Loss (name: value) recons: 2.30610115191\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:19 INFO 140471953696576] Loss (name: value) logppx: 2.97644856745\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:19 INFO 140471953696576] #quality_metric: host=algo-2, epoch=14, train total_loss <loss>=2.97644856745\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:19 INFO 140471953696576] patience losses:[2.973481985019601, 2.9754239797592161, 2.9757002040095952] min patience loss:2.97348198502 current loss:2.97644856745 absolute loss difference:0.00296658242958\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:19 INFO 140471953696576] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:19 INFO 140471953696576] Timing: train: 23.69s, val: 0.00s, epoch: 23.69s\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:19 INFO 140471953696576] #progress_metric: host=algo-2, completed 9 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Total Batches Seen\": {\"count\": 1, \"max\": 20608, \"sum\": 20608.0, \"min\": 20608}, \"Total Records Seen\": {\"count\": 1, \"max\": 618072, \"sum\": 618072.0, \"min\": 618072}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1550446219.172282, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1550446195.476908}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:19 INFO 140471953696576] #throughput_metric: host=algo-2, train throughput=1863.13464196 records/second\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:30:19.172] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 23695, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:19 INFO 140471953696576] \u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:19 INFO 140471953696576] # Starting training for epoch 15\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[02/17/2019 23:30:40 INFO 140262573676352] # Finished training epoch 15 on 44144 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:40 INFO 140262573676352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:40 INFO 140262573676352] Loss (name: value) total: 2.96664211111\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:40 INFO 140262573676352] Loss (name: value) kld: 0.670781430127\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:40 INFO 140262573676352] Loss (name: value) recons: 2.29586068383\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:40 INFO 140262573676352] Loss (name: value) logppx: 2.96664211111\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:40 INFO 140262573676352] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=2.96664211111\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:40 INFO 140262573676352] patience losses:[2.9533414614373359, 2.9555389970109083, 2.9581367403700733] min patience loss:2.95334146144 current loss:2.96664211111 absolute loss difference:0.0133006496706\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:40 INFO 140262573676352] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:40 INFO 140262573676352] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:40 INFO 140262573676352] Timing: train: 22.73s, val: 0.00s, epoch: 22.73s\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:40 INFO 140262573676352] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:40 INFO 140262573676352] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Total Batches Seen\": {\"count\": 1, \"max\": 22080, \"sum\": 22080.0, \"min\": 22080}, \"Total Records Seen\": {\"count\": 1, \"max\": 662160, \"sum\": 662160.0, \"min\": 662160}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44144, \"sum\": 44144.0, \"min\": 44144}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1550446240.863045, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1550446218.134104}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:40 INFO 140262573676352] #throughput_metric: host=algo-1, train throughput=1942.17259705 records/second\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] # Finished training epoch 15 on 44148 examples from 1472 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Loss (name: value) total: 2.98408995271\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Loss (name: value) kld: 0.673293775795\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Loss (name: value) recons: 2.31079617302\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Loss (name: value) logppx: 2.98408995271\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] #quality_metric: host=algo-2, epoch=15, train total_loss <loss>=2.98408995271\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] patience losses:[2.9754239797592161, 2.9757002040095952, 2.9764485674491827] min patience loss:2.97542397976 current loss:2.98408995271 absolute loss difference:0.00866597294807\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Timing: train: 22.35s, val: 0.00s, epoch: 22.35s\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] #progress_metric: host=algo-2, completed 100 % epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1472, \"sum\": 1472.0, \"min\": 1472}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Total Batches Seen\": {\"count\": 1, \"max\": 22080, \"sum\": 22080.0, \"min\": 22080}, \"Total Records Seen\": {\"count\": 1, \"max\": 662220, \"sum\": 662220.0, \"min\": 662220}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 44148, \"sum\": 44148.0, \"min\": 44148}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1550446241.522045, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1550446219.17261}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] #throughput_metric: host=algo-2, train throughput=1975.33657977 records/second\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Best model based on early stopping at epoch 11. Best loss: 2.97348198502\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Topics from epoch:final (num_topics:5) [wetc 0.50, tu 0.87]:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] [0.56, 0.93] dies charged found dead man crash woman murder accident missing three car search killed attack death injured charge four baghdad\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] [0.61, 0.82] lead final wins cup takes test win first england world open face title take second one top day court charges\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] [0.49, 0.78] set back pakistan residents win appeal top rain clash home killed world crash england water title return bid plans two\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] [0.37, 0.88] deal probe new wa death record power denies says australian fire police open council home pay vic hits face rejects\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] [0.49, 0.95] un report pm war calls iraq australia plans troops inquiry urged warns iraqi us election tour defends may bush fire\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Serializing model to /opt/ml/model/model_algo-2\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Saved checkpoint to \"/tmp/tmpuRprpf/state-0001.params\"\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:30:41.556] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 355423, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] Best model based on early stopping at epoch 12. Best loss: 2.95334146144\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] Topics from epoch:final (num_topics:5) [wetc 0.49, tu 0.88]:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] [0.56, 0.93] dies charged found dead man crash woman murder accident missing search three car killed attack injured charge death four baghdad\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] [0.61, 0.82] lead final wins cup takes test win first england world open face title take second top court day one charges\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] [0.49, 0.78] back set pakistan residents win appeal top clash home england rain world killed title crash water return bid south record\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] [0.32, 0.88] deal probe wa new death record denies power says australian police open fire council home hits rejects face qld vic\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] [0.49, 1.00] un pm report war calls plans australia iraq warns inquiry troops urged election tour us iraqi may bush defends claim\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] Saved checkpoint to \"/tmp/tmp9LWE9N/state-0001.params\"\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:30:41.554] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 356968, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] Loss (name: value) total: 2.93362134402\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] Loss (name: value) kld: 0.627517072295\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] Loss (name: value) recons: 2.30610427302\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] Loss (name: value) logppx: 2.93362134402\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 368, \"sum\": 368.0, \"min\": 368}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 368, \"sum\": 368.0, \"min\": 368}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 11036, \"sum\": 11036.0, \"min\": 11036}, \"Total Batches Seen\": {\"count\": 1, \"max\": 368, \"sum\": 368.0, \"min\": 368}, \"Total Records Seen\": {\"count\": 1, \"max\": 11036, \"sum\": 11036.0, \"min\": 11036}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 11036, \"sum\": 11036.0, \"min\": 11036}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1550446241.958482, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1550446241.554828}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/17/2019 23:30:41 INFO 140262573676352] #test_score (algo-1) : ('log_perplexity', 2.933621344016315)\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:30:41.958] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 15, \"duration\": 23824, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:30:41.958] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"duration\": 357376, \"num_epochs\": 16, \"num_examples\": 22081}\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:30:41.958] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 403, \"num_examples\": 368}\u001b[0m\n",
      "\u001b[31m[2019-02-17 23:30:41.958] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/test\", \"duration\": 357371, \"num_epochs\": 2, \"num_examples\": 369}\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 358101.44090652466, \"sum\": 358101.44090652466, \"min\": 358101.44090652466}, \"finalize.time\": {\"count\": 1, \"max\": 29.597997665405273, \"sum\": 29.597997665405273, \"min\": 29.597997665405273}, \"initialize.time\": {\"count\": 1, \"max\": 12659.371137619019, \"sum\": 12659.371137619019, \"min\": 12659.371137619019}, \"model.serialize.time\": {\"count\": 1, \"max\": 1.8541812896728516, \"sum\": 1.8541812896728516, \"min\": 1.8541812896728516}, \"setuptime\": {\"count\": 1, \"max\": 553.4529685974121, \"sum\": 553.4529685974121, \"min\": 553.4529685974121}, \"early_stop.time\": {\"count\": 15, \"max\": 5.010843276977539, \"sum\": 42.13762283325195, \"min\": 0.21386146545410156}, \"update.time\": {\"count\": 15, \"max\": 23847.924947738647, \"sum\": 343600.50559043884, \"min\": 22350.244998931885}, \"epochs\": {\"count\": 1, \"max\": 150, \"sum\": 150.0, \"min\": 150}, \"model.score.time\": {\"count\": 1, \"max\": 403.58710289001465, \"sum\": 403.58710289001465, \"min\": 403.58710289001465}}, \"EndTime\": 1550446241.959722, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1550445884.568321}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Finished scoring on 11010 examples from 367 batches, each of size 30.\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Loss (name: value) total: 2.89965695235\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Loss (name: value) kld: 0.646557020123\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Loss (name: value) recons: 2.25309992105\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] Loss (name: value) logppx: 2.89965695235\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 368, \"sum\": 368.0, \"min\": 368}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 368, \"sum\": 368.0, \"min\": 368}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 11036, \"sum\": 11036.0, \"min\": 11036}, \"Total Batches Seen\": {\"count\": 1, \"max\": 368, \"sum\": 368.0, \"min\": 368}, \"Total Records Seen\": {\"count\": 1, \"max\": 11036, \"sum\": 11036.0, \"min\": 11036}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 11036, \"sum\": 11036.0, \"min\": 11036}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1550446241.987002, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1550446241.556518}\n",
      "\u001b[0m\n",
      "\u001b[32m[02/17/2019 23:30:41 INFO 140471953696576] #test_score (algo-2) : ('log_perplexity', 2.8996569523477858)\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:30:41.987] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 15, \"duration\": 22814, \"num_examples\": 1472}\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:30:41.987] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"duration\": 355849, \"num_epochs\": 16, \"num_examples\": 22081}\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:30:41.987] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 430, \"num_examples\": 368}\u001b[0m\n",
      "\u001b[32m[2019-02-17 23:30:41.987] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/test\", \"duration\": 355853, \"num_epochs\": 2, \"num_examples\": 369}\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 355942.8927898407, \"sum\": 355942.8927898407, \"min\": 355942.8927898407}, \"finalize.time\": {\"count\": 1, \"max\": 31.03494644165039, \"sum\": 31.03494644165039, \"min\": 31.03494644165039}, \"initialize.time\": {\"count\": 1, \"max\": 11119.71116065979, \"sum\": 11119.71116065979, \"min\": 11119.71116065979}, \"model.serialize.time\": {\"count\": 1, \"max\": 2.2110939025878906, \"sum\": 2.2110939025878906, \"min\": 2.2110939025878906}, \"setuptime\": {\"count\": 1, \"max\": 45.867919921875, \"sum\": 45.867919921875, \"min\": 45.867919921875}, \"early_stop.time\": {\"count\": 15, \"max\": 9.602069854736328, \"sum\": 40.87710380554199, \"min\": 0.2090930938720703}, \"update.time\": {\"count\": 15, \"max\": 23958.97912979126, \"sum\": 344257.2810649872, \"min\": 22349.08890724182}, \"epochs\": {\"count\": 1, \"max\": 150, \"sum\": 150.0, \"min\": 150}, \"model.score.time\": {\"count\": 1, \"max\": 430.3908348083496, \"sum\": 430.3908348083496, \"min\": 430.3908348083496}}, \"EndTime\": 1550446241.988232, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1550445886.123261}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-02-17 23:30:48 Uploading - Uploading generated training model\n",
      "2019-02-17 23:30:48 Completed - Training job completed\n",
      "Billable seconds: 756\n"
     ]
    }
   ],
   "source": [
    "ntm.fit({'train': s3_train, 'test': s3_val_data, 'auxiliary': s3_aux})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics extracted, with the confidence range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.56, 0.93] dies charged found dead man crash woman murder accident missing three car search killed attack death injured charge four baghdad\n",
    "\n",
    "[0.61, 0.82] lead final wins cup takes test win first england world open face title take second one top day court charges\n",
    "\n",
    "[0.49, 0.78] set back pakistan residents win appeal top rain clash home killed world crash england water title return bid plans two\n",
    "\n",
    "[0.37, 0.88] deal probe new wa death record power denies says australian fire police open council home pay vic hits face rejects\n",
    "\n",
    "[0.49, 0.95] un report pm war calls iraq australia plans troops inquiry urged warns iraqi us election tour defends may bush fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: ntm-2019-02-17-23-21-55-061\n"
     ]
    }
   ],
   "source": [
    "print('Training job name: {}'.format(ntm.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hosting and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint with name ntm-2019-02-17-23-21-55-061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "job_name = 'ntm-2019-02-17-23-21-55-061'\n",
    "region = 'us-east-1'\n",
    "\n",
    "image_name = container\n",
    "\n",
    "# Configuration of EndPoint\n",
    "endpoint_name = sess.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    deployment_image=image_name,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "ntm_predictor = sagemaker.predictor.RealTimePredictor(\n",
    "    endpoint_name, \n",
    "    sagemaker_session=sess, \n",
    "    content_type=\"application/json\")\n",
    "\n",
    "#ntm_predictor = ntm.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: ntm-2019-02-17-23-21-55-061\n"
     ]
    }
   ],
   "source": [
    "print('Endpoint name: {}'.format(ntm_predictor.endpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert test vectors from compressed sparse matrix to dense matrix\n",
    "test_data = np.array(test_vectors.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'topic_weights': [0.0783672631, 0.0791266486, 0.0964549705, 0.0687241182, 0.6773269773]}, {'topic_weights': [0.1267461479, 0.1984856278, 0.4162698984, 0.1384287626, 0.1200695932]}, {'topic_weights': [0.0883882344, 0.102564916, 0.3129917681, 0.4054643512, 0.0905907378]}, {'topic_weights': [0.5119438171, 0.23860237, 0.0679632947, 0.1207858324, 0.0607047118]}, {'topic_weights': [0.4962130785, 0.1524228603, 0.0667938441, 0.2229692191, 0.0616010763]}]}\n"
     ]
    }
   ],
   "source": [
    "results = ntm_predictor.predict(test_data[:5])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07836726 0.07912665 0.09645497 0.06872412 0.67732698]\n",
      " [0.12674615 0.19848563 0.4162699  0.13842876 0.12006959]\n",
      " [0.08838823 0.10256492 0.31299177 0.40546435 0.09059074]\n",
      " [0.51194382 0.23860237 0.06796329 0.12078583 0.06070471]\n",
      " [0.49621308 0.15242286 0.06679384 0.22296922 0.06160108]]\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Topic ID')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fs=12\n",
    "df=pd.DataFrame(predictions.T)\n",
    "df.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "plt.xlabel('Topic ID', fontsize=fs+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SageMaker to create inference pipeline\n",
    "\n",
    "SageMaker Python SDK provides classes, such Model, SparkMLModel, & PipelineModel, to create an inference pipeline that can be used to conduct feature processing and then fit target algorithm to the processed data. Subsequently, the PipelineModel created can be deployed as an endpoint for real time inferences. Additionally, the PipelineModel can also be deployed in batch mode (Batch Transform), to get inferences for a large volume of data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SparkMLModel requires schema of the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": [{\"name\": \"headline_text\", \"type\": \"string\"}], \"output\": {\"name\": \"features\", \"type\": \"double\", \"struct\": \"vector\"}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "schema = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"name\": \"headline_text\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    ],\n",
    "    \"output\": \n",
    "        {\n",
    "            \"name\": \"features\",\n",
    "            \"type\": \"double\",\n",
    "            \"struct\": \"vector\"\n",
    "        }\n",
    "}\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://ai-in-aws/sagemaker/inference-pipeline/output/ntm-2019-02-17-23-21-55-061/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "#Get the data location of the trained ntm model\n",
    "#modeldataurl = 's3://ai-in-aws/sagemaker/inference-pipeline/output/ntm-2019-02-17-23-21-55-061/output/model.tar.gz'\n",
    "s3_ntm_output_key_prefix = 'sagemaker/inference-pipeline/output'\n",
    "modeldataurl = 's3://{}/{}/{}/{}'.format(s3_model_bucket, s3_ntm_output_key_prefix, 'ntm-2019-02-17-23-21-55-061', 'output/model.tar.gz')\n",
    "print(modeldataurl)\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Real Time Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get MLeap serialized model\n",
    "s3_ntm_output_key_prefix = 'sagemaker/inference-pipeline/output'\n",
    "\n",
    "#Get the data location of the trained ntm model\n",
    "#modeldataurl = 's3://{}/{}/{}/{}'.format(s3_model_bucket, s3_ntm_output_key_prefix, ntm.latest_training_job.job_name, 'output/model.tar.gz')\n",
    "\n",
    "sparkml_data = 's3://{}/{}/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\n",
    "\n",
    "ntm_model = Model(model_data=modeldataurl, image=container)\n",
    "\n",
    "# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\n",
    "sparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\n",
    "\n",
    "model_name = 'inference-pipeline-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "sm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, ntm_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deploy the PipelineModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'inference-pipeline-ep-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "sm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pass json payload\n",
    "Because the output of SparkML model is a dense vector, we will use JSON format (instead of CSV format) to pass input to the pipeline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"predictions\":[{\"topic_weights\":[0.1988317221,0.2747220397,0.0973563567,0.0495365895,0.379553318]}]}'\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"schema\": {\n",
    "        \"input\": [\n",
    "        {\n",
    "            \"name\": \"headline_text\",\n",
    "            \"type\": \"string\"\n",
    "        }, \n",
    "    ],\n",
    "    \"output\": \n",
    "        {\n",
    "            \"name\": \"features\",\n",
    "            \"type\": \"double\",\n",
    "            \"struct\": \"vector\"\n",
    "        }\n",
    "    },\n",
    "    \"data\": [\n",
    "            #[\"murder conviction court of criminal appeal\"]\n",
    "        #[\"is dabiq captured opposition forces\"]\n",
    "        [\"lisa scaffidi public hearing possible over expenses scandal\"]\n",
    "            ]\n",
    "            \n",
    "}\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=json_serializer,\n",
    "                                content_type=CONTENT_TYPE_JSON, accept=CONTENT_TYPE_CSV)\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Create batch transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: serial-inference-batch-2019-02-22-01-21-21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................!\n"
     ]
    }
   ],
   "source": [
    "#Number of headlines - choose 10-15\n",
    "input_data_path = 's3://{}/{}/{}'.format(default_bucket, 'sagemaker/inference-pipeline/batch', 'abcnews-batch-input.csv')\n",
    "\n",
    "output_data_path = 's3://{}/{}/{}'.format(default_bucket, 'sagemaker/inference-pipeline/batch_output/abcnews', timestamp_prefix)\n",
    "\n",
    "job_name = 'serial-inference-batch-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "#Define the SageMaker PipelineModel Name\n",
    "model_name = 'inference-pipeline-2019-02-22-01-15-00'\n",
    "\n",
    "transformer = sagemaker.transformer.Transformer(\n",
    "    # This was the model created using PipelineModel and it contains feature processing and NTM stages\n",
    "    model_name = model_name,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge',\n",
    "    strategy = 'SingleRecord',\n",
    "    assemble_with = 'Line',\n",
    "    output_path = output_data_path,\n",
    "    base_transform_job_name='serial-inference-batch',\n",
    "    sagemaker_session=sess,\n",
    "    accept = CONTENT_TYPE_CSV\n",
    ")\n",
    "\n",
    "transformer.transform(data = input_data_path,\n",
    "                      job_name = job_name,\n",
    "                      content_type = CONTENT_TYPE_CSV, \n",
    "                      split_type = 'Line')\n",
    "transformer.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
