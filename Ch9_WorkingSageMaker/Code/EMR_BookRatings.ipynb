{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline - Spark EMR Cluster, with Livy as the REST interface to interact with spark clusters \n",
    "\n",
    "We will look at [book ratings dataset](https://www.kaggle.com/zygmunt/goodbooks-10k) containing 6 million ratings, 10,000 books and 53,424 users. The goal of this notebook is to create a subset of ratings, where it contains users who have rated more than 1% of books and books that have been rated by at least 2% of the users. The resulting dataset will then have rich history of user preferences, along with popularity of books. The intent is to use the generated dataset to recommend books to users.\n",
    "\n",
    "The two source datasets can be placed in a designated s3 bucket:\n",
    "\n",
    "-  ratings.csv\n",
    "-  books.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = 's3://ai-in-aws/'\n",
    "prefix = 'Chapter7/'\n",
    "output_ds_loc = prefix + 'object2vec/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Book Ratings Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window \n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings = spark.read.option(\"header\",\"true\").option(\"quote\", \"\\\"\").option(\"delimiter\", \";\").csv(\"s3://ai-in-aws/awsglue-datasets/BX-Book-Ratings.csv\")\n",
    "\n",
    "# Read ratings dataset\n",
    "ratings = spark.read.option(\"header\",\"true\").option(\"delimiter\", \",\").csv(s3_bucket + prefix + \"ratings.csv\")\n",
    "#Read books csv to load book title\n",
    "books_csv = spark.read.option(\"header\",\"true\").option(\"delimiter\", \",\").csv(s3_bucket + prefix + \"books.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5976479"
     ]
    }
   ],
   "source": [
    "ratings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the first few records of ratings dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|user_id|book_id|rating|\n",
      "+-------+-------+------+\n",
      "|      1|    258|     5|\n",
      "|      2|   4081|     4|\n",
      "|      2|    260|     5|\n",
      "|      2|   9296|     5|\n",
      "|      2|   2318|     3|\n",
      "|      2|     26|     4|\n",
      "|      2|    315|     3|\n",
      "|      2|     33|     4|\n",
      "|      2|    301|     5|\n",
      "|      2|   2686|     5|\n",
      "|      2|   3753|     5|\n",
      "|      2|   8519|     5|\n",
      "|      4|     70|     4|\n",
      "|      4|    264|     3|\n",
      "|      4|    388|     4|\n",
      "|      4|     18|     5|\n",
      "|      4|     27|     5|\n",
      "|      4|     21|     5|\n",
      "|      4|      2|     5|\n",
      "|      4|     23|     5|\n",
      "+-------+-------+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "ratings.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the first few records of the books_csv dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------+-------+-----------+---------+-----------------+--------------------+-------------------------+--------------------+--------------------+-------------+--------------+-------------+------------------+-----------------------+---------+---------+---------+---------+---------+--------------------+--------------------+\n",
      "|book_id|goodreads_book_id|best_book_id|work_id|books_count|     isbn|           isbn13|             authors|original_publication_year|      original_title|               title|language_code|average_rating|ratings_count|work_ratings_count|work_text_reviews_count|ratings_1|ratings_2|ratings_3|ratings_4|ratings_5|           image_url|     small_image_url|\n",
      "+-------+-----------------+------------+-------+-----------+---------+-----------------+--------------------+-------------------------+--------------------+--------------------+-------------+--------------+-------------+------------------+-----------------------+---------+---------+---------+---------+---------+--------------------+--------------------+\n",
      "|      1|          2767052|     2767052|2792775|        272|439023483|9.78043902348e+12|     Suzanne Collins|                   2008.0|    The Hunger Games|The Hunger Games ...|          eng|          4.34|      4780653|           4942365|                 155254|    66715|   127936|   560092|  1481305|  2706317|https://images.gr...|https://images.gr...|\n",
      "|      2|                3|           3|4640799|        491|439554934|9.78043955493e+12|J.K. Rowling, Mar...|                   1997.0|Harry Potter and ...|Harry Potter and ...|          eng|          4.44|      4602479|           4800065|                  75867|    75504|   101676|   455024|  1156318|  3011543|https://images.gr...|https://images.gr...|\n",
      "|      3|            41865|       41865|3212258|        226|316015849|9.78031601584e+12|     Stephenie Meyer|                   2005.0|            Twilight|Twilight (Twiligh...|        en-US|          3.57|      3866839|           3916824|                  95009|   456191|   436802|   793319|   875073|  1355439|https://images.gr...|https://images.gr...|\n",
      "|      4|             2657|        2657|3275794|        487| 61120081|9.78006112008e+12|          Harper Lee|                   1960.0|To Kill a Mocking...|To Kill a Mocking...|          eng|          4.25|      3198671|           3340896|                  72586|    60427|   117415|   446835|  1001952|  1714267|https://images.gr...|https://images.gr...|\n",
      "|      5|             4671|        4671| 245494|       1356|743273567|9.78074327356e+12| F. Scott Fitzgerald|                   1925.0|    The Great Gatsby|    The Great Gatsby|          eng|          3.89|      2683664|           2773745|                  51992|    86236|   197621|   606158|   936012|   947718|https://images.gr...|https://images.gr...|\n",
      "+-------+-----------------+------------+-------+-----------+---------+-----------------+--------------------+-------------------------+--------------------+--------------------+-------------+--------------+-------------+------------------+-----------------------+---------+---------+---------+---------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "books_csv.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain book title and average rating across all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|               title|average_rating|\n",
      "+--------------------+--------------+\n",
      "|The Hunger Games ...|          4.34|\n",
      "|Harry Potter and ...|          4.44|\n",
      "|Twilight (Twiligh...|          3.57|\n",
      "|To Kill a Mocking...|          4.25|\n",
      "|    The Great Gatsby|          3.89|\n",
      "+--------------------+--------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "books_csv.select(\"title\", \"average_rating\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand characteristics of the Data\n",
    "Let us analyze number of ratings by user and book. We will a function value_counts() to group ratings by user and book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_counts(df, colName):\n",
    "    return (df.groupby(colName).count()\n",
    "              .orderBy('count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by seeing the distribution of users by number of books rated by them (bottom 1% of users; bottom 2% of users; .....top 1% of users). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19.0, 19.0, 61.0, 70.0, 71.0, 71.0, 82.0, 96.0, 112.0, 128.0, 148.0, 162.0, 164.0, 164.0, 171.0, 200.0, 200.0]"
     ]
    }
   ],
   "source": [
    "# Number of ratings per user\n",
    "users = value_counts(ratings, 'user_id')\n",
    "users.approxQuantile('count', [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0], 0.01)\n",
    "#approxQuantile(col, probabilities, relativeError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "approxQuantile() function of PySpark dataframe takes list of quantile probabilities, along with relative error. The output of this function is approximate quantiles at given probabilities. \n",
    "As can be see above, top 1% of the users rated 200 books, users who are middle of the distribution rated 111 books, while users in the bottom 25% rated 96 books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id|count|\n",
      "+-------+-----+\n",
      "|  30944|  200|\n",
      "|  12874|  200|\n",
      "|  52036|  199|\n",
      "|  28158|  199|\n",
      "|  12381|  199|\n",
      "|   6630|  197|\n",
      "|  45554|  197|\n",
      "|  15604|  196|\n",
      "|   9806|  196|\n",
      "|  19729|  196|\n",
      "|  37834|  196|\n",
      "|   9668|  196|\n",
      "|  14372|  196|\n",
      "|  24143|  196|\n",
      "|   7563|  196|\n",
      "|   9731|  195|\n",
      "|  38798|  195|\n",
      "|  10509|  195|\n",
      "|  33065|  195|\n",
      "|  25840|  195|\n",
      "+-------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "users.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see the distribution of books by number of users who rated them (bottom 1% of books; bottom 2% of books; .....top 1% of books)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.0, 8.0, 96.0, 101.0, 101.0, 102.0, 119.0, 156.0, 254.0, 547.0, 1274.0, 2587.0, 3029.0, 3948.0, 3948.0, 22806.0, 22806.0]"
     ]
    }
   ],
   "source": [
    "# Number of ratings per book\n",
    "books = value_counts(ratings, 'book_id')\n",
    "books.approxQuantile('count', [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0], 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be see above, top 1% of the books are rated by 22,806 users, books that are in the middle of the distribution are rated by 249 users, while books in the bottom 25% are rated by 156 users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Analytics Ready Dataset\n",
    "\n",
    "We will filter ratings dataset to include only books that have been rated by at at least 1200 users (2.2% of the entire user population) and only users who have rated at least 130 books (1.3% of the entire book population). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter ratings by selecting books that have been rated by at least 1200 users and users who have rated at least 130 books\n",
    "fil_users = users.filter(F.col(\"count\") >= 130)\n",
    "fil_books = books.filter(F.col(\"count\") >= 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37084"
     ]
    }
   ],
   "source": [
    "#Number of books meeting the threshold\n",
    "fil_books.count()\n",
    "#Number of users meeting the threshold\n",
    "fil_users.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the title and average rating for each of the books shortlisted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_books = fil_books.join(books_csv, on=['book_id'], how='inner')\\\n",
    "                    .select(F.col(\"book_id\"),\n",
    "                       F.col(\"count\"),\n",
    "                       F.col(\"title\"),\n",
    "                       F.col(\"average_rating\")     \n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the ratings dataset to only include selective books and users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filtered ratings containing users and books meeting thresholds \n",
    "fil_ratings = ratings.join(fil_users, on=['user_id'], how='inner').join(fil_books, on=['book_id'], how='inner')\\\n",
    "                .select(F.col(\"book_id\"),\n",
    "                       F.col(\"user_id\"),\n",
    "                       F.col(\"rating\"),\n",
    "                       F.col(\"title\"),\n",
    "                       F.col(\"average_rating\")\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "985"
     ]
    }
   ],
   "source": [
    "# Final count of ratings, users and books\n",
    "\n",
    "fil_ratings.count() # ~1 million - 1,051,299\n",
    "fil_ratings.select('user_id').distinct().count() #12,347 users\n",
    "fil_ratings.select('book_id').distinct().count()  #985 books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create integer indexes for users and books to develop a recommender system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create indexes for books and users\n",
    "\n",
    "#Determine unique users and books from ratings\n",
    "uniq_users = value_counts(fil_ratings, 'user_id')\n",
    "uniq_books = value_counts(fil_ratings, 'book_id')\n",
    "\n",
    "#object2vec algorithm takes user_ind and book_ind starting from zero\n",
    "w1 = Window.orderBy(\"user_id\") \n",
    "uniq_users = uniq_users.withColumn(\"user_ind\", F.row_number().over(w1)-1)\n",
    "\n",
    "w2 = Window.orderBy(\"book_id\") \n",
    "uniq_books = uniq_books.withColumn(\"book_ind\", F.row_number().over(w2)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filtered ratings containing user and book indexes, along with rating\n",
    "upd_fil_ratings = fil_ratings.join(uniq_users, on=['user_id'], how='inner').join(uniq_books, on=['book_id'], how='inner')\\\n",
    "                .select(F.col(\"book_id\"),\n",
    "                       F.col(\"user_id\"),\n",
    "                       F.col(\"rating\"),\n",
    "                       F.col(\"title\"), \n",
    "                       F.col(\"book_ind\"),\n",
    "                       F.col(\"user_ind\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze if we have sequential listing of user ids and book ids and that rating values are greater than zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(max(book_ind)=984)"
     ]
    }
   ],
   "source": [
    "fil_ratings.select('rating').distinct().show()\n",
    "print(upd_fil_ratings.agg({\"user_ind\": \"max\"}).collect()[0])\n",
    "print(upd_fil_ratings.agg({\"book_ind\": \"max\"}).collect()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prepared dataset in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "upd_fil_ratings.write.parquet(s3_bucket + output_ds_loc + \"/bookratings.parquet\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
